root@node1 /var/lib/ceph/osd/ceph-0 # ^C
root@node1 /var/lib/ceph/osd/ceph-0 # ^C
root@node1 /var/lib/ceph/osd/ceph-0 # ^C
root@node1 /var/lib/ceph/osd/ceph-0 # clear

root@node1 /var/lib/ceph/osd/ceph-0 # ls
activate.monmap  active  ceph_fsid  current  fsid  journal  keyring  magic  ready  store_version  superblock  upstart  whoami
root@node1 /var/lib/ceph/osd/ceph-0 # pwd
/var/lib/ceph/osd/ceph-0
root@node1 /var/lib/ceph/osd/ceph-0 # cd ~
root@node1 ~ # sudo start ceph-osd-all
start: Job is already running: ceph-osd-all
root@node1 ~ # sudo start ceph-osd id=2
ceph-osd (ceph/2) stop/pre-start, process 25040
root@node1 ~ # sudo start ceph-osd id=3
ceph-osd (ceph/3) stop/pre-start, process 25045
root@node1 ~ # ceph status
    cluster 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
     health HEALTH_OK
     monmap e1: 1 mons at {node1=136.243.49.217:6789/0}
            election epoch 1, quorum 0 node1
     osdmap e138: 4 osds: 2 up, 2 in
            flags sortbitwise
      pgmap v416619: 466 pgs, 16 pools, 72642 MB data, 18387 objects
            151 GB used, 10378 GB / 11089 GB avail
                 466 active+clean
  client io 0 B/s rd, 9828 B/s wr, 2 op/s
root@node1 ~ # ceph osd -h

General usage: 
==============
usage: ceph [-h] [-c CEPHCONF] [-i INPUT_FILE] [-o OUTPUT_FILE]
            [--id CLIENT_ID] [--name CLIENT_NAME] [--cluster CLUSTER]
            [--admin-daemon ADMIN_SOCKET] [--admin-socket ADMIN_SOCKET_NOPE]
            [-s] [-w] [--watch-debug] [--watch-info] [--watch-sec]
            [--watch-warn] [--watch-error] [--version] [--verbose] [--concise]
            [-f {json,json-pretty,xml,xml-pretty,plain}]
            [--connect-timeout CLUSTER_TIMEOUT]

Ceph administration tool

optional arguments:
  -h, --help            request mon help
  -c CEPHCONF, --conf CEPHCONF
                        ceph configuration file
  -i INPUT_FILE, --in-file INPUT_FILE
                        input file
  -o OUTPUT_FILE, --out-file OUTPUT_FILE
                        output file
  --id CLIENT_ID, --user CLIENT_ID
                        client id for authentication
  --name CLIENT_NAME, -n CLIENT_NAME
                        client name for authentication
  --cluster CLUSTER     cluster name
  --admin-daemon ADMIN_SOCKET
                        submit admin-socket commands ("help" for help
  --admin-socket ADMIN_SOCKET_NOPE
                        you probably mean --admin-daemon
  -s, --status          show cluster status
  -w, --watch           watch live cluster changes
  --watch-debug         watch debug events
  --watch-info          watch info events
  --watch-sec           watch security events
  --watch-warn          watch warn events
  --watch-error         watch error events
  --version, -v         display version
  --verbose             make verbose
  --concise             make less verbose
  -f {json,json-pretty,xml,xml-pretty,plain}, --format {json,json-pretty,xml,xml-pretty,plain}
  --connect-timeout CLUSTER_TIMEOUT
                        set a timeout for connecting to the cluster

Monitor commands: 
=================
[Contacting monitor, timeout after 5 seconds]
osd blacklist add|rm <EntityAddr>        add (optionally until <expire> seconds 
 {<float[0.0-]>}                          from now) or remove <addr> from 
                                          blacklist
osd blacklist ls                         show blacklisted clients
osd blocked-by                           print histogram of which OSDs are 
                                          blocking their peers
osd create {<uuid>} {<int[0-]>}          create new osd (with optional UUID and 
                                          ID)
osd crush add <osdname (id|osd.id)>      add or update crushmap position and 
 <float[0.0-]> <args> [<args>...]         weight for <name> with <weight> and 
                                          location <args>
osd crush add-bucket <name> <type>       add no-parent (probably root) crush 
                                          bucket <name> of type <type>
osd crush create-or-move <osdname (id|   create entry or move existing entry 
 osd.id)> <float[0.0-]> <args> [<args>..  for <name> <weight> at/to location 
 .]                                       <args>
osd crush dump                           dump crush map
osd crush get-tunable straw_calc_version get crush tunable <tunable>
osd crush link <name> <args> [<args>...] link existing entry for <name> under 
                                          location <args>
osd crush move <name> <args> [<args>...] move existing entry for <name> to 
                                          location <args>
osd crush remove <name> {<ancestor>}     remove <name> from crush map (
                                          everywhere, or just at <ancestor>)
osd crush rename-bucket <srcname>        rename bucket <srcname> to <dstname>
 <dstname>                               
osd crush reweight <name> <float[0.0-]>  change <name>'s weight to <weight> in 
                                          crush map
osd crush reweight-all                   recalculate the weights for the tree 
                                          to ensure they sum correctly
osd crush reweight-subtree <name>        change all leaf items beneath <name> 
 <float[0.0-]>                            to <weight> in crush map
osd crush rm <name> {<ancestor>}         remove <name> from crush map (
                                          everywhere, or just at <ancestor>)
osd crush rule create-erasure <name>     create crush rule <name> for erasure 
 {<profile>}                              coded pool created with <profile> (
                                          default default)
osd crush rule create-simple <name>      create crush rule <name> to start from 
 <root> <type> {firstn|indep}             <root>, replicate across buckets of 
                                          type <type>, using a choose mode of 
                                          <firstn|indep> (default firstn; indep 
                                          best for erasure pools)
osd crush rule dump {<name>}             dump crush rule <name> (default all)
osd crush rule list                      list crush rules
osd crush rule ls                        list crush rules
osd crush rule rm <name>                 remove crush rule <name>
osd crush set                            set crush map from input file
osd crush set <osdname (id|osd.id)>      update crushmap position and weight 
 <float[0.0-]> <args> [<args>...]         for <name> to <weight> with location 
                                          <args>
osd crush set-tunable straw_calc_        set crush tunable <tunable> to <value>
 version <int>                           
osd crush show-tunables                  show current crush tunables
osd crush tree                           dump crush buckets and items in a tree 
                                          view
osd crush tunables legacy|argonaut|      set crush tunables values to <profile>
 bobtail|firefly|hammer|optimal|default  
osd crush unlink <name> {<ancestor>}     unlink <name> from crush map (
                                          everywhere, or just at <ancestor>)
osd deep-scrub <who>                     initiate deep scrub on osd <who>
osd df {plain|tree}                      show OSD utilization
osd down <ids> [<ids>...]                set osd(s) <id> [<id>...] down
osd dump {<int[0-]>}                     print summary of OSD map
osd erasure-code-profile get <name>      get erasure code profile <name>
osd erasure-code-profile ls              list all erasure code profiles
osd erasure-code-profile rm <name>       remove erasure code profile <name>
osd erasure-code-profile set <name>      create erasure code profile <name> 
 {<profile> [<profile>...]}               with [<key[=value]> ...] pairs. Add a 
                                          --force at the end to override an 
                                          existing profile (VERY DANGEROUS)
osd find <int[0-]>                       find osd <id> in the CRUSH map and 
                                          show its location
osd getcrushmap {<int[0-]>}              get CRUSH map
osd getmap {<int[0-]>}                   get OSD map
osd getmaxosd                            show largest OSD id
osd in <ids> [<ids>...]                  set osd(s) <id> [<id>...] in
osd lost <int[0-]> {--yes-i-really-mean- mark osd as permanently lost. THIS 
 it}                                      DESTROYS DATA IF NO MORE REPLICAS 
                                          EXIST, BE CAREFUL
osd ls {<int[0-]>}                       show all OSD ids
osd lspools {<int>}                      list pools
osd map <poolname> <objectname>          find pg for <object> in <pool> with 
 {<nspace>}                               [namespace]
osd metadata {<int[0-]>}                 fetch metadata for osd {id} (default 
                                          all)
osd out <ids> [<ids>...]                 set osd(s) <id> [<id>...] out
osd pause                                pause osd
osd perf                                 print dump of OSD perf summary stats
osd pg-temp <pgid> {<id> [<id>...]}      set pg_temp mapping pgid:[<id> [<id>...
                                          ]] (developers only)
osd pool create <poolname> <int[0-]>     create pool
 {<int[0-]>} {replicated|erasure}        
 {<erasure_code_profile>} {<ruleset>}    
 {<int>}                                 
osd pool delete <poolname> {<poolname>}  delete pool
 {--yes-i-really-really-mean-it}         
osd pool get <poolname> size|min_size|   get pool parameter <var>
 crash_replay_interval|pg_num|pgp_num|   
 crush_ruleset|hashpspool|nodelete|      
 nopgchange|nosizechange|write_fadvise_  
 dontneed|noscrub|nodeep-scrub|hit_set_  
 type|hit_set_period|hit_set_count|hit_  
 set_fpp|auid|target_max_objects|target_ 
 max_bytes|cache_target_dirty_ratio|     
 cache_target_dirty_high_ratio|cache_    
 target_full_ratio|cache_min_flush_age|  
 cache_min_evict_age|erasure_code_       
 profile|min_read_recency_for_promote|   
 all|min_write_recency_for_promote|fast_ 
 read                                    
osd pool get-quota <poolname>            obtain object or byte limits for pool
osd pool ls {detail}                     list pools
osd pool mksnap <poolname> <snap>        make snapshot <snap> in <pool>
osd pool rename <poolname> <poolname>    rename <srcpool> to <destpool>
osd pool rmsnap <poolname> <snap>        remove snapshot <snap> from <pool>
osd pool set <poolname> size|min_size|   set pool parameter <var> to <val>
 crash_replay_interval|pg_num|pgp_num|   
 crush_ruleset|hashpspool|nodelete|      
 nopgchange|nosizechange|write_fadvise_  
 dontneed|noscrub|nodeep-scrub|hit_set_  
 type|hit_set_period|hit_set_count|hit_  
 set_fpp|use_gmt_hitset|debug_fake_ec_   
 pool|target_max_bytes|target_max_       
 objects|cache_target_dirty_ratio|cache_ 
 target_dirty_high_ratio|cache_target_   
 full_ratio|cache_min_flush_age|cache_   
 min_evict_age|auid|min_read_recency_    
 for_promote|min_write_recency_for_      
 promote|fast_read <val> {--yes-i-       
 really-mean-it}                         
osd pool set-quota <poolname> max_       set object or byte limit on pool
 objects|max_bytes <val>                 
osd pool stats {<name>}                  obtain stats from all pools, or from 
                                          specified pool
osd primary-affinity <osdname (id|osd.   adjust osd primary-affinity from 0.0 <=
 id)> <float[0.0-1.0]>                     <weight> <= 1.0
osd primary-temp <pgid> <id>             set primary_temp mapping pgid:<id>|-1 (
                                          developers only)
osd repair <who>                         initiate repair on osd <who>
osd reweight <int[0-]> <float[0.0-1.0]>  reweight osd to 0.0 < <weight> < 1.0
osd reweight-by-pg <int[100-]>           reweight OSDs by PG distribution 
 {<poolname> [<poolname>...]}             [overload-percentage-for-
                                          consideration, default 120]
osd reweight-by-utilization {<int[100-   reweight OSDs by utilization [overload-
 ]>}                                      percentage-for-consideration, default 
                                          120]
osd rm <ids> [<ids>...]                  remove osd(s) <id> [<id>...] in
osd scrub <who>                          initiate scrub on osd <who>
osd set full|pause|noup|nodown|noout|    set <key>
 noin|nobackfill|norebalance|norecover|  
 noscrub|nodeep-scrub|notieragent|       
 sortbitwise                             
osd setcrushmap                          set crush map from input file
osd setmaxosd <int[0-]>                  set new maximum osd value
osd stat                                 print summary of OSD map
osd thrash <int[0-]>                     thrash OSDs for <num_epochs>
osd tier add <poolname> <poolname> {--   add the tier <tierpool> (the second 
 force-nonempty}                          one) to base pool <pool> (the first 
                                          one)
osd tier add-cache <poolname>            add a cache <tierpool> (the second one)
 <poolname> <int[0-]>                     of size <size> to existing pool 
                                          <pool> (the first one)
osd tier cache-mode <poolname> none|     specify the caching mode for cache 
 writeback|forward|readonly|readforward|  tier <pool>
 readproxy                               
osd tier remove <poolname> <poolname>    remove the tier <tierpool> (the second 
                                          one) from base pool <pool> (the first 
                                          one)
osd tier remove-overlay <poolname>       remove the overlay pool for base pool 
                                          <pool>
osd tier set-overlay <poolname>          set the overlay pool for base pool 
 <poolname>                               <pool> to be <overlaypool>
osd tree {<int[0-]>}                     print OSD tree
osd unpause                              unpause osd
osd unset full|pause|noup|nodown|noout|  unset <key>
 noin|nobackfill|norebalance|norecover|  
 noscrub|nodeep-scrub|notieragent|       
 sortbitwise                             
root@node1 ~ # ceph osd in 2 3
marked in osd.2. marked in osd.3. 
root@node1 ~ # ceph status
    cluster 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
     health HEALTH_WARN
            2/4 in osds are down
     monmap e1: 1 mons at {node1=136.243.49.217:6789/0}
            election epoch 1, quorum 0 node1
     osdmap e141: 4 osds: 2 up, 4 in; 274 remapped pgs
            flags sortbitwise
      pgmap v416633: 466 pgs, 16 pools, 72642 MB data, 18387 objects
            151 GB used, 10378 GB / 11089 GB avail
                 466 active+clean
root@node1 ~ # restart ceph-osd id=2
restart: Unknown instance: ceph/2
root@node1 ~ # restart ceph-osd id=3
restart: Unknown instance: ceph/3
root@node1 ~ # ceph-deploy disk list node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy disk list node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : list
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd6c32bf638>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fd6c3723578>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', None, None)]
[ceph_deploy][ERROR ] ConfigError: Cannot load config: [Errno 2] No such file or directory: 'ceph.conf'; has `ceph-deploy new` been run in this directory?

root@node1 ~ # cd /home/megdc/ceph-cluster/
root@node1 /home/megdc/ceph-cluster # su megdc
megdc@node1:~/ceph-cluster$ ceph-deploy disk list node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy disk list node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : list
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc7de00a638>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fc7de46e578>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', None, None)]
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Listing disks on node2...
[node2][DEBUG ] find the location of an executable
[node2][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[node2][DEBUG ] /dev/loop0 other, unknown
[node2][DEBUG ] /dev/loop1 other, unknown
[node2][DEBUG ] /dev/loop2 other, unknown
[node2][DEBUG ] /dev/loop3 other, unknown
[node2][DEBUG ] /dev/loop4 other, unknown
[node2][DEBUG ] /dev/loop5 other, unknown
[node2][DEBUG ] /dev/loop6 other, unknown
[node2][DEBUG ] /dev/loop7 other, unknown
[node2][DEBUG ] /dev/md0 other, ext3, mounted on /boot
[node2][DEBUG ] /dev/md1 swap, swap
[node2][DEBUG ] /dev/md2 other, ext4, mounted on /
[node2][DEBUG ] /dev/ram0 other, unknown
[node2][DEBUG ] /dev/ram1 other, unknown
[node2][DEBUG ] /dev/ram10 other, unknown
[node2][DEBUG ] /dev/ram11 other, unknown
[node2][DEBUG ] /dev/ram12 other, unknown
[node2][DEBUG ] /dev/ram13 other, unknown
[node2][DEBUG ] /dev/ram14 other, unknown
[node2][DEBUG ] /dev/ram15 other, unknown
[node2][DEBUG ] /dev/ram2 other, unknown
[node2][DEBUG ] /dev/ram3 other, unknown
[node2][DEBUG ] /dev/ram4 other, unknown
[node2][DEBUG ] /dev/ram5 other, unknown
[node2][DEBUG ] /dev/ram6 other, unknown
[node2][DEBUG ] /dev/ram7 other, unknown
[node2][DEBUG ] /dev/ram8 other, unknown
[node2][DEBUG ] /dev/ram9 other, unknown
[node2][DEBUG ] /dev/sda :
[node2][DEBUG ]  /dev/sda1 other, linux_raid_member
[node2][DEBUG ]  /dev/sda2 other, linux_raid_member
[node2][DEBUG ]  /dev/sda3 other, linux_raid_member
[node2][DEBUG ] /dev/sdb :
[node2][DEBUG ]  /dev/sdb1 other, linux_raid_member
[node2][DEBUG ]  /dev/sdb2 other, linux_raid_member
[node2][DEBUG ]  /dev/sdb3 other, linux_raid_member
[node2][DEBUG ] /dev/sdc :
[node2][DEBUG ]  /dev/sdc1 other, ext4, mounted on /storage1
[node2][DEBUG ] /dev/sdd :
[node2][DEBUG ]  /dev/sdd1 other, ext4, mounted on /storage2
megdc@node1:~/ceph-cluster$ ceph status
    cluster 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
     health HEALTH_OK
     monmap e1: 1 mons at {node1=136.243.49.217:6789/0}
            election epoch 1, quorum 0 node1
     osdmap e143: 4 osds: 2 up, 2 in
            flags sortbitwise
      pgmap v416688: 466 pgs, 16 pools, 72657 MB data, 18392 objects
            151 GB used, 10378 GB / 11089 GB avail
                 466 active+clean
  client io 0 B/s rd, 5324 B/s wr, 1 op/s
megdc@node1:~/ceph-cluster$ ceph osd tree
ID WEIGHT   TYPE NAME               UP/DOWN REWEIGHT PRIMARY-AFFINITY 
-5        0 rack unknownrack                                          
-4        0     host 138.201.21.215                                   
-1 21.64975 root default                                              
-2 10.82977     host node1                                            
 0  5.41489         osd.0                up  1.00000          1.00000 
 1  5.41489         osd.1                up  1.00000          1.00000 
-3 10.81998     host node2                                            
 2  5.40999         osd.2              down        0          1.00000 
 3  5.40999         osd.3              down        0          1.00000 
megdc@node1:~/ceph-cluster$ ls /var/log/ceph
ls: cannot open directory /var/log/ceph: Permission denied
megdc@node1:~/ceph-cluster$ sudo -s
root@node1:~/ceph-cluster# ls /var/log/ceph
ceph.audit.log       ceph.log       ceph.log.7.gz            ceph-mon.node1.log.6.gz  ceph-osd.0.log.5.gz  ceph-osd.1.log.4.gz
ceph.audit.log.1.gz  ceph.log.1.gz  ceph-mon.node1.log       ceph-mon.node1.log.7.gz  ceph-osd.0.log.6.gz  ceph-osd.1.log.5.gz
ceph.audit.log.2.gz  ceph.log.2.gz  ceph-mon.node1.log.1.gz  ceph-osd.0.log           ceph-osd.0.log.7.gz  ceph-osd.1.log.6.gz
ceph.audit.log.3.gz  ceph.log.3.gz  ceph-mon.node1.log.2.gz  ceph-osd.0.log.1.gz      ceph-osd.1.log       ceph-osd.1.log.7.gz
ceph.audit.log.4.gz  ceph.log.4.gz  ceph-mon.node1.log.3.gz  ceph-osd.0.log.2.gz      ceph-osd.1.log.1.gz
ceph.audit.log.5.gz  ceph.log.5.gz  ceph-mon.node1.log.4.gz  ceph-osd.0.log.3.gz      ceph-osd.1.log.2.gz
ceph.audit.log.6.gz  ceph.log.6.gz  ceph-mon.node1.log.5.gz  ceph-osd.0.log.4.gz      ceph-osd.1.log.3.gz
root@node1:~/ceph-cluster# tail -100 ceph.log 
[2016-05-18 16:58:23,402][ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage1/osd', None), ('node2', '/storage2/osd', None)]
[2016-05-18 16:58:23,402][ceph_deploy.osd][DEBUG ] Activating cluster ceph disks node2:/storage1/osd: node2:/storage2/osd:
[2016-05-18 16:58:23,630][node2][DEBUG ] connection detected need for sudo
[2016-05-18 16:58:23,840][node2][DEBUG ] connected to host: node2 
[2016-05-18 16:58:23,841][node2][DEBUG ] detect platform information from remote host
[2016-05-18 16:58:23,866][node2][DEBUG ] detect machine type
[2016-05-18 16:58:23,869][node2][DEBUG ] find the location of an executable
[2016-05-18 16:58:23,871][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[2016-05-18 16:58:23,871][ceph_deploy.osd][DEBUG ] activating host node2 disk /storage1/osd
[2016-05-18 16:58:23,871][ceph_deploy.osd][DEBUG ] will use init type: upstart
[2016-05-18 16:58:23,874][node2][INFO  ] Running command: sudo ceph-disk -v activate --mark-init upstart --mount /storage1/osd
[2016-05-18 16:58:23,997][node2][WARNING] DEBUG:ceph-disk:Cluster uuid is 271ea3ed-a709-4dae-8612-e172945efc72
[2016-05-18 16:58:23,997][node2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2016-05-18 16:58:23,997][node2][WARNING] DEBUG:ceph-disk:Cluster name is ceph
[2016-05-18 16:58:23,998][node2][WARNING] DEBUG:ceph-disk:OSD uuid is d2723fe3-f6f3-4bc5-8897-44d378a9abf1
[2016-05-18 16:58:23,998][node2][WARNING] DEBUG:ceph-disk:Allocating OSD id...
[2016-05-18 16:58:23,998][node2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise d2723fe3-f6f3-4bc5-8897-44d378a9abf1
[2016-05-18 17:03:23,998][node2][WARNING] No data was received after 300 seconds, disconnecting...
[2016-05-18 17:03:29,004][node2][INFO  ] checking OSD status...
[2016-05-18 17:03:29,006][node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[2016-05-18 17:03:29,374][node2][DEBUG ] connection detected need for sudo
[2016-05-18 17:03:29,568][node2][DEBUG ] connected to host: node2 
[2016-05-18 17:03:29,569][node2][DEBUG ] detect platform information from remote host
[2016-05-18 17:03:29,586][node2][DEBUG ] detect machine type
[2016-05-18 17:03:29,589][node2][DEBUG ] find the location of an executable
[2016-05-18 17:03:29,590][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[2016-05-18 17:03:29,590][ceph_deploy.osd][DEBUG ] activating host node2 disk /storage2/osd
[2016-05-18 17:03:29,590][ceph_deploy.osd][DEBUG ] will use init type: upstart
[2016-05-18 17:03:29,591][node2][INFO  ] Running command: sudo ceph-disk -v activate --mark-init upstart --mount /storage2/osd
[2016-05-18 17:03:29,709][node2][WARNING] DEBUG:ceph-disk:Cluster uuid is 271ea3ed-a709-4dae-8612-e172945efc72
[2016-05-18 17:03:29,709][node2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[2016-05-18 17:03:29,709][node2][WARNING] DEBUG:ceph-disk:Cluster name is ceph
[2016-05-18 17:03:29,709][node2][WARNING] DEBUG:ceph-disk:OSD uuid is 04e2aa6c-c03f-4cfe-aa91-b92a758ae6aa
[2016-05-18 17:03:29,709][node2][WARNING] DEBUG:ceph-disk:Allocating OSD id...
[2016-05-18 17:03:29,710][node2][WARNING] INFO:ceph-disk:Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 04e2aa6c-c03f-4cfe-aa91-b92a758ae6aa
[2016-05-18 17:08:29,710][node2][WARNING] No data was received after 300 seconds, disconnecting...
[2016-05-18 17:08:34,712][node2][INFO  ] checking OSD status...
[2016-05-18 17:08:34,715][node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[2016-05-19 08:28:54,960][ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[2016-05-19 08:28:54,961][ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy disk list node2
[2016-05-19 08:28:54,961][ceph_deploy.cli][INFO  ] ceph-deploy options:
[2016-05-19 08:28:54,961][ceph_deploy.cli][INFO  ]  username                      : None
[2016-05-19 08:28:54,961][ceph_deploy.cli][INFO  ]  verbose                       : False
[2016-05-19 08:28:54,961][ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[2016-05-19 08:28:54,961][ceph_deploy.cli][INFO  ]  subcommand                    : list
[2016-05-19 08:28:54,961][ceph_deploy.cli][INFO  ]  quiet                         : False
[2016-05-19 08:28:54,961][ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc7de00a638>
[2016-05-19 08:28:54,962][ceph_deploy.cli][INFO  ]  cluster                       : ceph
[2016-05-19 08:28:54,962][ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fc7de46e578>
[2016-05-19 08:28:54,962][ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[2016-05-19 08:28:54,962][ceph_deploy.cli][INFO  ]  default_release               : False
[2016-05-19 08:28:54,962][ceph_deploy.cli][INFO  ]  disk                          : [('node2', None, None)]
[2016-05-19 08:28:55,182][node2][DEBUG ] connection detected need for sudo
[2016-05-19 08:28:55,389][node2][DEBUG ] connected to host: node2 
[2016-05-19 08:28:55,390][node2][DEBUG ] detect platform information from remote host
[2016-05-19 08:28:55,413][node2][DEBUG ] detect machine type
[2016-05-19 08:28:55,417][node2][DEBUG ] find the location of an executable
[2016-05-19 08:28:55,418][ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[2016-05-19 08:28:55,419][ceph_deploy.osd][DEBUG ] Listing disks on node2...
[2016-05-19 08:28:55,419][node2][DEBUG ] find the location of an executable
[2016-05-19 08:28:55,422][node2][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[2016-05-19 08:28:56,898][node2][DEBUG ] /dev/loop0 other, unknown
[2016-05-19 08:28:56,899][node2][DEBUG ] /dev/loop1 other, unknown
[2016-05-19 08:28:56,899][node2][DEBUG ] /dev/loop2 other, unknown
[2016-05-19 08:28:56,899][node2][DEBUG ] /dev/loop3 other, unknown
[2016-05-19 08:28:56,899][node2][DEBUG ] /dev/loop4 other, unknown
[2016-05-19 08:28:56,899][node2][DEBUG ] /dev/loop5 other, unknown
[2016-05-19 08:28:56,900][node2][DEBUG ] /dev/loop6 other, unknown
[2016-05-19 08:28:56,900][node2][DEBUG ] /dev/loop7 other, unknown
[2016-05-19 08:28:56,900][node2][DEBUG ] /dev/md0 other, ext3, mounted on /boot
[2016-05-19 08:28:56,900][node2][DEBUG ] /dev/md1 swap, swap
[2016-05-19 08:28:56,900][node2][DEBUG ] /dev/md2 other, ext4, mounted on /
[2016-05-19 08:28:56,900][node2][DEBUG ] /dev/ram0 other, unknown
[2016-05-19 08:28:56,900][node2][DEBUG ] /dev/ram1 other, unknown
[2016-05-19 08:28:56,900][node2][DEBUG ] /dev/ram10 other, unknown
[2016-05-19 08:28:56,901][node2][DEBUG ] /dev/ram11 other, unknown
[2016-05-19 08:28:56,901][node2][DEBUG ] /dev/ram12 other, unknown
[2016-05-19 08:28:56,901][node2][DEBUG ] /dev/ram13 other, unknown
[2016-05-19 08:28:56,901][node2][DEBUG ] /dev/ram14 other, unknown
[2016-05-19 08:28:56,901][node2][DEBUG ] /dev/ram15 other, unknown
[2016-05-19 08:28:56,901][node2][DEBUG ] /dev/ram2 other, unknown
[2016-05-19 08:28:56,901][node2][DEBUG ] /dev/ram3 other, unknown
[2016-05-19 08:28:56,902][node2][DEBUG ] /dev/ram4 other, unknown
[2016-05-19 08:28:56,902][node2][DEBUG ] /dev/ram5 other, unknown
[2016-05-19 08:28:56,902][node2][DEBUG ] /dev/ram6 other, unknown
[2016-05-19 08:28:56,902][node2][DEBUG ] /dev/ram7 other, unknown
[2016-05-19 08:28:56,902][node2][DEBUG ] /dev/ram8 other, unknown
[2016-05-19 08:28:56,902][node2][DEBUG ] /dev/ram9 other, unknown
[2016-05-19 08:28:56,902][node2][DEBUG ] /dev/sda :
[2016-05-19 08:28:56,902][node2][DEBUG ]  /dev/sda1 other, linux_raid_member
[2016-05-19 08:28:56,903][node2][DEBUG ]  /dev/sda2 other, linux_raid_member
[2016-05-19 08:28:56,903][node2][DEBUG ]  /dev/sda3 other, linux_raid_member
[2016-05-19 08:28:56,903][node2][DEBUG ] /dev/sdb :
[2016-05-19 08:28:56,903][node2][DEBUG ]  /dev/sdb1 other, linux_raid_member
[2016-05-19 08:28:56,903][node2][DEBUG ]  /dev/sdb2 other, linux_raid_member
[2016-05-19 08:28:56,903][node2][DEBUG ]  /dev/sdb3 other, linux_raid_member
[2016-05-19 08:28:56,903][node2][DEBUG ] /dev/sdc :
[2016-05-19 08:28:56,904][node2][DEBUG ]  /dev/sdc1 other, ext4, mounted on /storage1
[2016-05-19 08:28:56,904][node2][DEBUG ] /dev/sdd :
[2016-05-19 08:28:56,904][node2][DEBUG ]  /dev/sdd1 other, ext4, mounted on /storage2
root@node1:~/ceph-cluster# start ceph-osd id=2
ceph-osd (ceph/2) stop/pre-start, process 27022
root@node1:~/ceph-cluster# su megdc
megdc@node1:~/ceph-cluster$ start ceph-osd id=2
start: Rejected send message, 1 matched rules; type="method_call", sender=":1.49528" (uid=9870 pid=27041 comm="start ceph-osd id=2 ") interface="com.ubuntu.Upstart0_6.Job" member="Start" error name="(unset)" requested_reply="0" destination="com.ubuntu.Upstart" (uid=0 pid=1 comm="/sbin/init")
megdc@node1:~/ceph-cluster$ ceph status
    cluster 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
     health HEALTH_OK
     monmap e1: 1 mons at {node1=136.243.49.217:6789/0}
            election epoch 1, quorum 0 node1
     osdmap e143: 4 osds: 2 up, 2 in
            flags sortbitwise
      pgmap v416738: 466 pgs, 16 pools, 72657 MB data, 18392 objects
            151 GB used, 10378 GB / 11089 GB avail
                 466 active+clean
  client io 0 B/s rd, 6962 B/s wr, 2 op/s
megdc@node1:~/ceph-cluster$ ceph osd tree
ID WEIGHT   TYPE NAME               UP/DOWN REWEIGHT PRIMARY-AFFINITY 
-5        0 rack unknownrack                                          
-4        0     host 138.201.21.215                                   
-1 21.64975 root default                                              
-2 10.82977     host node1                                            
 0  5.41489         osd.0                up  1.00000          1.00000 
 1  5.41489         osd.1                up  1.00000          1.00000 
-3 10.81998     host node2                                            
 2  5.40999         osd.2              down        0          1.00000 
 3  5.40999         osd.3              down        0          1.00000 
megdc@node1:~/ceph-cluster$ nano /etc/ceph/ceph.conf 
megdc@node1:~/ceph-cluster$ sudo nano /etc/ceph/ceph.conf 
megdc@node1:~/ceph-cluster$ ceph osd tree
ID WEIGHT   TYPE NAME               UP/DOWN REWEIGHT PRIMARY-AFFINITY 
-5        0 rack unknownrack                                          
-4        0     host 138.201.21.215                                   
-1 21.64975 root default                                              
-2 10.82977     host node1                                            
 0  5.41489         osd.0                up  1.00000          1.00000 
 1  5.41489         osd.1                up  1.00000          1.00000 
-3 10.81998     host node2                                            
 2  5.40999         osd.2              down        0          1.00000 
 3  5.40999         osd.3              down        0          1.00000 
megdc@node1:~/ceph-cluster$ start ceph-osd id=2
start: Rejected send message, 1 matched rules; type="method_call", sender=":1.49539" (uid=9870 pid=27746 comm="start ceph-osd id=2 ") interface="com.ubuntu.Upstart0_6.Job" member="Start" error name="(unset)" requested_reply="0" destination="com.ubuntu.Upstart" (uid=0 pid=1 comm="/sbin/init")
megdc@node1:~/ceph-cluster$ ceph status
    cluster 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
     health HEALTH_OK
     monmap e1: 1 mons at {node1=136.243.49.217:6789/0}
            election epoch 1, quorum 0 node1
     osdmap e143: 4 osds: 2 up, 2 in
            flags sortbitwise
      pgmap v416782: 466 pgs, 16 pools, 72657 MB data, 18392 objects
            151 GB used, 10378 GB / 11089 GB avail
                 466 active+clean
  client io 0 B/s rd, 4914 B/s wr, 2 op/s
megdc@node1:~/ceph-cluster$ sudo nano /etc/ceph/ceph.conf 
megdc@node1:~/ceph-cluster$ ceph status
2016-05-19 08:40:53.446104 7f4bebb81700  1 -- :/0 messenger.start
2016-05-19 08:40:53.446866 7f4bebb81700  1 -- :/3204162468 --> 136.243.49.217:6789/0 -- auth(proto 0 30 bytes epoch 0) v1 -- ?+0 0x7f4be4060830 con 0x7f4be405f330
2016-05-19 08:40:53.447159 7f4be8401700  1 -- 136.243.49.217:0/3204162468 learned my addr 136.243.49.217:0/3204162468
2016-05-19 08:40:53.447758 7f4be9403700  1 -- 136.243.49.217:0/3204162468 <== mon.0 136.243.49.217:6789/0 1 ==== mon_map magic: 0 v1 ==== 195+0+0 (359242282 0 0) 0x7f4bd8000b00 con 0x7f4be405f330
2016-05-19 08:40:53.447860 7f4be9403700  1 -- 136.243.49.217:0/3204162468 <== mon.0 136.243.49.217:6789/0 2 ==== auth_reply(proto 2 0 (0) Success) v1 ==== 33+0+0 (526060582 0 0) 0x7f4bd8000fe0 con 0x7f4be405f330
2016-05-19 08:40:53.447982 7f4be9403700  1 -- 136.243.49.217:0/3204162468 --> 136.243.49.217:6789/0 -- auth(proto 2 32 bytes epoch 0) v1 -- ?+0 0x7f4bcc001340 con 0x7f4be405f330
2016-05-19 08:40:53.448562 7f4be9403700  1 -- 136.243.49.217:0/3204162468 <== mon.0 136.243.49.217:6789/0 3 ==== auth_reply(proto 2 0 (0) Success) v1 ==== 206+0+0 (838120645 0 0) 0x7f4bd8000fe0 con 0x7f4be405f330
2016-05-19 08:40:53.448681 7f4be9403700  1 -- 136.243.49.217:0/3204162468 --> 136.243.49.217:6789/0 -- auth(proto 2 165 bytes epoch 0) v1 -- ?+0 0x7f4bcc006a40 con 0x7f4be405f330
2016-05-19 08:40:53.449252 7f4be9403700  1 -- 136.243.49.217:0/3204162468 <== mon.0 136.243.49.217:6789/0 4 ==== auth_reply(proto 2 0 (0) Success) v1 ==== 393+0+0 (1304446436 0 0) 0x7f4bd80012c0 con 0x7f4be405f330
2016-05-19 08:40:53.449362 7f4be9403700  1 -- 136.243.49.217:0/3204162468 --> 136.243.49.217:6789/0 -- mon_subscribe({monmap=0+}) v2 -- ?+0 0x7f4be4060da0 con 0x7f4be405f330
2016-05-19 08:40:53.449453 7f4bebb81700  1 -- 136.243.49.217:0/3204162468 --> 136.243.49.217:6789/0 -- mon_subscribe({monmap=2+,osdmap=0}) v2 -- ?+0 0x7f4be4060da0 con 0x7f4be405f330
2016-05-19 08:40:53.449559 7f4bebb81700  1 -- 136.243.49.217:0/3204162468 --> 136.243.49.217:6789/0 -- mon_subscribe({monmap=2+,osdmap=0}) v2 -- ?+0 0x7f4be405c150 con 0x7f4be405f330
2016-05-19 08:40:53.449690 7f4be9403700  1 -- 136.243.49.217:0/3204162468 <== mon.0 136.243.49.217:6789/0 5 ==== mon_map magic: 0 v1 ==== 195+0+0 (359242282 0 0) 0x7f4bd8001180 con 0x7f4be405f330
2016-05-19 08:40:53.449745 7f4be9403700  1 -- 136.243.49.217:0/3204162468 <== mon.0 136.243.49.217:6789/0 6 ==== mon_subscribe_ack(300s) v1 ==== 20+0+0 (488052265 0 0) 0x7f4bd8001500 con 0x7f4be405f330
2016-05-19 08:40:53.449918 7f4be9403700  1 -- 136.243.49.217:0/3204162468 <== mon.0 136.243.49.217:6789/0 7 ==== osd_map(143..143 src has 1..143) v3 ==== 7861+0+0 (1308219935 0 0) 0x7f4bd8003040 con 0x7f4be405f330
2016-05-19 08:40:53.450103 7f4be9403700  1 -- 136.243.49.217:0/3204162468 <== mon.0 136.243.49.217:6789/0 8 ==== mon_subscribe_ack(300s) v1 ==== 20+0+0 (488052265 0 0) 0x7f4bd80033b0 con 0x7f4be405f330
2016-05-19 08:40:53.450125 7f4be9403700  1 -- 136.243.49.217:0/3204162468 <== mon.0 136.243.49.217:6789/0 9 ==== osd_map(143..143 src has 1..143) v3 ==== 7861+0+0 (1308219935 0 0) 0x7f4bd8005650 con 0x7f4be405f330
2016-05-19 08:40:53.450145 7f4be9403700  1 -- 136.243.49.217:0/3204162468 <== mon.0 136.243.49.217:6789/0 10 ==== mon_subscribe_ack(300s) v1 ==== 20+0+0 (488052265 0 0) 0x7f4bd8005b70 con 0x7f4be405f330
2016-05-19 08:40:53.452564 7f4bebb81700  1 -- 136.243.49.217:0/3204162468 --> 136.243.49.217:6789/0 -- mon_command({"prefix": "get_command_descriptions"} v 0) v1 -- ?+0 0x7f4be404d330 con 0x7f4be405f330
2016-05-19 08:40:53.455469 7f4be9403700  1 -- 136.243.49.217:0/3204162468 <== mon.0 136.243.49.217:6789/0 11 ==== mon_command_ack([{"prefix": "get_command_descriptions"}]=0  v0) v1 ==== 72+0+36815 (1092875540 0 1215666908) 0x7f4bd8005b70 con 0x7f4be405f330
2016-05-19 08:40:53.520242 7f4bebb81700  1 -- 136.243.49.217:0/3204162468 --> 136.243.49.217:6789/0 -- mon_command({"prefix": "status"} v 0) v1 -- ?+0 0x7f4be404d330 con 0x7f4be405f330
2016-05-19 08:40:53.520918 7f4be9403700  1 -- 136.243.49.217:0/3204162468 <== mon.0 136.243.49.217:6789/0 12 ==== mon_command_ack([{"prefix": "status"}]=0  v0) v1 ==== 54+0+436 (1155462804 0 1519386669) 0x7f4bd8001370 con 0x7f4be405f330
    cluster 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
     health HEALTH_OK
     monmap e1: 1 mons at {node1=136.243.49.217:6789/0}
            election epoch 1, quorum 0 node1
     osdmap e143: 4 osds: 2 up, 2 in
            flags sortbitwise
      pgmap v416783: 466 pgs, 16 pools, 72657 MB data, 18392 objects
            151 GB used, 10378 GB / 11089 GB avail
                 466 active+clean
  client io 0 B/s rd, 12442 B/s wr, 3 op/s
2016-05-19 08:40:53.522344 7f4bebb81700  1 -- 136.243.49.217:0/3204162468 mark_down 0x7f4be405f330 -- 0x7f4be4062550
2016-05-19 08:40:53.522395 7f4bebb81700  1 -- 136.243.49.217:0/3204162468 mark_down_all
2016-05-19 08:40:53.522802 7f4bebb81700  1 -- 136.243.49.217:0/3204162468 shutdown complete.
megdc@node1:~/ceph-cluster$ sudo nano /etc/ceph/ceph.conf 
megdc@node1:~/ceph-cluster$ ceph status
    cluster 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
     health HEALTH_OK
     monmap e1: 1 mons at {node1=136.243.49.217:6789/0}
            election epoch 1, quorum 0 node1
     osdmap e143: 4 osds: 2 up, 2 in
            flags sortbitwise
      pgmap v416787: 466 pgs, 16 pools, 72657 MB data, 18392 objects
            151 GB used, 10378 GB / 11089 GB avail
                 466 active+clean
  client io 0 B/s rd, 13105 B/s wr, 4 op/s
megdc@node1:~/ceph-cluster$ netstat -a | grep ceph
unix  2      [ ACC ]     STREAM     LISTENING     9807     /var/run/ceph/ceph-mon.node1.asok
unix  2      [ ACC ]     STREAM     LISTENING     19596    /var/run/ceph/ceph-osd.0.asok
unix  2      [ ACC ]     STREAM     LISTENING     9846     /var/run/ceph/ceph-osd.1.asok
megdc@node1:~/ceph-cluster$ sudo /etc/init.d/ceph -a start osd.2
=== osd.2 === 
root@node2's password: 
Permission denied, please try again.
root@node2's password: 
megdc@node1:~/ceph-cluster$ sudo /etc/init.d/ceph -a start osd.2
=== osd.2 === 
root@node2's password: 
root@node2's password: 
df: ‘/var/lib/ceph/osd/ceph-2/.’: No such file or directory
root@node2's password: 
Connection closed by 138.201.21.215
failed: 'ssh node2 timeout 30 /usr/bin/ceph -c /etc/ceph/ceph.conf --name=osd.2 --keyring=/var/lib/ceph/osd/ceph-2/keyring osd crush create-or-move -- 2 1 host=node1 root=default'
megdc@node1:~/ceph-cluster$ ceph status
    cluster 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
     health HEALTH_OK
     monmap e1: 1 mons at {node1=136.243.49.217:6789/0}
            election epoch 1, quorum 0 node1
     osdmap e143: 4 osds: 2 up, 2 in
            flags sortbitwise
      pgmap v416942: 466 pgs, 16 pools, 72657 MB data, 18392 objects
            151 GB used, 10378 GB / 11089 GB avail
                 466 active+clean
  client io 0 B/s rd, 13378 B/s wr, 5 op/s
megdc@node1:~/ceph-cluster$ ceph osd rm 2 3
removed osd.2, osd.3
megdc@node1:~/ceph-cluster$ ceph status
    cluster 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
     health HEALTH_OK
     monmap e1: 1 mons at {node1=136.243.49.217:6789/0}
            election epoch 1, quorum 0 node1
     osdmap e144: 2 osds: 2 up, 2 in
            flags sortbitwise
      pgmap v416946: 466 pgs, 16 pools, 72657 MB data, 18392 objects
            151 GB used, 10378 GB / 11089 GB avail
                 466 active+clean
  client io 35205 B/s rd, 1 op/s
megdc@node1:~/ceph-cluster$ ceph osd tree 
ID WEIGHT   TYPE NAME               UP/DOWN REWEIGHT PRIMARY-AFFINITY 
-5        0 rack unknownrack                                          
-4        0     host 138.201.21.215                                   
-1 21.64975 root default                                              
-2 10.82977     host node1                                            
 0  5.41489         osd.0                up  1.00000          1.00000 
 1  5.41489         osd.1                up  1.00000          1.00000 
-3 10.81998     host node2                                            
 2  5.40999         osd.2               DNE        0                  
 3  5.40999         osd.3               DNE        0                  
megdc@node1:~/ceph-cluster$ ceph-deploy --overwrite-conf osd prepare node2:/storage1/osd node2:/storage2/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy --overwrite-conf osd prepare node2:/storage1/osd node2:/storage2/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage1/osd', None), ('node2', '/storage2/osd', None)]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe7e7d65d40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fe7e81c4500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks node2:/storage1/osd: node2:/storage2/osd:
megdc@node2's password: 
[node2][DEBUG ] connection detected need for sudo
megdc@node2's password: 
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to node2
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[node2][WARNIN] osd keyring does not exist yet, creating one
[node2][DEBUG ] create a keyring file
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage1/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage1/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Preparing osd data dir /storage1/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /bin/chown -R ceph:ceph /storage1/osd/ceph_fsid.23183.tmp
[node2][WARNIN] INFO:ceph-disk:Running command: /bin/chown -R ceph:ceph /storage1/osd/fsid.23183.tmp
[node2][WARNIN] INFO:ceph-disk:Running command: /bin/chown -R ceph:ceph /storage1/osd/magic.23183.tmp
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
megdc@node2's password: 
[node2][DEBUG ] connection detected need for sudo
megdc@node2's password: 
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage2/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage2/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Preparing osd data dir /storage2/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /bin/chown -R ceph:ceph /storage2/osd/ceph_fsid.23358.tmp
[node2][WARNIN] INFO:ceph-disk:Running command: /bin/chown -R ceph:ceph /storage2/osd/fsid.23358.tmp
[node2][WARNIN] INFO:ceph-disk:Running command: /bin/chown -R ceph:ceph /storage2/osd/magic.23358.tmp
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
megdc@node1:~/ceph-cluster$ ssh node2
megdc@node2's password: 
Welcome to Ubuntu 14.04.4 LTS (GNU/Linux 4.2.0-35-generic x86_64)

 * Documentation:  https://help.ubuntu.com/
Last login: Mon May  9 16:20:01 2016 from node1
megdc@node2:~$ exit
logout
Connection to node2 closed.
megdc@node1:~/ceph-cluster$ cd ..
megdc@node1:~$ ls -la
total 72
drwxr-xr-x 5 megdc megdc  4096 May  9 16:36 .
drwxr-xr-x 3 root  root   4096 Apr 28 16:00 ..
-rw------- 1 megdc megdc 19646 May 18 18:12 .bash_history
-rw-r--r-- 1 megdc megdc   220 Mar  5  2014 .bash_logout
-rw-r--r-- 1 megdc megdc  3637 Mar  5  2014 .bashrc
drwx------ 2 megdc megdc  4096 Apr 29 10:29 .cache
drwxrwxr-x 2 megdc megdc  4096 May 18 14:48 ceph-cluster
-rw-rw-r-- 1 megdc megdc  1077 Apr 28 16:00 .cephdeploy.conf
-rw-rw-r-- 1 megdc megdc 10995 May 18 12:45 ceph.log
-rw-r--r-- 1 megdc megdc   675 Mar  5  2014 .profile
drwx------ 2 megdc root   4096 May  9 16:12 .ssh
megdc@node1:~$ ls -la .ssh/
total 32
drwx------ 2 megdc root  4096 May  9 16:12 .
drwxr-xr-x 5 megdc megdc 4096 May  9 16:36 ..
-rw-r--r-- 1 megdc megdc  393 Apr 29 17:11 authorized_keys
-rwxr-xr-x 1 megdc root    80 May  9 16:37 config
-rw------- 1 megdc megdc 1675 Apr 29 17:11 id_rsa
-rw-r--r-- 1 megdc megdc  393 Apr 29 17:11 id_rsa.pub
-rw-r--r-- 1 megdc megdc  363 May 10 08:28 known_hosts
-rwxr-xr-x 1 megdc root    54 Apr 29 17:11 ssh_config
megdc@node1:~$ cat .ssh/authorized_keys 
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCuHDW7rQXDbpoLz428RQFfmBWFUYMZo7cz6KNh5Phmw2kmEIZFo7mZjm4z2XNxw/ENAsAONIqYyIruCXpHdh1iBB816xTdjA5L9sxw6DIH6NXGJzTREjBzBN5vKZrOHLaac/ZdA8TjkpZlQsCxHMMMxTD8SwPtXOxt8/7mFDZBy4FqiW5NCqvlCr9sOxXUbHFzVtMmwJkHo/5ZIP6zXrrroHeUhVZSe2eraTmO70Wyh6xPVg7dcH2XX9qRI2F7VHMChJnNXDKJsVDcBAlHysBRkkza2RdQrzPLSuB1xzXXli/1vo6rw0jRRRc3JrdrAGSebWvWKpm1JB/cGBInA3oP megdc@node1
megdc@node1:~$ cat .ssh/id_rsa.pub 
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCuHDW7rQXDbpoLz428RQFfmBWFUYMZo7cz6KNh5Phmw2kmEIZFo7mZjm4z2XNxw/ENAsAONIqYyIruCXpHdh1iBB816xTdjA5L9sxw6DIH6NXGJzTREjBzBN5vKZrOHLaac/ZdA8TjkpZlQsCxHMMMxTD8SwPtXOxt8/7mFDZBy4FqiW5NCqvlCr9sOxXUbHFzVtMmwJkHo/5ZIP6zXrrroHeUhVZSe2eraTmO70Wyh6xPVg7dcH2XX9qRI2F7VHMChJnNXDKJsVDcBAlHysBRkkza2RdQrzPLSuB1xzXXli/1vo6rw0jRRRc3JrdrAGSebWvWKpm1JB/cGBInA3oP megdc@node1
megdc@node1:~$ ssh node2
Welcome to Ubuntu 14.04.4 LTS (GNU/Linux 4.2.0-35-generic x86_64)

 * Documentation:  https://help.ubuntu.com/
Last login: Thu May 19 09:08:35 2016 from node1
megdc@node2:~$ exit
logout
Connection to node2 closed.
megdc@node1:~$ cd ceph-cluster/
megdc@node1:~/ceph-cluster$ ls
ceph.bootstrap-mds.keyring  ceph.client.admin.keyring    ceph.conf         client.libvirt.key  s3.py       tmpuAA45G
ceph.bootstrap-osd.keyring  ceph.client.libvirt.keyring  ceph.log          rbdmap              secret.xml  uid
ceph.bootstrap-rgw.keyring  ceph.client.radosgw.keyring  ceph.mon.keyring  release.asc         tmpU4w_cD
megdc@node1:~/ceph-cluster$ scp /etc/ceph/ceph.conf megdc@node2:/etc/ceph/ceph.conf
scp: /etc/ceph/ceph.conf: Permission denied
megdc@node1:~/ceph-cluster$ sudo scp /etc/ceph/ceph.conf megdc@node2:/etc/ceph/ceph.conf
megdc@node2's password: 
scp: /etc/ceph/ceph.conf: Permission denied
megdc@node1:~/ceph-cluster$ sudo scp /etc/ceph/ceph.conf megdc@node2:/home/megdc/ceph.conf
megdc@node2's password: 
ceph.conf                                                                                                    100%  953     0.9KB/s   00:00    
megdc@node1:~/ceph-cluster$ ssh node2 'sudo cp ceph.conf /etc/ceph/ceph.conf'
megdc@node1:~/ceph-cluster$ ceph-deploy osd activate node2:/storage1/osd node2:/storage2/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy osd activate node2:/storage1/osd node2:/storage2/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : activate
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f131e174d40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f131e5d3500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage1/osd', None), ('node2', '/storage2/osd', None)]
[ceph_deploy.osd][DEBUG ] Activating cluster ceph disks node2:/storage1/osd: node2:/storage2/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] activating host node2 disk /storage1/osd
[ceph_deploy.osd][DEBUG ] will use init type: upstart
[node2][INFO  ] Running command: sudo ceph-disk -v activate --mark-init upstart --mount /storage1/osd
[node2][WARNIN] DEBUG:ceph-disk:Cluster uuid is 271ea3ed-a709-4dae-8612-e172945efc72
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] Traceback (most recent call last):
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 3589, in <module>
[node2][WARNIN]     main(sys.argv[1:])
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 3543, in main
[node2][WARNIN]     args.func(args)
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 2446, in main_activate
[node2][WARNIN]     init=args.mark_init,
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 2272, in activate_dir
[node2][WARNIN]     (osd_id, cluster) = activate(path, activate_key_template, init)
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 2345, in activate
[node2][WARNIN]     raise Error('No cluster conf found in ' + SYSCONFDIR + ' with fsid %s' % ceph_fsid)
[node2][WARNIN] __main__.Error: Error: No cluster conf found in /etc/ceph with fsid 271ea3ed-a709-4dae-8612-e172945efc72
[node2][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: ceph-disk -v activate --mark-init upstart --mount /storage1/osd

megdc@node1:~/ceph-cluster$ cat /storage2/osd/fsid 
a8b70304-c760-44e1-86bd-81c8207d26a8
megdc@node1:~/ceph-cluster$ cat /storage1/osd/fsid 
597b1cb5-0f7a-47e5-a8f3-de6810bcff85
megdc@node1:~/ceph-cluster$ ceph-deploy uninstall node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy uninstall node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2f02ae5c68>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  func                          : <function uninstall at 0x7f2f033740c8>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[ceph_deploy.install][INFO  ] like: librbd1 and librados2
[ceph_deploy.install][DEBUG ] Uninstalling on cluster ceph hosts node2
[ceph_deploy.install][DEBUG ] Detecting platform for host node2 ...
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] Uninstalling Ceph on node2
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove ceph ceph-mds ceph-common ceph-fs-common radosgw
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] Package 'radosgw' is not installed, so not removed
[node2][DEBUG ] The following packages were automatically installed and are no longer required:
[node2][DEBUG ]   ceph-fuse cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
[node2][DEBUG ]   libcryptsetup4 libgoogle-perftools4 libjs-jquery libleveldb1
[node2][DEBUG ]   libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-blinker
[node2][DEBUG ]   python-cephfs python-flask python-itsdangerous python-openssl
[node2][DEBUG ]   python-pyinotify python-rados python-rbd python-werkzeug
[node2][DEBUG ] Use 'apt-get autoremove' to remove them.
[node2][DEBUG ] The following packages will be REMOVED:
[node2][DEBUG ]   ceph ceph-common ceph-fs-common ceph-mds
[node2][DEBUG ] 0 upgraded, 0 newly installed, 4 to remove and 39 not upgraded.
[node2][DEBUG ] After this operation, 139 MB disk space will be freed.
[node2][DEBUG ] (Reading database ... 37296 files and directories currently installed.)
[node2][DEBUG ] Removing ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-mds-all stop/waiting
[node2][DEBUG ] === mds.b === 
[node2][DEBUG ] Stopping Ceph mds.b on node2...done
[node2][DEBUG ] Removing ceph (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-all stop/waiting
[node2][DEBUG ] === osd.3 === 
[node2][DEBUG ] Stopping Ceph osd.3 on node2...done
[node2][DEBUG ] === osd.2 === 
[node2][DEBUG ] Stopping Ceph osd.2 on node2...done
[node2][DEBUG ] === mds.b === 
[node2][DEBUG ] Stopping Ceph mds.b on node2...done
[node2][DEBUG ] === mon.b === 
[node2][DEBUG ] Stopping Ceph mon.b on node2...done
[node2][DEBUG ] Removing ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Removing ceph-fs-common (9.2.1-1trusty) ...
[node2][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[node2][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.7) ...
megdc@node1:~/ceph-cluster$ ceph-deploy purgedata node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy purgedata node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5864ae8998>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f58653f01b8>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts node2
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] purging data on node2
[node2][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[node2][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
megdc@node1:~/ceph-cluster$ ceph-deploy purge node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy purge node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f222acb6320>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f222b5be140>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[ceph_deploy.install][INFO  ] like: librbd1 and librados2
[ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts node2
[ceph_deploy.install][DEBUG ] Detecting platform for host node2 ...
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] Purging Ceph on node2
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[node2][DEBUG ] Package 'radosgw' is not installed, so not removed
[node2][DEBUG ] The following packages were automatically installed and are no longer required:
[node2][DEBUG ]   ceph-fuse cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
[node2][DEBUG ]   libcryptsetup4 libgoogle-perftools4 libjs-jquery libleveldb1
[node2][DEBUG ]   libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-blinker
[node2][DEBUG ]   python-cephfs python-flask python-itsdangerous python-openssl
[node2][DEBUG ]   python-pyinotify python-rados python-rbd python-werkzeug
[node2][DEBUG ] Use 'apt-get autoremove' to remove them.
[node2][DEBUG ] The following packages will be REMOVED:
[node2][DEBUG ]   ceph* ceph-common* ceph-mds*
[node2][DEBUG ] 0 upgraded, 0 newly installed, 3 to remove and 39 not upgraded.
[node2][DEBUG ] After this operation, 0 B of additional disk space will be used.
[node2][DEBUG ] (Reading database ... 37009 files and directories currently installed.)
[node2][DEBUG ] Removing ceph (9.2.1-1trusty) ...
[node2][DEBUG ] Purging configuration files for ceph (9.2.1-1trusty) ...
[node2][DEBUG ] Removing ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Purging configuration files for ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Removing ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] Purging configuration files for ceph-mds (9.2.1-1trusty) ...
megdc@node1:~/ceph-cluster$ ceph status
    cluster 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
     health HEALTH_OK
     monmap e1: 1 mons at {node1=136.243.49.217:6789/0}
            election epoch 1, quorum 0 node1
     osdmap e144: 2 osds: 2 up, 2 in
            flags sortbitwise
      pgmap v417178: 466 pgs, 16 pools, 72697 MB data, 18402 objects
            151 GB used, 10378 GB / 11089 GB avail
                 466 active+clean
  client io 0 B/s rd, 41771 B/s wr, 13 op/s
megdc@node1:~/ceph-cluster$ ceph-deploy --overwrite-conf osd prepare node2:/storage1/osd node2:/storage2/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy --overwrite-conf osd prepare node2:/storage1/osd node2:/storage2/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage1/osd', None), ('node2', '/storage2/osd', None)]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f493dbf6d40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f493e055500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks node2:/storage1/osd: node2:/storage2/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to node2
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[node2][WARNIN] osd keyring does not exist yet, creating one
[node2][DEBUG ] create a keyring file
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage1/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage1/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] Traceback (most recent call last):
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 3589, in <module>
[node2][WARNIN]     main(sys.argv[1:])
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 3543, in main
[node2][WARNIN]     args.func(args)
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 1719, in main_prepare
[node2][WARNIN]     raise Error('data path does not exist', args.data)
[node2][WARNIN] __main__.Error: Error: data path does not exist: /storage1/osd
[node2][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage1/osd
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage2/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage2/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] Traceback (most recent call last):
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 3589, in <module>
[node2][WARNIN]     main(sys.argv[1:])
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 3543, in main
[node2][WARNIN]     args.func(args)
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 1719, in main_prepare
[node2][WARNIN]     raise Error('data path does not exist', args.data)
[node2][WARNIN] __main__.Error: Error: data path does not exist: /storage2/osd
[node2][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy.osd][ERROR ] Failed to execute command: ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage2/osd
[ceph_deploy][ERROR ] GenericError: Failed to create 2 OSDs

megdc@node1:~/ceph-cluster$ ceph-disk list node2
Traceback (most recent call last):
  File "/usr/sbin/ceph-disk", line 3589, in <module>
    main(sys.argv[1:])
  File "/usr/sbin/ceph-disk", line 3539, in main
    setup_statedir(args.statedir)
  File "/usr/sbin/ceph-disk", line 3200, in setup_statedir
    os.mkdir(STATEDIR + "/tmp")
OSError: [Errno 13] Permission denied: '/var/lib/ceph/tmp'
megdc@node1:~/ceph-cluster$ ceph-deploy disk list node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy disk list node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : list
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc0b2f34638>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fc0b3398578>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', None, None)]
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Listing disks on node2...
[node2][DEBUG ] find the location of an executable
[node2][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[node2][DEBUG ] /dev/loop0 other, unknown
[node2][DEBUG ] /dev/loop1 other, unknown
[node2][DEBUG ] /dev/loop2 other, unknown
[node2][DEBUG ] /dev/loop3 other, unknown
[node2][DEBUG ] /dev/loop4 other, unknown
[node2][DEBUG ] /dev/loop5 other, unknown
[node2][DEBUG ] /dev/loop6 other, unknown
[node2][DEBUG ] /dev/loop7 other, unknown
[node2][DEBUG ] /dev/md0 other, ext3, mounted on /boot
[node2][DEBUG ] /dev/md1 swap, swap
[node2][DEBUG ] /dev/md2 other, ext4, mounted on /
[node2][DEBUG ] /dev/ram0 other, unknown
[node2][DEBUG ] /dev/ram1 other, unknown
[node2][DEBUG ] /dev/ram10 other, unknown
[node2][DEBUG ] /dev/ram11 other, unknown
[node2][DEBUG ] /dev/ram12 other, unknown
[node2][DEBUG ] /dev/ram13 other, unknown
[node2][DEBUG ] /dev/ram14 other, unknown
[node2][DEBUG ] /dev/ram15 other, unknown
[node2][DEBUG ] /dev/ram2 other, unknown
[node2][DEBUG ] /dev/ram3 other, unknown
[node2][DEBUG ] /dev/ram4 other, unknown
[node2][DEBUG ] /dev/ram5 other, unknown
[node2][DEBUG ] /dev/ram6 other, unknown
[node2][DEBUG ] /dev/ram7 other, unknown
[node2][DEBUG ] /dev/ram8 other, unknown
[node2][DEBUG ] /dev/ram9 other, unknown
[node2][DEBUG ] /dev/sda :
[node2][DEBUG ]  /dev/sda1 other, linux_raid_member
[node2][DEBUG ]  /dev/sda2 other, linux_raid_member
[node2][DEBUG ]  /dev/sda3 other, linux_raid_member
[node2][DEBUG ] /dev/sdb :
[node2][DEBUG ]  /dev/sdb1 other, linux_raid_member
[node2][DEBUG ]  /dev/sdb2 other, linux_raid_member
[node2][DEBUG ]  /dev/sdb3 other, linux_raid_member
[node2][DEBUG ] /dev/sdc :
[node2][DEBUG ]  /dev/sdc1 other, ext4, mounted on /storage3
[node2][DEBUG ] /dev/sdd :
[node2][DEBUG ]  /dev/sdd1 other, ext4, mounted on /storage4
megdc@node1:~/ceph-cluster$ ceph-deploy --overwrite-conf osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy --overwrite-conf osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa6333c6d40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fa633825500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to node2
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage3/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage3/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Data dir /storage3/osd already exists
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage4/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage4/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Data dir /storage4/osd already exists
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
megdc@node1:~/ceph-cluster$ ceph-deploy osd activate node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy osd activate node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : activate
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f43c98bfd40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f43c9d1e500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.osd][DEBUG ] Activating cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] activating host node2 disk /storage3/osd
[ceph_deploy.osd][DEBUG ] will use init type: upstart
[node2][INFO  ] Running command: sudo ceph-disk -v activate --mark-init upstart --mount /storage3/osd
[node2][WARNIN] DEBUG:ceph-disk:Cluster uuid is 271ea3ed-a709-4dae-8612-e172945efc72
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] DEBUG:ceph-disk:Cluster name is ceph
[node2][WARNIN] DEBUG:ceph-disk:OSD uuid is cd9acdb3-54a9-4368-92cb-523de6df1e4f
[node2][WARNIN] DEBUG:ceph-disk:Allocating OSD id...
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise cd9acdb3-54a9-4368-92cb-523de6df1e4f
[node2][WARNIN] No data was received after 300 seconds, disconnecting...
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] activating host node2 disk /storage4/osd
[ceph_deploy.osd][DEBUG ] will use init type: upstart
[node2][INFO  ] Running command: sudo ceph-disk -v activate --mark-init upstart --mount /storage4/osd
[node2][WARNIN] DEBUG:ceph-disk:Cluster uuid is 271ea3ed-a709-4dae-8612-e172945efc72
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] DEBUG:ceph-disk:Cluster name is ceph
[node2][WARNIN] DEBUG:ceph-disk:OSD uuid is 47f306ad-27e9-457b-97e3-7cac058b1638
[node2][WARNIN] DEBUG:ceph-disk:Allocating OSD id...
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 47f306ad-27e9-457b-97e3-7cac058b1638


[node2][WARNIN] No data was received after 300 seconds, disconnecting...
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
megdc@node1:~/ceph-cluster$ 
megdc@node1:~/ceph-cluster$ 
megdc@node1:~/ceph-cluster$ ceph status
    cluster 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
     health HEALTH_OK
     monmap e1: 1 mons at {node1=136.243.49.217:6789/0}
            election epoch 1, quorum 0 node1
     osdmap e144: 2 osds: 2 up, 2 in
            flags sortbitwise
      pgmap v417407: 466 pgs, 16 pools, 73487 MB data, 18602 objects
            153 GB used, 10377 GB / 11089 GB avail
                 466 active+clean
  client io 907 kB/s rd, 3868 B/s wr, 10 op/s
megdc@node1:~/ceph-cluster$ cat /storage1/osd/ceph_fsid 
6773ed61-b6c1-48a4-9be2-c6840e0de9e7
megdc@node1:~/ceph-cluster$ cat /storage2/osd/ceph_fsid 
6773ed61-b6c1-48a4-9be2-c6840e0de9e7
megdc@node1:~/ceph-cluster$ ls
ceph.bootstrap-mds.keyring  ceph.client.admin.keyring    ceph.conf         client.libvirt.key  s3.py       tmpuAA45G
ceph.bootstrap-osd.keyring  ceph.client.libvirt.keyring  ceph.log          rbdmap              secret.xml  uid
ceph.bootstrap-rgw.keyring  ceph.client.radosgw.keyring  ceph.mon.keyring  release.asc         tmpU4w_cD
megdc@node1:~/ceph-cluster$ sudo scp /etc/ceph/ceph.conf megdc@node2:/home/megdc/ceph.conf
megdc@node2's password: 
ceph.conf                                                                                                    100%  953     0.9KB/s   00:00    
megdc@node1:~/ceph-cluster$ ssh node2 'sudo mv ceph.conf /ect/ceph/ceph.conf'
mv: cannot move ‘ceph.conf’ to ‘/ect/ceph/ceph.conf’: No such file or directory
megdc@node1:~/ceph-cluster$ ceph-deploy --overwrite-conf osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy --overwrite-conf osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2084f5ed40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f20853bd500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to node2
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[node2][WARNIN] osd keyring does not exist yet, creating one
[node2][DEBUG ] create a keyring file
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage3/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage3/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Preparing osd data dir /storage3/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /bin/chown -R ceph:ceph /storage3/osd/ceph_fsid.7148.tmp
[node2][WARNIN] INFO:ceph-disk:Running command: /bin/chown -R ceph:ceph /storage3/osd/fsid.7148.tmp
[node2][WARNIN] INFO:ceph-disk:Running command: /bin/chown -R ceph:ceph /storage3/osd/magic.7148.tmp
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage4/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage4/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Preparing osd data dir /storage4/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /bin/chown -R ceph:ceph /storage4/osd/ceph_fsid.7249.tmp
[node2][WARNIN] INFO:ceph-disk:Running command: /bin/chown -R ceph:ceph /storage4/osd/fsid.7249.tmp
[node2][WARNIN] INFO:ceph-disk:Running command: /bin/chown -R ceph:ceph /storage4/osd/magic.7249.tmp
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
megdc@node1:~/ceph-cluster$ ceph-deploy osd activate node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy osd activate node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : activate
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7efd8244fd40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7efd828ae500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.osd][DEBUG ] Activating cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] activating host node2 disk /storage3/osd
[ceph_deploy.osd][DEBUG ] will use init type: upstart
[node2][INFO  ] Running command: sudo ceph-disk -v activate --mark-init upstart --mount /storage3/osd
[node2][WARNIN] DEBUG:ceph-disk:Cluster uuid is 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] Traceback (most recent call last):
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 3589, in <module>
[node2][WARNIN]     main(sys.argv[1:])
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 3543, in main
[node2][WARNIN]     args.func(args)
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 2446, in main_activate
[node2][WARNIN]     init=args.mark_init,
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 2272, in activate_dir
[node2][WARNIN]     (osd_id, cluster) = activate(path, activate_key_template, init)
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 2345, in activate
[node2][WARNIN]     raise Error('No cluster conf found in ' + SYSCONFDIR + ' with fsid %s' % ceph_fsid)
[node2][WARNIN] __main__.Error: Error: No cluster conf found in /etc/ceph with fsid 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
[node2][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: ceph-disk -v activate --mark-init upstart --mount /storage3/osd

megdc@node1:~/ceph-cluster$ ceph-deploy osd activate node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy osd activate node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : activate
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f32b1e11d40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f32b2270500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.osd][DEBUG ] Activating cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] activating host node2 disk /storage3/osd
[ceph_deploy.osd][DEBUG ] will use init type: upstart
[node2][INFO  ] Running command: sudo ceph-disk -v activate --mark-init upstart --mount /storage3/osd
[node2][WARNIN] DEBUG:ceph-disk:Cluster uuid is 271ea3ed-a709-4dae-8612-e172945efc72
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] DEBUG:ceph-disk:Cluster name is ceph
[node2][WARNIN] DEBUG:ceph-disk:OSD uuid is 85169284-fab7-40df-acdb-928436978d82
[node2][WARNIN] DEBUG:ceph-disk:Allocating OSD id...
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 85169284-fab7-40df-acdb-928436978d82
[node2][WARNIN] No data was received after 300 seconds, disconnecting...
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] activating host node2 disk /storage4/osd
[ceph_deploy.osd][DEBUG ] will use init type: upstart
[node2][INFO  ] Running command: sudo ceph-disk -v activate --mark-init upstart --mount /storage4/osd
[node2][WARNIN] DEBUG:ceph-disk:Cluster uuid is 271ea3ed-a709-4dae-8612-e172945efc72
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] DEBUG:ceph-disk:Cluster name is ceph
[node2][WARNIN] DEBUG:ceph-disk:OSD uuid is c9356e33-5ce1-4d55-9e51-a8392459517d
[node2][WARNIN] DEBUG:ceph-disk:Allocating OSD id...
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise c9356e33-5ce1-4d55-9e51-a8392459517d
[node2][WARNIN] No data was received after 300 seconds, disconnecting...
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
megdc@node1:~/ceph-cluster$ sudo scp /home/megdc/ceph-cluster/ceph.bootstrap-osd.keyring megdc@node2:/home/cibadmin/ceph.keyring
megdc@node2's password: 
scp: /home/cibadmin/ceph.keyring: No such file or directory
megdc@node1:~/ceph-cluster$ sudo scp /home/megdc/ceph-cluster/ceph.bootstrap-osd.keyring megdc@node2:/home/megdc/ceph.keyring
megdc@node2's password: 
ceph.bootstrap-osd.keyring                                                                                   100%   71     0.1KB/s   00:00    
megdc@node1:~/ceph-cluster$ cat ceph.bootstrap-osd.keyring 
[client.bootstrap-osd]
	key = AQCoayNXrPpFIhAAfcNRmR6pjmem2GspWwlwtQ==
megdc@node1:~/ceph-cluster$ sudo scp /etc/ceph/*.keyring megdc@node2:/home/megdc/
megdc@node2's password: 
ceph.bootstrap-mds.keyring                                                                                   100%   71     0.1KB/s   00:00    
ceph.bootstrap-osd.keyring                                                                                   100%   71     0.1KB/s   00:00    
ceph.bootstrap-rgw.keyring                                                                                   100%   71     0.1KB/s   00:00    
ceph.client.admin.keyring                                                                                    100%   63     0.1KB/s   00:00    
ceph.client.libvirt.keyring                                                                                  100%  165     0.2KB/s   00:00    
ceph.client.radosgw.keyring                                                                                  100%  119     0.1KB/s   00:00    
ceph.mon.keyring                                                                                             100%   73     0.1KB/s   00:00    
megdc@node1:~/ceph-cluster$ ssh megdc@node2 'sudo mv -r /home/megdc/*.keyring /etc/ceph/'
mv: invalid option -- 'r'
Try 'mv --help' for more information.
megdc@node1:~/ceph-cluster$ ssh megdc@node2 'sudo mv /home/megdc/*.keyring /etc/ceph/'
megdc@node1:~/ceph-cluster$ sudo scp /etc/ceph/*.conf megdc@node2:/home/megdc/
megdc@node2's password: 
scp: /home/megdc//ceph.conf: Permission denied
megdc@node1:~/ceph-cluster$ sudo scp /etc/ceph/*.conf megdc@node2:/home/megdc/
megdc@node2's password: 
ceph.conf                                                                                                    100%  953     0.9KB/s   00:00    
megdc@node1:~/ceph-cluster$ ceph-deploy
usage: ceph-deploy [-h] [-v | -q] [--version] [--username USERNAME]
                   [--overwrite-conf] [--cluster NAME] [--ceph-conf CEPH_CONF]
                   COMMAND ...

Easy Ceph deployment

    -^-
   /   \
   |O o|  ceph-deploy v1.5.31
   ).-.(
  '/|||\`
  | '|` |
    '|`

Full documentation can be found at: http://ceph.com/ceph-deploy/docs

optional arguments:
  -h, --help            show this help message and exit
  -v, --verbose         be more verbose
  -q, --quiet           be less verbose
  --version             the current installed version of ceph-deploy
  --username USERNAME   the username to connect to the remote host
  --overwrite-conf      overwrite an existing conf file on remote host (if
                        present)
  --cluster NAME        name of the cluster
  --ceph-conf CEPH_CONF
                        use (or reuse) a given ceph.conf file

commands:
  COMMAND               description
    new                 Start deploying a new cluster, and write a
                        CLUSTER.conf and keyring for it.
    install             Install Ceph packages on remote hosts.
    rgw                 Ceph RGW daemon management
    mon                 Ceph MON Daemon management
    mds                 Ceph MDS daemon management
    gatherkeys          Gather authentication keys for provisioning new nodes.
    disk                Manage disks on a remote host.
    osd                 Prepare a data disk on remote host.
    admin               Push configuration and client.admin key to a remote
                        host.
    repo                Repo definition management
    config              Copy ceph.conf to/from remote host(s)
    uninstall           Remove Ceph packages from remote hosts.
    purge               Remove Ceph packages from remote hosts and purge all
                        data.
    purgedata           Purge (delete, destroy, discard, shred) any Ceph data
                        from /var/lib/ceph
    calamari            Install and configure Calamari nodes. Assumes that a
                        repository with Calamari packages is already
                        configured. Refer to the docs for examples
                        (http://ceph.com/ceph-deploy/docs/conf.html)
    forgetkeys          Remove authentication keys from the local directory.
    pkg                 Manage packages on remote hosts.
megdc@node1:~/ceph-cluster$ ceph-deploy --overwrite-conf osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy --overwrite-conf osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f70d108cd40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f70d14eb500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][ERROR ] ceph needs to be installed in remote host: node2
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][ERROR ] ceph needs to be installed in remote host: node2
[ceph_deploy][ERROR ] GenericError: Failed to create 2 OSDs

megdc@node1:~/ceph-cluster$ ceph-deploy install node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy install node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  testing                       : None
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6612b7f0e0>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  dev_commit                    : None
[ceph_deploy.cli][INFO  ]  install_mds                   : False
[ceph_deploy.cli][INFO  ]  stable                        : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  adjust_repos                  : True
[ceph_deploy.cli][INFO  ]  func                          : <function install at 0x7f6613437de8>
[ceph_deploy.cli][INFO  ]  install_all                   : False
[ceph_deploy.cli][INFO  ]  repo                          : False
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  install_rgw                   : False
[ceph_deploy.cli][INFO  ]  install_tests                 : False
[ceph_deploy.cli][INFO  ]  repo_url                      : None
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  install_osd                   : False
[ceph_deploy.cli][INFO  ]  version_kind                  : stable
[ceph_deploy.cli][INFO  ]  install_common                : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  dev                           : master
[ceph_deploy.cli][INFO  ]  local_mirror                  : None
[ceph_deploy.cli][INFO  ]  release                       : None
[ceph_deploy.cli][INFO  ]  install_mon                   : False
[ceph_deploy.cli][INFO  ]  gpg_url                       : None
[ceph_deploy.install][DEBUG ] Installing stable version infernalis on cluster ceph hosts node2
[ceph_deploy.install][DEBUG ] Detecting platform for host node2 ...
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] installing Ceph on node2
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates apt-transport-https
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] apt-transport-https is already the newest version.
[node2][DEBUG ] ca-certificates is already the newest version.
[node2][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.
[node2][INFO  ] Running command: sudo wget -O release.asc https://download.ceph.com/keys/release.asc
[node2][WARNIN] --2016-05-19 11:02:05--  https://download.ceph.com/keys/release.asc
[node2][WARNIN] Resolving download.ceph.com (download.ceph.com)... 2607:f298:6050:51f3:f816:3eff:fe71:9135, 173.236.253.173
[node2][WARNIN] Connecting to download.ceph.com (download.ceph.com)|2607:f298:6050:51f3:f816:3eff:fe71:9135|:443... connected.
[node2][WARNIN] HTTP request sent, awaiting response... 200 OK
[node2][WARNIN] Length: 1645 (1.6K) [application/octet-stream]
[node2][WARNIN] Saving to: ‘release.asc’
[node2][WARNIN] 
[node2][WARNIN]      0K .                                                     100%  391M=0s
[node2][WARNIN] 
[node2][WARNIN] 2016-05-19 11:02:06 (391 MB/s) - ‘release.asc’ saved [1645/1645]
[node2][WARNIN] 
[node2][INFO  ] Running command: sudo apt-key add release.asc
[node2][DEBUG ] OK
[node2][DEBUG ] add deb repo to /etc/apt/sources.list.d/
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q update
[node2][DEBUG ] Ign http://mirror.hetzner.de trusty InRelease
[node2][DEBUG ] Get:1 http://mirror.hetzner.de trusty-backports InRelease [65.9 kB]
[node2][DEBUG ] Get:2 http://mirror.hetzner.de trusty-updates InRelease [65.9 kB]
[node2][DEBUG ] Ign http://de.archive.ubuntu.com trusty InRelease
[node2][DEBUG ] Get:3 http://security.ubuntu.com trusty-security InRelease [65.9 kB]
[node2][DEBUG ] Get:4 http://de.archive.ubuntu.com trusty-updates InRelease [65.9 kB]
[node2][DEBUG ] Get:5 http://de.archive.ubuntu.com trusty-backports InRelease [65.9 kB]
[node2][DEBUG ] Get:6 http://security.ubuntu.com trusty-security/main Sources [116 kB]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty Release.gpg
[node2][DEBUG ] Get:7 http://de.archive.ubuntu.com trusty-updates/main Sources [275 kB]
[node2][DEBUG ] Get:8 http://security.ubuntu.com trusty-security/restricted Sources [4,035 B]
[node2][DEBUG ] Get:9 http://de.archive.ubuntu.com trusty-updates/restricted Sources [5,352 B]
[node2][DEBUG ] Get:10 http://security.ubuntu.com trusty-security/universe Sources [36.2 kB]
[node2][DEBUG ] Get:11 http://de.archive.ubuntu.com trusty-updates/universe Sources [154 kB]
[node2][DEBUG ] Get:12 http://security.ubuntu.com trusty-security/multiverse Sources [2,760 B]
[node2][DEBUG ] Get:13 http://de.archive.ubuntu.com trusty-updates/multiverse Sources [5,939 B]
[node2][DEBUG ] Get:14 http://security.ubuntu.com trusty-security/main amd64 Packages [480 kB]
[node2][DEBUG ] Get:15 http://de.archive.ubuntu.com trusty-updates/main amd64 Packages [768 kB]
[node2][DEBUG ] Get:16 http://mirror.hetzner.de trusty-security InRelease [65.9 kB]
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty Release.gpg
[node2][DEBUG ] Get:17 http://de.archive.ubuntu.com trusty-updates/restricted amd64 Packages [15.9 kB]
[node2][DEBUG ] Get:18 http://security.ubuntu.com trusty-security/restricted amd64 Packages [13.0 kB]
[node2][DEBUG ] Get:19 http://de.archive.ubuntu.com trusty-updates/universe amd64 Packages [359 kB]
[node2][DEBUG ] Get:20 http://security.ubuntu.com trusty-security/universe amd64 Packages [128 kB]
[node2][DEBUG ] Get:21 http://de.archive.ubuntu.com trusty-updates/multiverse amd64 Packages [13.2 kB]
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty Release
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/restricted amd64 Packages
[node2][DEBUG ] Get:22 http://mirror.hetzner.de trusty-backports/universe amd64 Packages [52.3 kB]
[node2][DEBUG ] Get:23 http://security.ubuntu.com trusty-security/multiverse amd64 Packages [4,978 B]
[node2][DEBUG ] Get:24 http://de.archive.ubuntu.com trusty-updates/main i386 Packages [736 kB]
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/multiverse amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/main i386 Packages
[node2][DEBUG ] Get:25 http://security.ubuntu.com trusty-security/main i386 Packages [454 kB]
[node2][DEBUG ] Get:26 http://de.archive.ubuntu.com trusty-updates/restricted i386 Packages [15.6 kB]
[node2][DEBUG ] Get:27 http://security.ubuntu.com trusty-security/restricted i386 Packages [12.7 kB]
[node2][DEBUG ] Hit http://get.megam.io trusty InRelease
[node2][DEBUG ] Get:28 http://de.archive.ubuntu.com trusty-updates/universe i386 Packages [360 kB]
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/restricted i386 Packages
[node2][DEBUG ] Get:29 http://mirror.hetzner.de trusty-backports/universe i386 Packages [52.3 kB]
[node2][DEBUG ] Get:30 http://security.ubuntu.com trusty-security/universe i386 Packages [129 kB]
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/multiverse i386 Packages
[node2][DEBUG ] Get:31 http://mirror.hetzner.de trusty-updates/main amd64 Packages [964 kB]
[node2][DEBUG ] Get:32 http://de.archive.ubuntu.com trusty-updates/multiverse i386 Packages [13.6 kB]
[node2][DEBUG ] Get:33 http://security.ubuntu.com trusty-security/multiverse i386 Packages [5,168 B]
[node2][DEBUG ] Ign http://downloads.opennebula.org stable InRelease
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty Release
[node2][DEBUG ] Get:34 http://de.archive.ubuntu.com trusty-backports/main Sources [9,560 B]
[node2][DEBUG ] Get:35 http://de.archive.ubuntu.com trusty-backports/restricted Sources [28 B]
[node2][DEBUG ] Get:36 http://de.archive.ubuntu.com trusty-backports/universe Sources [34.8 kB]
[node2][DEBUG ] Get:37 http://de.archive.ubuntu.com trusty-backports/multiverse Sources [1,898 B]
[node2][DEBUG ] Get:38 http://de.archive.ubuntu.com trusty-backports/main amd64 Packages [13.0 kB]
[node2][DEBUG ] Get:39 http://de.archive.ubuntu.com trusty-backports/restricted amd64 Packages [28 B]
[node2][DEBUG ] Get:40 http://de.archive.ubuntu.com trusty-backports/universe amd64 Packages [43.0 kB]
[node2][DEBUG ] Get:41 http://de.archive.ubuntu.com trusty-backports/multiverse amd64 Packages [1,571 B]
[node2][DEBUG ] Get:42 http://de.archive.ubuntu.com trusty-backports/main i386 Packages [13.1 kB]
[node2][DEBUG ] Get:43 http://de.archive.ubuntu.com trusty-backports/restricted i386 Packages [28 B]
[node2][DEBUG ] Get:44 http://de.archive.ubuntu.com trusty-backports/universe i386 Packages [43.0 kB]
[node2][DEBUG ] Get:45 http://de.archive.ubuntu.com trusty-backports/multiverse i386 Packages [1,552 B]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/main Sources
[node2][DEBUG ] Hit http://downloads.opennebula.org stable Release.gpg
[node2][DEBUG ] Hit http://get.megam.io trusty/testing amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/restricted Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/universe Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/multiverse Sources
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/restricted amd64 Packages
[node2][DEBUG ] Get:46 http://mirror.hetzner.de trusty-updates/universe amd64 Packages [464 kB]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/main amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/restricted amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/universe amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/multiverse amd64 Packages
[node2][DEBUG ] Get:47 http://mirror.hetzner.de trusty-updates/main i386 Packages [925 kB]
[node2][DEBUG ] Hit https://download.ceph.com trusty InRelease
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/multiverse amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/main i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/restricted i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/universe i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/multiverse i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/restricted i386 Packages
[node2][DEBUG ] Get:48 http://mirror.hetzner.de trusty-updates/universe i386 Packages [466 kB]
[node2][DEBUG ] Hit https://download.ceph.com trusty/main amd64 Packages
[node2][DEBUG ] Hit http://downloads.opennebula.org stable Release
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/multiverse i386 Packages
[node2][DEBUG ] Hit https://download.ceph.com trusty/main i386 Packages
[node2][DEBUG ] Get:49 http://mirror.hetzner.de trusty-security/main amd64 Packages [602 kB]
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/restricted amd64 Packages
[node2][DEBUG ] Get:50 http://mirror.hetzner.de trusty-security/universe amd64 Packages [167 kB]
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/multiverse amd64 Packages
[node2][DEBUG ] Get:51 http://mirror.hetzner.de trusty-security/main i386 Packages [565 kB]
[node2][DEBUG ] Hit http://downloads.opennebula.org stable/opennebula amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/restricted i386 Packages
[node2][DEBUG ] Get:52 http://mirror.hetzner.de trusty-security/universe i386 Packages [167 kB]
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/multiverse i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/universe amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/multiverse amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/restricted i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/universe i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/multiverse i386 Packages
[node2][DEBUG ] Hit http://downloads.opennebula.org stable/opennebula i386 Packages
[node2][DEBUG ] Fetched 9,091 kB in 3s (2,846 kB/s)
[node2][DEBUG ] Reading package lists...
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew ceph ceph-mds radosgw
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] The following extra packages will be installed:
[node2][DEBUG ]   ceph-common cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
[node2][DEBUG ]   libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
[node2][DEBUG ]   libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
[node2][DEBUG ]   python-flask python-itsdangerous python-rados python-rbd python-werkzeug
[node2][DEBUG ] Suggested packages:
[node2][DEBUG ]   javascript-common python-flask-doc ipython python-genshi python-lxml
[node2][DEBUG ]   python-greenlet python-redis python-pylibmc python-memcache
[node2][DEBUG ]   python-werkzeug-doc
[node2][DEBUG ] Recommended packages:
[node2][DEBUG ]   ceph-fs-common ceph-fuse python-blinker python-openssl python-pyinotify
[node2][DEBUG ] The following NEW packages will be installed:
[node2][DEBUG ]   ceph ceph-common ceph-mds cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
[node2][DEBUG ]   libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
[node2][DEBUG ]   libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
[node2][DEBUG ]   python-flask python-itsdangerous python-rados python-rbd python-werkzeug
[node2][DEBUG ]   radosgw
[node2][DEBUG ] 0 upgraded, 25 newly installed, 0 to remove and 39 not upgraded.
[node2][DEBUG ] Need to get 0 B/39.7 MB of archives.
[node2][DEBUG ] After this operation, 174 MB of additional disk space will be used.
[node2][DEBUG ] Selecting previously unselected package libboost-program-options1.54.0:amd64.
[node2][DEBUG ] (Reading database ... 36619 files and directories currently installed.)
[node2][DEBUG ] Preparing to unpack .../libboost-program-options1.54.0_1.54.0-4ubuntu3.1_amd64.deb ...
[node2][DEBUG ] Unpacking libboost-program-options1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Selecting previously unselected package libboost-random1.54.0:amd64.
[node2][DEBUG ] Preparing to unpack .../libboost-random1.54.0_1.54.0-4ubuntu3.1_amd64.deb ...
[node2][DEBUG ] Unpacking libboost-random1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Selecting previously unselected package libsnappy1.
[node2][DEBUG ] Preparing to unpack .../libsnappy1_1.1.0-1ubuntu1_amd64.deb ...
[node2][DEBUG ] Unpacking libsnappy1 (1.1.0-1ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package libleveldb1:amd64.
[node2][DEBUG ] Preparing to unpack .../libleveldb1_1.15.0-2_amd64.deb ...
[node2][DEBUG ] Unpacking libleveldb1:amd64 (1.15.0-2) ...
[node2][DEBUG ] Selecting previously unselected package libunwind8.
[node2][DEBUG ] Preparing to unpack .../libunwind8_1.1-2.2ubuntu3_amd64.deb ...
[node2][DEBUG ] Unpacking libunwind8 (1.1-2.2ubuntu3) ...
[node2][DEBUG ] Selecting previously unselected package libbabeltrace1:amd64.
[node2][DEBUG ] Preparing to unpack .../libbabeltrace1_1.2.1-2_amd64.deb ...
[node2][DEBUG ] Unpacking libbabeltrace1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Selecting previously unselected package libbabeltrace-ctf1:amd64.
[node2][DEBUG ] Preparing to unpack .../libbabeltrace-ctf1_1.2.1-2_amd64.deb ...
[node2][DEBUG ] Unpacking libbabeltrace-ctf1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Selecting previously unselected package libradosstriper1.
[node2][DEBUG ] Preparing to unpack .../libradosstriper1_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking libradosstriper1 (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package python-rados.
[node2][DEBUG ] Preparing to unpack .../python-rados_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking python-rados (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libcephfs1.
[node2][DEBUG ] Preparing to unpack .../libcephfs1_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking libcephfs1 (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package python-cephfs.
[node2][DEBUG ] Preparing to unpack .../python-cephfs_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking python-cephfs (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package python-rbd.
[node2][DEBUG ] Preparing to unpack .../python-rbd_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking python-rbd (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package ceph-common.
[node2][DEBUG ] Preparing to unpack .../ceph-common_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libcryptsetup4.
[node2][DEBUG ] Preparing to unpack .../libcryptsetup4_2%3a1.6.1-1ubuntu1_amd64.deb ...
[node2][DEBUG ] Unpacking libcryptsetup4 (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package cryptsetup-bin.
[node2][DEBUG ] Preparing to unpack .../cryptsetup-bin_2%3a1.6.1-1ubuntu1_amd64.deb ...
[node2][DEBUG ] Unpacking cryptsetup-bin (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package libjs-jquery.
[node2][DEBUG ] Preparing to unpack .../libjs-jquery_1.7.2+dfsg-2ubuntu1_all.deb ...
[node2][DEBUG ] Unpacking libjs-jquery (1.7.2+dfsg-2ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package python-werkzeug.
[node2][DEBUG ] Preparing to unpack .../python-werkzeug_0.9.4+dfsg-1.1ubuntu2_all.deb ...
[node2][DEBUG ] Unpacking python-werkzeug (0.9.4+dfsg-1.1ubuntu2) ...
[node2][DEBUG ] Selecting previously unselected package python-itsdangerous.
[node2][DEBUG ] Preparing to unpack .../python-itsdangerous_0.22+dfsg1-1build1_all.deb ...
[node2][DEBUG ] Unpacking python-itsdangerous (0.22+dfsg1-1build1) ...
[node2][DEBUG ] Selecting previously unselected package python-flask.
[node2][DEBUG ] Preparing to unpack .../python-flask_0.10.1-2build1_all.deb ...
[node2][DEBUG ] Unpacking python-flask (0.10.1-2build1) ...
[node2][DEBUG ] Selecting previously unselected package libtcmalloc-minimal4.
[node2][DEBUG ] Preparing to unpack .../libtcmalloc-minimal4_2.1-2ubuntu1.1_amd64.deb ...
[node2][DEBUG ] Unpacking libtcmalloc-minimal4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Selecting previously unselected package libgoogle-perftools4.
[node2][DEBUG ] Preparing to unpack .../libgoogle-perftools4_2.1-2ubuntu1.1_amd64.deb ...
[node2][DEBUG ] Unpacking libgoogle-perftools4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Selecting previously unselected package ceph.
[node2][DEBUG ] Preparing to unpack .../ceph_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package ceph-mds.
[node2][DEBUG ] Preparing to unpack .../ceph-mds_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libfcgi0ldbl.
[node2][DEBUG ] Preparing to unpack .../libfcgi0ldbl_2.4.0-8.1ubuntu5_amd64.deb ...
[node2][DEBUG ] Unpacking libfcgi0ldbl (2.4.0-8.1ubuntu5) ...
[node2][DEBUG ] Selecting previously unselected package radosgw.
[node2][DEBUG ] Preparing to unpack .../radosgw_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking radosgw (9.2.1-1trusty) ...
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[node2][DEBUG ] Setting up libboost-program-options1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Setting up libboost-random1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Setting up libsnappy1 (1.1.0-1ubuntu1) ...
[node2][DEBUG ] Setting up libleveldb1:amd64 (1.15.0-2) ...
[node2][DEBUG ] Setting up libunwind8 (1.1-2.2ubuntu3) ...
[node2][DEBUG ] Setting up libbabeltrace1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Setting up libbabeltrace-ctf1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Setting up libradosstriper1 (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up python-rados (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up libcephfs1 (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up python-cephfs (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up python-rbd (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Setting system user ceph properties....done
[node2][WARNIN] usermod: no changes
[node2][DEBUG ] Setting up libcryptsetup4 (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Setting up cryptsetup-bin (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Setting up libjs-jquery (1.7.2+dfsg-2ubuntu1) ...
[node2][DEBUG ] Setting up python-werkzeug (0.9.4+dfsg-1.1ubuntu2) ...
[node2][DEBUG ] Setting up python-itsdangerous (0.22+dfsg1-1build1) ...
[node2][DEBUG ] Setting up python-flask (0.10.1-2build1) ...
[node2][DEBUG ] Setting up libtcmalloc-minimal4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Setting up libgoogle-perftools4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Setting up libfcgi0ldbl (2.4.0-8.1ubuntu5) ...
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][DEBUG ] Setting up radosgw (9.2.1-1trusty) ...
[node2][DEBUG ] radosgw-all start/running
[node2][DEBUG ] Setting up ceph (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-all start/running
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][DEBUG ] Setting up ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-mds-all start/running
[node2][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.7) ...
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][INFO  ] Running command: sudo ceph --version
[node2][DEBUG ] ceph version 9.2.1 (752b6a3020c3de74e07d2a8b4c5e48dab5a6b6fd)
megdc@node1:~/ceph-cluster$ ceph-deploy --overwrite-conf osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy --overwrite-conf osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5c49b17d40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f5c49f76500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to node2
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[node2][WARNIN] osd keyring does not exist yet, creating one
[node2][DEBUG ] create a keyring file
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage3/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage3/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Data dir /storage3/osd already exists
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage4/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage4/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Data dir /storage4/osd already exists
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
megdc@node1:~/ceph-cluster$ ceph-deploy^Csd prepare node2:/storage3/osd node2:/storage4/osd
megdc@node1:~/ceph-cluster$ ceph stutus
no valid command found; 10 closest matches:
osd down <ids> [<ids>...]
osd out <ids> [<ids>...]
osd unset full|pause|noup|nodown|noout|noin|nobackfill|norebalance|norecover|noscrub|nodeep-scrub|notieragent|sortbitwise
osd erasure-code-profile ls
osd set full|pause|noup|nodown|noout|noin|nobackfill|norebalance|norecover|noscrub|nodeep-scrub|notieragent|sortbitwise
osd erasure-code-profile get <name>
osd erasure-code-profile rm <name>
osd unpause
osd erasure-code-profile set <name> {<profile> [<profile>...]}
mds cluster_down
Error EINVAL: invalid command
megdc@node1:~/ceph-cluster$ ceph status
    cluster 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
     health HEALTH_OK
     monmap e1: 1 mons at {node1=136.243.49.217:6789/0}
            election epoch 1, quorum 0 node1
     osdmap e144: 2 osds: 2 up, 2 in
            flags sortbitwise
      pgmap v418437: 466 pgs, 16 pools, 73680 MB data, 18658 objects
            153 GB used, 10376 GB / 11089 GB avail
                 466 active+clean
  client io 0 B/s rd, 9009 B/s wr, 3 op/s
megdc@node1:~/ceph-cluster$ sudo nano /etc/ceph/ceph.conf 
megdc@node1:~/ceph-cluster$ ~/.ceph/config
bash: /home/megdc/.ceph/config: No such file or directory
megdc@node1:~/ceph-cluster$ ll -d /var/lib/ceph/*osd*
ls: cannot access /var/lib/ceph/*osd*: Permission denied
megdc@node1:~/ceph-cluster$ sudo ll -d /var/lib/ceph/*osd*
sudo: ll: command not found
megdc@node1:~/ceph-cluster$ sudo ls -d /var/lib/ceph/*osd*
ls: cannot access /var/lib/ceph/*osd*: No such file or directory
megdc@node1:~/ceph-cluster$ ls
ceph.bootstrap-mds.keyring  ceph.client.admin.keyring    ceph.conf         client.libvirt.key  s3.py       tmpuAA45G
ceph.bootstrap-osd.keyring  ceph.client.libvirt.keyring  ceph.log          rbdmap              secret.xml  uid
ceph.bootstrap-rgw.keyring  ceph.client.radosgw.keyring  ceph.mon.keyring  release.asc         tmpU4w_cD
megdc@node1:~/ceph-cluster$ cat ceph.client.admin.keyring 
[client.admin]
	key = AQCoayNX6fP0FxAAuviTKCdb0bF2Exu7bdGrUw==
megdc@node1:~/ceph-cluster$ ceph-deploy install node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy install node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  testing                       : None
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7183c2b0e0>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  dev_commit                    : None
[ceph_deploy.cli][INFO  ]  install_mds                   : False
[ceph_deploy.cli][INFO  ]  stable                        : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  adjust_repos                  : True
[ceph_deploy.cli][INFO  ]  func                          : <function install at 0x7f71844e3de8>
[ceph_deploy.cli][INFO  ]  install_all                   : False
[ceph_deploy.cli][INFO  ]  repo                          : False
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  install_rgw                   : False
[ceph_deploy.cli][INFO  ]  install_tests                 : False
[ceph_deploy.cli][INFO  ]  repo_url                      : None
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  install_osd                   : False
[ceph_deploy.cli][INFO  ]  version_kind                  : stable
[ceph_deploy.cli][INFO  ]  install_common                : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  dev                           : master
[ceph_deploy.cli][INFO  ]  local_mirror                  : None
[ceph_deploy.cli][INFO  ]  release                       : None
[ceph_deploy.cli][INFO  ]  install_mon                   : False
[ceph_deploy.cli][INFO  ]  gpg_url                       : None
[ceph_deploy.install][DEBUG ] Installing stable version infernalis on cluster ceph hosts node2
[ceph_deploy.install][DEBUG ] Detecting platform for host node2 ...
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] installing Ceph on node2
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates apt-transport-https
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] apt-transport-https is already the newest version.
[node2][DEBUG ] ca-certificates is already the newest version.
[node2][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.
[node2][INFO  ] Running command: sudo wget -O release.asc https://download.ceph.com/keys/release.asc
[node2][WARNIN] --2016-05-19 13:19:26--  https://download.ceph.com/keys/release.asc
[node2][WARNIN] Resolving download.ceph.com (download.ceph.com)... 2607:f298:6050:51f3:f816:3eff:fe71:9135, 173.236.253.173
[node2][WARNIN] Connecting to download.ceph.com (download.ceph.com)|2607:f298:6050:51f3:f816:3eff:fe71:9135|:443... connected.
[node2][WARNIN] HTTP request sent, awaiting response... 200 OK
[node2][WARNIN] Length: 1645 (1.6K) [application/octet-stream]
[node2][WARNIN] Saving to: ‘release.asc’
[node2][WARNIN] 
[node2][WARNIN]      0K .                                                     100%  551M=0s
[node2][WARNIN] 
[node2][WARNIN] 2016-05-19 13:19:27 (551 MB/s) - ‘release.asc’ saved [1645/1645]
[node2][WARNIN] 
[node2][INFO  ] Running command: sudo apt-key add release.asc
[node2][DEBUG ] OK
[node2][DEBUG ] add deb repo to /etc/apt/sources.list.d/
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q update
[node2][DEBUG ] Ign http://mirror.hetzner.de trusty InRelease
[node2][DEBUG ] Get:1 http://mirror.hetzner.de trusty-backports InRelease [65.9 kB]
[node2][DEBUG ] Get:2 http://mirror.hetzner.de trusty-updates InRelease [65.9 kB]
[node2][DEBUG ] Get:3 http://mirror.hetzner.de trusty-security InRelease [65.9 kB]
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty Release.gpg
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty Release
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/universe amd64 Packages
[node2][DEBUG ] Ign http://de.archive.ubuntu.com trusty InRelease
[node2][DEBUG ] Get:4 http://security.ubuntu.com trusty-security InRelease [65.9 kB]
[node2][DEBUG ] Get:5 http://de.archive.ubuntu.com trusty-updates InRelease [65.9 kB]
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/multiverse amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/restricted i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/universe i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/multiverse i386 Packages
[node2][DEBUG ] Get:6 http://de.archive.ubuntu.com trusty-backports InRelease [65.9 kB]
[node2][DEBUG ] Get:7 http://security.ubuntu.com trusty-security/main Sources [116 kB]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty Release.gpg
[node2][DEBUG ] Get:8 http://mirror.hetzner.de trusty-updates/main amd64 Packages [964 kB]
[node2][DEBUG ] Get:9 http://de.archive.ubuntu.com trusty-updates/main Sources [275 kB]
[node2][DEBUG ] Get:10 http://security.ubuntu.com trusty-security/restricted Sources [4,035 B]
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/restricted amd64 Packages
[node2][DEBUG ] Get:11 http://mirror.hetzner.de trusty-updates/universe amd64 Packages [464 kB]
[node2][DEBUG ] Get:12 http://de.archive.ubuntu.com trusty-updates/restricted Sources [5,352 B]
[node2][DEBUG ] Get:13 http://security.ubuntu.com trusty-security/universe Sources [36.2 kB]
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/multiverse amd64 Packages
[node2][DEBUG ] Get:14 http://mirror.hetzner.de trusty-updates/main i386 Packages [926 kB]
[node2][DEBUG ] Get:15 http://de.archive.ubuntu.com trusty-updates/universe Sources [154 kB]
[node2][DEBUG ] Get:16 http://security.ubuntu.com trusty-security/multiverse Sources [2,760 B]
[node2][DEBUG ] Get:17 http://de.archive.ubuntu.com trusty-updates/multiverse Sources [5,939 B]
[node2][DEBUG ] Get:18 http://security.ubuntu.com trusty-security/main amd64 Packages [480 kB]
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/restricted i386 Packages
[node2][DEBUG ] Get:19 http://de.archive.ubuntu.com trusty-updates/main amd64 Packages [768 kB]
[node2][DEBUG ] Get:20 http://mirror.hetzner.de trusty-updates/universe i386 Packages [466 kB]
[node2][DEBUG ] Get:21 http://de.archive.ubuntu.com trusty-updates/restricted amd64 Packages [15.9 kB]
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/multiverse i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/restricted amd64 Packages
[node2][DEBUG ] Get:22 http://security.ubuntu.com trusty-security/restricted amd64 Packages [13.0 kB]
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/universe amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/multiverse amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/restricted i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/universe i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/multiverse i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/universe amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/multiverse amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/restricted i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/universe i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/multiverse i386 Packages
[node2][DEBUG ] Get:23 http://de.archive.ubuntu.com trusty-updates/universe amd64 Packages [359 kB]
[node2][DEBUG ] Get:24 http://security.ubuntu.com trusty-security/universe amd64 Packages [128 kB]
[node2][DEBUG ] Get:25 http://de.archive.ubuntu.com trusty-updates/multiverse amd64 Packages [13.2 kB]
[node2][DEBUG ] Get:26 http://security.ubuntu.com trusty-security/multiverse amd64 Packages [4,978 B]
[node2][DEBUG ] Get:27 http://de.archive.ubuntu.com trusty-updates/main i386 Packages [736 kB]
[node2][DEBUG ] Get:28 http://security.ubuntu.com trusty-security/main i386 Packages [454 kB]
[node2][DEBUG ] Get:29 http://de.archive.ubuntu.com trusty-updates/restricted i386 Packages [15.6 kB]
[node2][DEBUG ] Get:30 http://security.ubuntu.com trusty-security/restricted i386 Packages [12.7 kB]
[node2][DEBUG ] Get:31 http://de.archive.ubuntu.com trusty-updates/universe i386 Packages [361 kB]
[node2][DEBUG ] Get:32 http://security.ubuntu.com trusty-security/universe i386 Packages [129 kB]
[node2][DEBUG ] Get:33 http://de.archive.ubuntu.com trusty-updates/multiverse i386 Packages [13.6 kB]
[node2][DEBUG ] Get:34 http://security.ubuntu.com trusty-security/multiverse i386 Packages [5,168 B]
[node2][DEBUG ] Get:35 http://de.archive.ubuntu.com trusty-backports/main Sources [9,560 B]
[node2][DEBUG ] Get:36 http://de.archive.ubuntu.com trusty-backports/restricted Sources [28 B]
[node2][DEBUG ] Hit http://get.megam.io trusty InRelease
[node2][DEBUG ] Get:37 http://de.archive.ubuntu.com trusty-backports/universe Sources [34.8 kB]
[node2][DEBUG ] Ign http://downloads.opennebula.org stable InRelease
[node2][DEBUG ] Get:38 http://de.archive.ubuntu.com trusty-backports/multiverse Sources [1,898 B]
[node2][DEBUG ] Get:39 http://de.archive.ubuntu.com trusty-backports/main amd64 Packages [13.0 kB]
[node2][DEBUG ] Get:40 http://de.archive.ubuntu.com trusty-backports/restricted amd64 Packages [28 B]
[node2][DEBUG ] Get:41 http://de.archive.ubuntu.com trusty-backports/universe amd64 Packages [43.0 kB]
[node2][DEBUG ] Get:42 http://de.archive.ubuntu.com trusty-backports/multiverse amd64 Packages [1,571 B]
[node2][DEBUG ] Get:43 http://de.archive.ubuntu.com trusty-backports/main i386 Packages [13.1 kB]
[node2][DEBUG ] Get:44 http://de.archive.ubuntu.com trusty-backports/restricted i386 Packages [28 B]
[node2][DEBUG ] Hit http://get.megam.io trusty/testing amd64 Packages
[node2][DEBUG ] Get:45 http://de.archive.ubuntu.com trusty-backports/universe i386 Packages [43.0 kB]
[node2][DEBUG ] Get:46 http://de.archive.ubuntu.com trusty-backports/multiverse i386 Packages [1,552 B]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty Release
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/main Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/restricted Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/universe Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/multiverse Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/main amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/restricted amd64 Packages
[node2][DEBUG ] Hit http://downloads.opennebula.org stable Release.gpg
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/universe amd64 Packages
[node2][DEBUG ] Hit https://download.ceph.com trusty InRelease
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/multiverse amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/main i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/restricted i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/universe i386 Packages
[node2][DEBUG ] Hit https://download.ceph.com trusty/main amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/multiverse i386 Packages
[node2][DEBUG ] Hit https://download.ceph.com trusty/main i386 Packages
[node2][DEBUG ] Hit http://downloads.opennebula.org stable Release
[node2][DEBUG ] Hit http://downloads.opennebula.org stable/opennebula amd64 Packages
[node2][DEBUG ] Hit http://downloads.opennebula.org stable/opennebula i386 Packages
[node2][DEBUG ] Fetched 7,485 kB in 2s (2,575 kB/s)
[node2][DEBUG ] Reading package lists...
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew ceph ceph-mds radosgw
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] The following extra packages will be installed:
[node2][DEBUG ]   ceph-common cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
[node2][DEBUG ]   libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
[node2][DEBUG ]   libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
[node2][DEBUG ]   python-flask python-itsdangerous python-rados python-rbd python-werkzeug
[node2][DEBUG ] Suggested packages:
[node2][DEBUG ]   javascript-common python-flask-doc ipython python-genshi python-lxml
[node2][DEBUG ]   python-greenlet python-redis python-pylibmc python-memcache
[node2][DEBUG ]   python-werkzeug-doc
[node2][DEBUG ] Recommended packages:
[node2][DEBUG ]   ceph-fs-common ceph-fuse python-blinker python-openssl python-pyinotify
[node2][DEBUG ] The following NEW packages will be installed:
[node2][DEBUG ]   ceph ceph-common ceph-mds cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
[node2][DEBUG ]   libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
[node2][DEBUG ]   libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
[node2][DEBUG ]   python-flask python-itsdangerous python-rados python-rbd python-werkzeug
[node2][DEBUG ]   radosgw
[node2][DEBUG ] 0 upgraded, 25 newly installed, 0 to remove and 39 not upgraded.
[node2][DEBUG ] Need to get 0 B/39.7 MB of archives.
[node2][DEBUG ] After this operation, 174 MB of additional disk space will be used.
[node2][DEBUG ] Selecting previously unselected package libboost-program-options1.54.0:amd64.
[node2][DEBUG ] (Reading database ... 36453 files and directories currently installed.)
[node2][DEBUG ] Preparing to unpack .../libboost-program-options1.54.0_1.54.0-4ubuntu3.1_amd64.deb ...
[node2][DEBUG ] Unpacking libboost-program-options1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Selecting previously unselected package libboost-random1.54.0:amd64.
[node2][DEBUG ] Preparing to unpack .../libboost-random1.54.0_1.54.0-4ubuntu3.1_amd64.deb ...
[node2][DEBUG ] Unpacking libboost-random1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Selecting previously unselected package libsnappy1.
[node2][DEBUG ] Preparing to unpack .../libsnappy1_1.1.0-1ubuntu1_amd64.deb ...
[node2][DEBUG ] Unpacking libsnappy1 (1.1.0-1ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package libleveldb1:amd64.
[node2][DEBUG ] Preparing to unpack .../libleveldb1_1.15.0-2_amd64.deb ...
[node2][DEBUG ] Unpacking libleveldb1:amd64 (1.15.0-2) ...
[node2][DEBUG ] Selecting previously unselected package libunwind8.
[node2][DEBUG ] Preparing to unpack .../libunwind8_1.1-2.2ubuntu3_amd64.deb ...
[node2][DEBUG ] Unpacking libunwind8 (1.1-2.2ubuntu3) ...
[node2][DEBUG ] Selecting previously unselected package libbabeltrace1:amd64.
[node2][DEBUG ] Preparing to unpack .../libbabeltrace1_1.2.1-2_amd64.deb ...
[node2][DEBUG ] Unpacking libbabeltrace1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Selecting previously unselected package libbabeltrace-ctf1:amd64.
[node2][DEBUG ] Preparing to unpack .../libbabeltrace-ctf1_1.2.1-2_amd64.deb ...
[node2][DEBUG ] Unpacking libbabeltrace-ctf1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Selecting previously unselected package libradosstriper1.
[node2][DEBUG ] Preparing to unpack .../libradosstriper1_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking libradosstriper1 (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package python-rados.
[node2][DEBUG ] Preparing to unpack .../python-rados_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking python-rados (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libcephfs1.
[node2][DEBUG ] Preparing to unpack .../libcephfs1_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking libcephfs1 (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package python-cephfs.
[node2][DEBUG ] Preparing to unpack .../python-cephfs_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking python-cephfs (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package python-rbd.
[node2][DEBUG ] Preparing to unpack .../python-rbd_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking python-rbd (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package ceph-common.
[node2][DEBUG ] Preparing to unpack .../ceph-common_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libcryptsetup4.
[node2][DEBUG ] Preparing to unpack .../libcryptsetup4_2%3a1.6.1-1ubuntu1_amd64.deb ...
[node2][DEBUG ] Unpacking libcryptsetup4 (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package cryptsetup-bin.
[node2][DEBUG ] Preparing to unpack .../cryptsetup-bin_2%3a1.6.1-1ubuntu1_amd64.deb ...
[node2][DEBUG ] Unpacking cryptsetup-bin (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package libjs-jquery.
[node2][DEBUG ] Preparing to unpack .../libjs-jquery_1.7.2+dfsg-2ubuntu1_all.deb ...
[node2][DEBUG ] Unpacking libjs-jquery (1.7.2+dfsg-2ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package python-werkzeug.
[node2][DEBUG ] Preparing to unpack .../python-werkzeug_0.9.4+dfsg-1.1ubuntu2_all.deb ...
[node2][DEBUG ] Unpacking python-werkzeug (0.9.4+dfsg-1.1ubuntu2) ...
[node2][DEBUG ] Selecting previously unselected package python-itsdangerous.
[node2][DEBUG ] Preparing to unpack .../python-itsdangerous_0.22+dfsg1-1build1_all.deb ...
[node2][DEBUG ] Unpacking python-itsdangerous (0.22+dfsg1-1build1) ...
[node2][DEBUG ] Selecting previously unselected package python-flask.
[node2][DEBUG ] Preparing to unpack .../python-flask_0.10.1-2build1_all.deb ...
[node2][DEBUG ] Unpacking python-flask (0.10.1-2build1) ...
[node2][DEBUG ] Selecting previously unselected package libtcmalloc-minimal4.
[node2][DEBUG ] Preparing to unpack .../libtcmalloc-minimal4_2.1-2ubuntu1.1_amd64.deb ...
[node2][DEBUG ] Unpacking libtcmalloc-minimal4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Selecting previously unselected package libgoogle-perftools4.
[node2][DEBUG ] Preparing to unpack .../libgoogle-perftools4_2.1-2ubuntu1.1_amd64.deb ...
[node2][DEBUG ] Unpacking libgoogle-perftools4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Selecting previously unselected package ceph.
[node2][DEBUG ] Preparing to unpack .../ceph_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package ceph-mds.
[node2][DEBUG ] Preparing to unpack .../ceph-mds_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libfcgi0ldbl.
[node2][DEBUG ] Preparing to unpack .../libfcgi0ldbl_2.4.0-8.1ubuntu5_amd64.deb ...
[node2][DEBUG ] Unpacking libfcgi0ldbl (2.4.0-8.1ubuntu5) ...
[node2][DEBUG ] Selecting previously unselected package radosgw.
[node2][DEBUG ] Preparing to unpack .../radosgw_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking radosgw (9.2.1-1trusty) ...
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[node2][DEBUG ] Setting up libboost-program-options1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Setting up libboost-random1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Setting up libsnappy1 (1.1.0-1ubuntu1) ...
[node2][DEBUG ] Setting up libleveldb1:amd64 (1.15.0-2) ...
[node2][DEBUG ] Setting up libunwind8 (1.1-2.2ubuntu3) ...
[node2][DEBUG ] Setting up libbabeltrace1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Setting up libbabeltrace-ctf1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Setting up libradosstriper1 (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up python-rados (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up libcephfs1 (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up python-cephfs (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up python-rbd (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Setting system user ceph properties....done
[node2][WARNIN] usermod: no changes
[node2][DEBUG ] Setting up libcryptsetup4 (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Setting up cryptsetup-bin (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Setting up libjs-jquery (1.7.2+dfsg-2ubuntu1) ...
[node2][DEBUG ] Setting up python-werkzeug (0.9.4+dfsg-1.1ubuntu2) ...
[node2][DEBUG ] Setting up python-itsdangerous (0.22+dfsg1-1build1) ...
[node2][DEBUG ] Setting up python-flask (0.10.1-2build1) ...
[node2][DEBUG ] Setting up libtcmalloc-minimal4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Setting up libgoogle-perftools4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Setting up libfcgi0ldbl (2.4.0-8.1ubuntu5) ...
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][DEBUG ] Setting up radosgw (9.2.1-1trusty) ...
[node2][DEBUG ] radosgw-all start/running
[node2][DEBUG ] Setting up ceph (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-all start/running
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][DEBUG ] Setting up ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-mds-all start/running
[node2][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.7) ...
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][INFO  ] Running command: sudo ceph --version
[node2][DEBUG ] ceph version 9.2.1 (752b6a3020c3de74e07d2a8b4c5e48dab5a6b6fd)
megdc@node1:~/ceph-cluster$ sudo nano /etc/ceph/ceph.conf 
megdc@node1:~/ceph-cluster$ eph-deploy admin node2
No command 'eph-deploy' found, did you mean:
 Command 'ceph-deploy' from package 'ceph-deploy' (universe)
eph-deploy: command not found
megdc@node1:~/ceph-cluster$ ceph-deploy admin node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy admin node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6eb42daa70>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  client                        : ['node2']
[ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f6eb4b6f758>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to node2
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
megdc@node1:~/ceph-cluster$ ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4fa8a92d40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f4fa8ef1500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to node2
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[node2][WARNIN] osd keyring does not exist yet, creating one
[node2][DEBUG ] create a keyring file
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage3/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage3/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Data dir /storage3/osd already exists
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[node2][WARNIN] No data was received after 300 seconds, disconnecting...
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage4/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage4/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Data dir /storage4/osd already exists
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[node2][WARNIN] No data was received after 300 seconds, disconnecting...
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
megdc@node1:~/ceph-cluster$ ceph --cluster ceph osd tree
ID WEIGHT   TYPE NAME               UP/DOWN REWEIGHT PRIMARY-AFFINITY 
-5        0 rack unknownrack                                          
-4        0     host 138.201.21.215                                   
-1 21.64975 root default                                              
-2 10.82977     host node1                                            
 0  5.41489         osd.0                up  1.00000          1.00000 
 1  5.41489         osd.1                up  1.00000          1.00000 
-3 10.81998     host node2                                            
 2  5.40999         osd.2               DNE        0                  
 3  5.40999         osd.3               DNE        0                  
megdc@node1:~/ceph-cluster$ ceph-deploy osd activate node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy osd activate node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : activate
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f524d483d40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f524d8e2500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.osd][DEBUG ] Activating cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] activating host node2 disk /storage3/osd
[ceph_deploy.osd][DEBUG ] will use init type: upstart
[node2][INFO  ] Running command: sudo ceph-disk -v activate --mark-init upstart --mount /storage3/osd
[node2][WARNIN] DEBUG:ceph-disk:Cluster uuid is 271ea3ed-a709-4dae-8612-e172945efc72
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] DEBUG:ceph-disk:Cluster name is ceph
[node2][WARNIN] DEBUG:ceph-disk:OSD uuid is 85169284-fab7-40df-acdb-928436978d82
[node2][WARNIN] DEBUG:ceph-disk:Allocating OSD id...
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 85169284-fab7-40df-acdb-928436978d82


^C[ceph_deploy][ERROR ] KeyboardInterrupt

megdc@node1:~/ceph-cluster$ ceph-deploy uninstall node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy uninstall node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f03e0d1ac68>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  func                          : <function uninstall at 0x7f03e15a90c8>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[ceph_deploy.install][INFO  ] like: librbd1 and librados2
[ceph_deploy.install][DEBUG ] Uninstalling on cluster ceph hosts node2
[ceph_deploy.install][DEBUG ] Detecting platform for host node2 ...
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] Uninstalling Ceph on node2
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove ceph ceph-mds ceph-common ceph-fs-common radosgw
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[node2][DEBUG ] The following packages were automatically installed and are no longer required:
[node2][DEBUG ]   cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
[node2][DEBUG ]   libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
[node2][DEBUG ]   libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
[node2][DEBUG ]   python-flask python-itsdangerous python-rados python-rbd python-werkzeug
[node2][DEBUG ] Use 'apt-get autoremove' to remove them.
[node2][DEBUG ] The following packages will be REMOVED:
[node2][DEBUG ]   ceph ceph-common ceph-mds radosgw
[node2][DEBUG ] 0 upgraded, 0 newly installed, 4 to remove and 39 not upgraded.
[node2][DEBUG ] After this operation, 151 MB disk space will be freed.
[node2][DEBUG ] (Reading database ... 37108 files and directories currently installed.)
[node2][DEBUG ] Removing ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-mds-all stop/waiting
[node2][DEBUG ] Removing ceph (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-all stop/waiting
[node2][DEBUG ] Removing radosgw (9.2.1-1trusty) ...
[node2][WARNIN] stop: Unknown instance: 
[node2][DEBUG ] Removing ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[node2][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.7) ...
megdc@node1:~/ceph-cluster$ ceph-deploy purgedata node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy purgedata node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0a98193998>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f0a98a9b1b8>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts node2
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] purging data on node2
[node2][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[node2][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
megdc@node1:~/ceph-cluster$ ceph-deploy purge node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy purge node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f8735e07320>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f873670f140>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[ceph_deploy.install][INFO  ] like: librbd1 and librados2
[ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts node2
[ceph_deploy.install][DEBUG ] Detecting platform for host node2 ...
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] Purging Ceph on node2
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[node2][DEBUG ] The following packages were automatically installed and are no longer required:
[node2][DEBUG ]   cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
[node2][DEBUG ]   libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
[node2][DEBUG ]   libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
[node2][DEBUG ]   python-flask python-itsdangerous python-rados python-rbd python-werkzeug
[node2][DEBUG ] Use 'apt-get autoremove' to remove them.
[node2][DEBUG ] The following packages will be REMOVED:
[node2][DEBUG ]   ceph* ceph-common* ceph-mds* radosgw*
[node2][DEBUG ] 0 upgraded, 0 newly installed, 4 to remove and 39 not upgraded.
[node2][DEBUG ] After this operation, 0 B of additional disk space will be used.
[node2][DEBUG ] (Reading database ... 36818 files and directories currently installed.)
[node2][DEBUG ] Removing ceph (9.2.1-1trusty) ...
[node2][DEBUG ] Purging configuration files for ceph (9.2.1-1trusty) ...
[node2][DEBUG ] Removing ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Purging configuration files for ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Removing ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] Purging configuration files for ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] Removing radosgw (9.2.1-1trusty) ...
[node2][DEBUG ] Purging configuration files for radosgw (9.2.1-1trusty) ...
megdc@node1:~/ceph-cluster$ ssh megdc@node2 'sudo apt-get -y autoremove'
Reading package lists...
Building dependency tree...
Reading state information...
The following packages will be REMOVED:
  cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
  libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
  libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
  libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
  python-flask python-itsdangerous python-rados python-rbd python-werkzeug
0 upgraded, 0 newly installed, 21 to remove and 39 not upgraded.
After this operation, 22.6 MB disk space will be freed.
(Reading database ... 36786 files and directories currently installed.)
Removing cryptsetup-bin (2:1.6.1-1ubuntu1) ...
Removing libbabeltrace-ctf1:amd64 (1.2.1-2) ...
Removing libbabeltrace1:amd64 (1.2.1-2) ...
Removing libboost-program-options1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
Removing libradosstriper1 (9.2.1-1trusty) ...
Removing libboost-random1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
Removing python-cephfs (9.2.1-1trusty) ...
Removing libcephfs1 (9.2.1-1trusty) ...
Removing libcryptsetup4 (2:1.6.1-1ubuntu1) ...
Removing libfcgi0ldbl (2.4.0-8.1ubuntu5) ...
Removing libgoogle-perftools4 (2.1-2ubuntu1.1) ...
Removing python-flask (0.10.1-2build1) ...
Removing python-werkzeug (0.9.4+dfsg-1.1ubuntu2) ...
Removing libjs-jquery (1.7.2+dfsg-2ubuntu1) ...
Removing libleveldb1:amd64 (1.15.0-2) ...
Removing libsnappy1 (1.1.0-1ubuntu1) ...
Removing libtcmalloc-minimal4 (2.1-2ubuntu1.1) ...
Removing libunwind8 (1.1-2.2ubuntu3) ...
Removing python-itsdangerous (0.22+dfsg1-1build1) ...
Removing python-rados (9.2.1-1trusty) ...
Removing python-rbd (9.2.1-1trusty) ...
Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
Processing triggers for libc-bin (2.19-0ubuntu6.7) ...
megdc@node1:~/ceph-cluster$ ssh megdc@node2 'sudo rm -r /run/ceph'
megdc@node1:~/ceph-cluster$ ssh megdc@node2 'sudo rm -r /var/lib/ceph'
rm: cannot remove ‘/var/lib/ceph’: No such file or directory
megdc@node1:~/ceph-cluster$ ssh megdc@node2 'sudo rm /var/log/upstart/ceph*'
rm: cannot remove ‘/var/log/upstart/ceph*’: No such file or directory
megdc@node1:~/ceph-cluster$ ssh megdc@node2 'sudo rm -rf /home/megdc/ceph-cluster/*'
megdc@node1:~/ceph-cluster$ ceph-deploy disk list
usage: ceph-deploy disk list [-h] HOST:DISK [HOST:DISK ...]
ceph-deploy disk list: error: too few arguments
megdc@node1:~/ceph-cluster$ ceph-deploy disk list node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy disk list node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : list
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7efeef657638>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7efeefabb578>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', None, None)]
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy][ERROR ] RuntimeError: ceph needs to be installed in remote host: node2

megdc@node1:~/ceph-cluster$ ceph-deploy install node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy install node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  testing                       : None
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff1266f50e0>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  dev_commit                    : None
[ceph_deploy.cli][INFO  ]  install_mds                   : False
[ceph_deploy.cli][INFO  ]  stable                        : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  adjust_repos                  : True
[ceph_deploy.cli][INFO  ]  func                          : <function install at 0x7ff126fadde8>
[ceph_deploy.cli][INFO  ]  install_all                   : False
[ceph_deploy.cli][INFO  ]  repo                          : False
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  install_rgw                   : False
[ceph_deploy.cli][INFO  ]  install_tests                 : False
[ceph_deploy.cli][INFO  ]  repo_url                      : None
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  install_osd                   : False
[ceph_deploy.cli][INFO  ]  version_kind                  : stable
[ceph_deploy.cli][INFO  ]  install_common                : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  dev                           : master
[ceph_deploy.cli][INFO  ]  local_mirror                  : None
[ceph_deploy.cli][INFO  ]  release                       : None
[ceph_deploy.cli][INFO  ]  install_mon                   : False
[ceph_deploy.cli][INFO  ]  gpg_url                       : None
[ceph_deploy.install][DEBUG ] Installing stable version infernalis on cluster ceph hosts node2
[ceph_deploy.install][DEBUG ] Detecting platform for host node2 ...
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] installing Ceph on node2
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates apt-transport-https
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] apt-transport-https is already the newest version.
[node2][DEBUG ] ca-certificates is already the newest version.
[node2][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.
[node2][INFO  ] Running command: sudo wget -O release.asc https://download.ceph.com/keys/release.asc
[node2][WARNIN] --2016-05-19 14:06:50--  https://download.ceph.com/keys/release.asc
[node2][WARNIN] Resolving download.ceph.com (download.ceph.com)... 2607:f298:6050:51f3:f816:3eff:fe71:9135, 173.236.253.173
[node2][WARNIN] Connecting to download.ceph.com (download.ceph.com)|2607:f298:6050:51f3:f816:3eff:fe71:9135|:443... connected.
[node2][WARNIN] HTTP request sent, awaiting response... 200 OK
[node2][WARNIN] Length: 1645 (1.6K) [application/octet-stream]
[node2][WARNIN] Saving to: ‘release.asc’
[node2][WARNIN] 
[node2][WARNIN]      0K .                                                     100%  776M=0s
[node2][WARNIN] 
[node2][WARNIN] 2016-05-19 14:06:50 (776 MB/s) - ‘release.asc’ saved [1645/1645]
[node2][WARNIN] 
[node2][INFO  ] Running command: sudo apt-key add release.asc
[node2][DEBUG ] OK
[node2][DEBUG ] add deb repo to /etc/apt/sources.list.d/
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q update
[node2][DEBUG ] Ign http://mirror.hetzner.de trusty InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty Release.gpg
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty Release
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/universe amd64 Packages
[node2][DEBUG ] Ign http://de.archive.ubuntu.com trusty InRelease
[node2][DEBUG ] Get:1 http://security.ubuntu.com trusty-security InRelease [65.9 kB]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/multiverse amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/restricted i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/universe i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/multiverse i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty Release.gpg
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/main Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/restricted Sources
[node2][DEBUG ] Get:2 http://security.ubuntu.com trusty-security/main Sources [116 kB]
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/universe amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/multiverse amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/restricted i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/universe Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/multiverse Sources
[node2][DEBUG ] Get:3 http://security.ubuntu.com trusty-security/restricted Sources [4,035 B]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/universe i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/multiverse i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/universe amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/multiverse amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/restricted i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/universe i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/multiverse i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/universe amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/multiverse amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/restricted i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/universe i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/multiverse i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/restricted amd64 Packages
[node2][DEBUG ] Get:4 http://security.ubuntu.com trusty-security/universe Sources [36.2 kB]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/universe amd64 Packages
[node2][DEBUG ] Get:5 http://security.ubuntu.com trusty-security/multiverse Sources [2,760 B]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/multiverse amd64 Packages
[node2][DEBUG ] Get:6 http://security.ubuntu.com trusty-security/main amd64 Packages [480 kB]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/main i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/restricted i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/universe i386 Packages
[node2][DEBUG ] Ign http://downloads.opennebula.org stable InRelease
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/multiverse i386 Packages
[node2][DEBUG ] Get:7 http://security.ubuntu.com trusty-security/restricted amd64 Packages [13.0 kB]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/main Sources
[node2][DEBUG ] Get:8 http://security.ubuntu.com trusty-security/universe amd64 Packages [128 kB]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/restricted Sources
[node2][DEBUG ] Get:9 http://security.ubuntu.com trusty-security/multiverse amd64 Packages [4,978 B]
[node2][DEBUG ] Get:10 http://security.ubuntu.com trusty-security/main i386 Packages [454 kB]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/universe Sources
[node2][DEBUG ] Get:11 http://security.ubuntu.com trusty-security/restricted i386 Packages [12.7 kB]
[node2][DEBUG ] Hit http://get.megam.io trusty InRelease
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/multiverse Sources
[node2][DEBUG ] Get:12 http://security.ubuntu.com trusty-security/universe i386 Packages [129 kB]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/main amd64 Packages
[node2][DEBUG ] Get:13 http://security.ubuntu.com trusty-security/multiverse i386 Packages [5,168 B]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/restricted amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/universe amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/multiverse amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/main i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/restricted i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/universe i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/multiverse i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty Release
[node2][DEBUG ] Hit http://downloads.opennebula.org stable Release.gpg
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/main Sources
[node2][DEBUG ] Hit http://get.megam.io trusty/testing amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/restricted Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/universe Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/multiverse Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/main amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/restricted amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/universe amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/multiverse amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/main i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/restricted i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/universe i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/multiverse i386 Packages
[node2][DEBUG ] Hit http://downloads.opennebula.org stable Release
[node2][DEBUG ] Hit https://download.ceph.com trusty InRelease
[node2][DEBUG ] Hit https://download.ceph.com trusty/main amd64 Packages
[node2][DEBUG ] Hit https://download.ceph.com trusty/main i386 Packages
[node2][DEBUG ] Hit http://downloads.opennebula.org stable/opennebula amd64 Packages
[node2][DEBUG ] Hit http://downloads.opennebula.org stable/opennebula i386 Packages
[node2][DEBUG ] Fetched 1,451 kB in 3s (481 kB/s)
[node2][DEBUG ] Reading package lists...
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew ceph ceph-mds radosgw
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] The following extra packages will be installed:
[node2][DEBUG ]   ceph-common cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
[node2][DEBUG ]   libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
[node2][DEBUG ]   libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
[node2][DEBUG ]   python-flask python-itsdangerous python-rados python-rbd python-werkzeug
[node2][DEBUG ] Suggested packages:
[node2][DEBUG ]   javascript-common python-flask-doc ipython python-genshi python-lxml
[node2][DEBUG ]   python-greenlet python-redis python-pylibmc python-memcache
[node2][DEBUG ]   python-werkzeug-doc
[node2][DEBUG ] Recommended packages:
[node2][DEBUG ]   ceph-fs-common ceph-fuse python-blinker python-openssl python-pyinotify
[node2][DEBUG ] The following NEW packages will be installed:
[node2][DEBUG ]   ceph ceph-common ceph-mds cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
[node2][DEBUG ]   libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
[node2][DEBUG ]   libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
[node2][DEBUG ]   python-flask python-itsdangerous python-rados python-rbd python-werkzeug
[node2][DEBUG ]   radosgw
[node2][DEBUG ] 0 upgraded, 25 newly installed, 0 to remove and 39 not upgraded.
[node2][DEBUG ] Need to get 0 B/39.7 MB of archives.
[node2][DEBUG ] After this operation, 174 MB of additional disk space will be used.
[node2][DEBUG ] Selecting previously unselected package libboost-program-options1.54.0:amd64.
[node2][DEBUG ] (Reading database ... 36453 files and directories currently installed.)
[node2][DEBUG ] Preparing to unpack .../libboost-program-options1.54.0_1.54.0-4ubuntu3.1_amd64.deb ...
[node2][DEBUG ] Unpacking libboost-program-options1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Selecting previously unselected package libboost-random1.54.0:amd64.
[node2][DEBUG ] Preparing to unpack .../libboost-random1.54.0_1.54.0-4ubuntu3.1_amd64.deb ...
[node2][DEBUG ] Unpacking libboost-random1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Selecting previously unselected package libsnappy1.
[node2][DEBUG ] Preparing to unpack .../libsnappy1_1.1.0-1ubuntu1_amd64.deb ...
[node2][DEBUG ] Unpacking libsnappy1 (1.1.0-1ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package libleveldb1:amd64.
[node2][DEBUG ] Preparing to unpack .../libleveldb1_1.15.0-2_amd64.deb ...
[node2][DEBUG ] Unpacking libleveldb1:amd64 (1.15.0-2) ...
[node2][DEBUG ] Selecting previously unselected package libunwind8.
[node2][DEBUG ] Preparing to unpack .../libunwind8_1.1-2.2ubuntu3_amd64.deb ...
[node2][DEBUG ] Unpacking libunwind8 (1.1-2.2ubuntu3) ...
[node2][DEBUG ] Selecting previously unselected package libbabeltrace1:amd64.
[node2][DEBUG ] Preparing to unpack .../libbabeltrace1_1.2.1-2_amd64.deb ...
[node2][DEBUG ] Unpacking libbabeltrace1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Selecting previously unselected package libbabeltrace-ctf1:amd64.
[node2][DEBUG ] Preparing to unpack .../libbabeltrace-ctf1_1.2.1-2_amd64.deb ...
[node2][DEBUG ] Unpacking libbabeltrace-ctf1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Selecting previously unselected package libradosstriper1.
[node2][DEBUG ] Preparing to unpack .../libradosstriper1_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking libradosstriper1 (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package python-rados.
[node2][DEBUG ] Preparing to unpack .../python-rados_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking python-rados (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libcephfs1.
[node2][DEBUG ] Preparing to unpack .../libcephfs1_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking libcephfs1 (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package python-cephfs.
[node2][DEBUG ] Preparing to unpack .../python-cephfs_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking python-cephfs (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package python-rbd.
[node2][DEBUG ] Preparing to unpack .../python-rbd_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking python-rbd (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package ceph-common.
[node2][DEBUG ] Preparing to unpack .../ceph-common_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libcryptsetup4.
[node2][DEBUG ] Preparing to unpack .../libcryptsetup4_2%3a1.6.1-1ubuntu1_amd64.deb ...
[node2][DEBUG ] Unpacking libcryptsetup4 (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package cryptsetup-bin.
[node2][DEBUG ] Preparing to unpack .../cryptsetup-bin_2%3a1.6.1-1ubuntu1_amd64.deb ...
[node2][DEBUG ] Unpacking cryptsetup-bin (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package libjs-jquery.
[node2][DEBUG ] Preparing to unpack .../libjs-jquery_1.7.2+dfsg-2ubuntu1_all.deb ...
[node2][DEBUG ] Unpacking libjs-jquery (1.7.2+dfsg-2ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package python-werkzeug.
[node2][DEBUG ] Preparing to unpack .../python-werkzeug_0.9.4+dfsg-1.1ubuntu2_all.deb ...
[node2][DEBUG ] Unpacking python-werkzeug (0.9.4+dfsg-1.1ubuntu2) ...
[node2][DEBUG ] Selecting previously unselected package python-itsdangerous.
[node2][DEBUG ] Preparing to unpack .../python-itsdangerous_0.22+dfsg1-1build1_all.deb ...
[node2][DEBUG ] Unpacking python-itsdangerous (0.22+dfsg1-1build1) ...
[node2][DEBUG ] Selecting previously unselected package python-flask.
[node2][DEBUG ] Preparing to unpack .../python-flask_0.10.1-2build1_all.deb ...
[node2][DEBUG ] Unpacking python-flask (0.10.1-2build1) ...
[node2][DEBUG ] Selecting previously unselected package libtcmalloc-minimal4.
[node2][DEBUG ] Preparing to unpack .../libtcmalloc-minimal4_2.1-2ubuntu1.1_amd64.deb ...
[node2][DEBUG ] Unpacking libtcmalloc-minimal4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Selecting previously unselected package libgoogle-perftools4.
[node2][DEBUG ] Preparing to unpack .../libgoogle-perftools4_2.1-2ubuntu1.1_amd64.deb ...
[node2][DEBUG ] Unpacking libgoogle-perftools4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Selecting previously unselected package ceph.
[node2][DEBUG ] Preparing to unpack .../ceph_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package ceph-mds.
[node2][DEBUG ] Preparing to unpack .../ceph-mds_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libfcgi0ldbl.
[node2][DEBUG ] Preparing to unpack .../libfcgi0ldbl_2.4.0-8.1ubuntu5_amd64.deb ...
[node2][DEBUG ] Unpacking libfcgi0ldbl (2.4.0-8.1ubuntu5) ...
[node2][DEBUG ] Selecting previously unselected package radosgw.
[node2][DEBUG ] Preparing to unpack .../radosgw_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking radosgw (9.2.1-1trusty) ...
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[node2][DEBUG ] Setting up libboost-program-options1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Setting up libboost-random1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Setting up libsnappy1 (1.1.0-1ubuntu1) ...
[node2][DEBUG ] Setting up libleveldb1:amd64 (1.15.0-2) ...
[node2][DEBUG ] Setting up libunwind8 (1.1-2.2ubuntu3) ...
[node2][DEBUG ] Setting up libbabeltrace1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Setting up libbabeltrace-ctf1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Setting up libradosstriper1 (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up python-rados (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up libcephfs1 (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up python-cephfs (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up python-rbd (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Setting system user ceph properties....done
[node2][WARNIN] usermod: no changes
[node2][DEBUG ] Setting up libcryptsetup4 (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Setting up cryptsetup-bin (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Setting up libjs-jquery (1.7.2+dfsg-2ubuntu1) ...
[node2][DEBUG ] Setting up python-werkzeug (0.9.4+dfsg-1.1ubuntu2) ...
[node2][DEBUG ] Setting up python-itsdangerous (0.22+dfsg1-1build1) ...
[node2][DEBUG ] Setting up python-flask (0.10.1-2build1) ...
[node2][DEBUG ] Setting up libtcmalloc-minimal4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Setting up libgoogle-perftools4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Setting up libfcgi0ldbl (2.4.0-8.1ubuntu5) ...
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][DEBUG ] Setting up radosgw (9.2.1-1trusty) ...
[node2][DEBUG ] radosgw-all start/running
[node2][DEBUG ] Setting up ceph (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-all start/running
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][DEBUG ] Setting up ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-mds-all start/running
[node2][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.7) ...
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][INFO  ] Running command: sudo ceph --version
[node2][DEBUG ] ceph version 9.2.1 (752b6a3020c3de74e07d2a8b4c5e48dab5a6b6fd)
megdc@node1:~/ceph-cluster$ sudo scp /etc/ceph/*.conf megdc@node2:/home/megdc/
megdc@node2's password: 
ceph.conf                                                                                                    100%  986     1.0KB/s   00:00    
megdc@node1:~/ceph-cluster$ ssh megdc@node2 'sudo mv -r /home/megdc/*.conf /etc/ceph/'
mv: invalid option -- 'r'
Try 'mv --help' for more information.
megdc@node1:~/ceph-cluster$ ssh megdc@node2 'sudo mv  /home/megdc/*.conf /etc/ceph/'
megdc@node1:~/ceph-cluster$ ceph-deploy osd create node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy osd create node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1ad4627d40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f1ad4a86500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to node2
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage4/osd journal None activate True
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage4/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Data dir /storage4/osd already exists
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

megdc@node1:~/ceph-cluster$ ceph-deploy disk list node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy disk list node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : list
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fae9d356638>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fae9d7ba578>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', None, None)]
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Listing disks on node2...
[node2][DEBUG ] find the location of an executable
[node2][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[node2][DEBUG ] /dev/loop0 other, unknown
[node2][DEBUG ] /dev/loop1 other, unknown
[node2][DEBUG ] /dev/loop2 other, unknown
[node2][DEBUG ] /dev/loop3 other, unknown
[node2][DEBUG ] /dev/loop4 other, unknown
[node2][DEBUG ] /dev/loop5 other, unknown
[node2][DEBUG ] /dev/loop6 other, unknown
[node2][DEBUG ] /dev/loop7 other, unknown
[node2][DEBUG ] /dev/md0 other, ext3, mounted on /boot
[node2][DEBUG ] /dev/md1 swap, swap
[node2][DEBUG ] /dev/md2 other, ext4, mounted on /
[node2][DEBUG ] /dev/ram0 other, unknown
[node2][DEBUG ] /dev/ram1 other, unknown
[node2][DEBUG ] /dev/ram10 other, unknown
[node2][DEBUG ] /dev/ram11 other, unknown
[node2][DEBUG ] /dev/ram12 other, unknown
[node2][DEBUG ] /dev/ram13 other, unknown
[node2][DEBUG ] /dev/ram14 other, unknown
[node2][DEBUG ] /dev/ram15 other, unknown
[node2][DEBUG ] /dev/ram2 other, unknown
[node2][DEBUG ] /dev/ram3 other, unknown
[node2][DEBUG ] /dev/ram4 other, unknown
[node2][DEBUG ] /dev/ram5 other, unknown
[node2][DEBUG ] /dev/ram6 other, unknown
[node2][DEBUG ] /dev/ram7 other, unknown
[node2][DEBUG ] /dev/ram8 other, unknown
[node2][DEBUG ] /dev/ram9 other, unknown
[node2][DEBUG ] /dev/sda :
[node2][DEBUG ]  /dev/sda1 other, linux_raid_member
[node2][DEBUG ]  /dev/sda2 other, linux_raid_member
[node2][DEBUG ]  /dev/sda3 other, linux_raid_member
[node2][DEBUG ] /dev/sdb :
[node2][DEBUG ]  /dev/sdb1 other, linux_raid_member
[node2][DEBUG ]  /dev/sdb2 other, linux_raid_member
[node2][DEBUG ]  /dev/sdb3 other, linux_raid_member
[node2][DEBUG ] /dev/sdc :
[node2][DEBUG ]  /dev/sdc1 other, ext4, mounted on /storage3
[node2][DEBUG ] /dev/sdd :
[node2][DEBUG ]  /dev/sdd1 other, ext4, mounted on /storage4
megdc@node1:~/ceph-cluster$ nano /etc/ceph/ceph.conf 
megdc@node1:~/ceph-cluster$ sudo nano /etc/ceph/ceph.conf 
megdc@node1:~/ceph-cluster$ ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6661c91d40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f66620f0500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to node2
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage4/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage4/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Data dir /storage4/osd already exists
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

megdc@node1:~/ceph-cluster$ ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fd75783ad40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fd757c99500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to node2
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage4/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage4/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Data dir /storage4/osd already exists
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

megdc@node1:~/ceph-cluster$ ceph-deploy osd prepare node2:/storage3/osd 
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy osd prepare node2:/storage3/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None)]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fcaee4abd40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fcaee90a500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks node2:/storage3/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to node2
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

megdc@node1:~/ceph-cluster$ ceph status
    cluster 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
     health HEALTH_OK
     monmap e1: 1 mons at {node1=136.243.49.217:6789/0}
            election epoch 1, quorum 0 node1
     osdmap e144: 2 osds: 2 up, 2 in
            flags sortbitwise
      pgmap v420680: 466 pgs, 16 pools, 73727 MB data, 18670 objects
            153 GB used, 10376 GB / 11089 GB avail
                 466 active+clean
  client io 0 B/s rd, 18838 B/s wr, 7 op/s
megdc@node1:~/ceph-cluster$ sudo scp /etc/ceph/*.keyring megdc@node2:/home/megdc/
megdc@node2's password: 
ceph.bootstrap-mds.keyring                                                                                   100%   71     0.1KB/s   00:00    
ceph.bootstrap-osd.keyring                                                                                   100%   71     0.1KB/s   00:00    
ceph.bootstrap-rgw.keyring                                                                                   100%   71     0.1KB/s   00:00    
ceph.client.admin.keyring                                                                                    100%   63     0.1KB/s   00:00    
ceph.client.libvirt.keyring                                                                                  100%  165     0.2KB/s   00:00    
ceph.client.radosgw.keyring                                                                                  100%  119     0.1KB/s   00:00    
ceph.mon.keyring                                                                                             100%   73     0.1KB/s   00:00    
megdc@node1:~/ceph-cluster$ ssh megdc@node2 'sudo mv  /home/megdc/*.keyring /etc/ceph/'
megdc@node1:~/ceph-cluster$ ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc131bf5d40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc132054500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to node2
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage4/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage4/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Data dir /storage4/osd already exists
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

megdc@node1:~/ceph-cluster$ ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f78c7f35d40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f78c8394500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to node2
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage4/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage4/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Data dir /storage4/osd already exists
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

megdc@node1:~/ceph-cluster$ sudo nano /etc/ceph/ceph.conf 
megdc@node1:~/ceph-cluster$ ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc26530ed40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc26576d500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to node2
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.osd][ERROR ] RuntimeError: config file /etc/ceph/ceph.conf exists with different content; use --overwrite-conf to overwrite
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage4/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage4/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Data dir /storage4/osd already exists
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
[ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs

megdc@node1:~/ceph-cluster$ ls /etc/ceph/ceph.conf 
/etc/ceph/ceph.conf
megdc@node1:~/ceph-cluster$ sudo nano /etc/ceph/ceph.conf 
megdc@node1:~/ceph-cluster$ ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd^C
megdc@node1:~/ceph-cluster$ ceph-deploy --overwrite-conf osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy --overwrite-conf osd prepare node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : True
[ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f6126d42d40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f61271a1500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to node2
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[node2][WARNIN] osd keyring does not exist yet, creating one
[node2][DEBUG ] create a keyring file
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage3/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage3/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Data dir /storage3/osd already exists
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[node2][WARNIN] No data was received after 300 seconds, disconnecting...
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage4/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type xfs -- /storage4/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Data dir /storage4/osd already exists
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[node2][WARNIN] No data was received after 300 seconds, disconnecting...
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
megdc@node1:~/ceph-cluster$ ssh node2
Welcome to Ubuntu 14.04.4 LTS (GNU/Linux 4.2.0-35-generic x86_64)

 * Documentation:  https://help.ubuntu.com/
Last login: Thu May 19 09:14:58 2016 from node1
megdc@node2:~$ exit
logout
Connection to node2 closed.
megdc@node1:~/ceph-cluster$ sudo ceph --cluster=ceph osd stat --format=json

{"epoch":144,"num_osds":2,"num_up_osds":2,"num_in_osds":2,"full":false,"nearfull":false,"num_remapped_pgs":0}megdc@node1:~/ceph-cluster$ sudo ceph --cluster=ceph osd stat -sh node2^C
megdc@node1:~/ceph-cluster$ 
megdc@node1:~/ceph-cluster$ 
megdc@node1:~/ceph-cluster$ ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd^C
megdc@node1:~/ceph-cluster$ ceph-deploy uninstall node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy uninstall node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f43a817ac68>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  func                          : <function uninstall at 0x7f43a8a090c8>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[ceph_deploy.install][INFO  ] like: librbd1 and librados2
[ceph_deploy.install][DEBUG ] Uninstalling on cluster ceph hosts node2
[ceph_deploy.install][DEBUG ] Detecting platform for host node2 ...
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] Uninstalling Ceph on node2
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove ceph ceph-mds ceph-common ceph-fs-common radosgw
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[node2][DEBUG ] The following packages were automatically installed and are no longer required:
[node2][DEBUG ]   cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
[node2][DEBUG ]   libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
[node2][DEBUG ]   libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
[node2][DEBUG ]   python-flask python-itsdangerous python-rados python-rbd python-werkzeug
[node2][DEBUG ] Use 'apt-get autoremove' to remove them.
[node2][DEBUG ] The following packages will be REMOVED:
[node2][DEBUG ]   ceph ceph-common ceph-mds radosgw
[node2][DEBUG ] 0 upgraded, 0 newly installed, 4 to remove and 39 not upgraded.
[node2][DEBUG ] After this operation, 151 MB disk space will be freed.
[node2][DEBUG ] (Reading database ... 37108 files and directories currently installed.)
[node2][DEBUG ] Removing ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-mds-all stop/waiting
[node2][DEBUG ] Removing ceph (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-all stop/waiting
[node2][DEBUG ] Removing radosgw (9.2.1-1trusty) ...
[node2][WARNIN] stop: Unknown instance: 
[node2][DEBUG ] Removing ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[node2][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.7) ...
megdc@node1:~/ceph-cluster$ ceph-deploy purgedata node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy purgedata node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7bf622a998>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f7bf6b321b8>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts node2
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] purging data on node2
[node2][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[node2][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
megdc@node1:~/ceph-cluster$ ceph-deploy install node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy install node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  testing                       : None
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3696fdf0e0>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  dev_commit                    : None
[ceph_deploy.cli][INFO  ]  install_mds                   : False
[ceph_deploy.cli][INFO  ]  stable                        : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  adjust_repos                  : True
[ceph_deploy.cli][INFO  ]  func                          : <function install at 0x7f3697897de8>
[ceph_deploy.cli][INFO  ]  install_all                   : False
[ceph_deploy.cli][INFO  ]  repo                          : False
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  install_rgw                   : False
[ceph_deploy.cli][INFO  ]  install_tests                 : False
[ceph_deploy.cli][INFO  ]  repo_url                      : None
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  install_osd                   : False
[ceph_deploy.cli][INFO  ]  version_kind                  : stable
[ceph_deploy.cli][INFO  ]  install_common                : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  dev                           : master
[ceph_deploy.cli][INFO  ]  local_mirror                  : None
[ceph_deploy.cli][INFO  ]  release                       : None
[ceph_deploy.cli][INFO  ]  install_mon                   : False
[ceph_deploy.cli][INFO  ]  gpg_url                       : None
[ceph_deploy.install][DEBUG ] Installing stable version infernalis on cluster ceph hosts node2
[ceph_deploy.install][DEBUG ] Detecting platform for host node2 ...
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] installing Ceph on node2
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates apt-transport-https
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] apt-transport-https is already the newest version.
[node2][DEBUG ] ca-certificates is already the newest version.
[node2][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.
[node2][INFO  ] Running command: sudo wget -O release.asc https://download.ceph.com/keys/release.asc
[node2][WARNIN] --2016-05-19 15:01:38--  https://download.ceph.com/keys/release.asc
[node2][WARNIN] Resolving download.ceph.com (download.ceph.com)... 2607:f298:6050:51f3:f816:3eff:fe71:9135, 173.236.253.173
[node2][WARNIN] Connecting to download.ceph.com (download.ceph.com)|2607:f298:6050:51f3:f816:3eff:fe71:9135|:443... connected.
[node2][WARNIN] HTTP request sent, awaiting response... 200 OK
[node2][WARNIN] Length: 1645 (1.6K) [application/octet-stream]
[node2][WARNIN] Saving to: ‘release.asc’
[node2][WARNIN] 
[node2][WARNIN]      0K .                                                     100%  407M=0s
[node2][WARNIN] 
[node2][WARNIN] 2016-05-19 15:01:38 (407 MB/s) - ‘release.asc’ saved [1645/1645]
[node2][WARNIN] 
[node2][INFO  ] Running command: sudo apt-key add release.asc
[node2][DEBUG ] OK
[node2][DEBUG ] add deb repo to /etc/apt/sources.list.d/
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q update
[node2][DEBUG ] Ign http://mirror.hetzner.de trusty InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty Release.gpg
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty Release
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/universe amd64 Packages
[node2][DEBUG ] Ign http://de.archive.ubuntu.com trusty InRelease
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates InRelease
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/multiverse amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/restricted i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/universe i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/multiverse i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty Release.gpg
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/main Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/restricted Sources
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/universe amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/multiverse amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/restricted i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/universe Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/multiverse Sources
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/universe i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/multiverse i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/main amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/universe amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/multiverse amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/restricted i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/universe i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/multiverse i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/universe amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/multiverse amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/restricted i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/universe i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/multiverse i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/universe amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/multiverse amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/main i386 Packages
[node2][DEBUG ] Get:1 http://security.ubuntu.com trusty-security InRelease [65.9 kB]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/restricted i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/universe i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/multiverse i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty Release
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/main Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/restricted Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/universe Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/multiverse Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/main amd64 Packages
[node2][DEBUG ] Hit http://get.megam.io trusty InRelease
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/restricted amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/universe amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/multiverse amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/main i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/restricted i386 Packages
[node2][DEBUG ] Ign http://downloads.opennebula.org stable InRelease
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/universe i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/multiverse i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/main Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/restricted Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/universe Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/multiverse Sources
[node2][DEBUG ] Hit http://get.megam.io trusty/testing amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/main amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/restricted amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/universe amd64 Packages
[node2][DEBUG ] Get:2 http://security.ubuntu.com trusty-security/main Sources [116 kB]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/multiverse amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/main i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/restricted i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/universe i386 Packages
[node2][DEBUG ] Hit https://download.ceph.com trusty InRelease
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/multiverse i386 Packages
[node2][DEBUG ] Hit http://downloads.opennebula.org stable Release.gpg
[node2][DEBUG ] Hit https://download.ceph.com trusty/main amd64 Packages
[node2][DEBUG ] Hit https://download.ceph.com trusty/main i386 Packages
[node2][DEBUG ] Hit http://downloads.opennebula.org stable Release
[node2][DEBUG ] Get:3 http://security.ubuntu.com trusty-security/restricted Sources [4,035 B]
[node2][DEBUG ] Get:4 http://security.ubuntu.com trusty-security/universe Sources [36.2 kB]
[node2][DEBUG ] Hit http://downloads.opennebula.org stable/opennebula amd64 Packages
[node2][DEBUG ] Get:5 http://security.ubuntu.com trusty-security/multiverse Sources [2,760 B]
[node2][DEBUG ] Hit http://downloads.opennebula.org stable/opennebula i386 Packages
[node2][DEBUG ] Get:6 http://security.ubuntu.com trusty-security/main amd64 Packages [480 kB]
[node2][DEBUG ] Get:7 http://security.ubuntu.com trusty-security/restricted amd64 Packages [13.0 kB]
[node2][DEBUG ] Get:8 http://security.ubuntu.com trusty-security/universe amd64 Packages [128 kB]
[node2][DEBUG ] Get:9 http://security.ubuntu.com trusty-security/multiverse amd64 Packages [4,978 B]
[node2][DEBUG ] Get:10 http://security.ubuntu.com trusty-security/main i386 Packages [454 kB]
[node2][DEBUG ] Get:11 http://security.ubuntu.com trusty-security/restricted i386 Packages [12.7 kB]
[node2][DEBUG ] Get:12 http://security.ubuntu.com trusty-security/universe i386 Packages [129 kB]
[node2][DEBUG ] Get:13 http://security.ubuntu.com trusty-security/multiverse i386 Packages [5,168 B]
[node2][DEBUG ] Fetched 1,451 kB in 7s (193 kB/s)
[node2][DEBUG ] Reading package lists...
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew ceph ceph-mds radosgw
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] The following extra packages will be installed:
[node2][DEBUG ]   ceph-common cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
[node2][DEBUG ]   libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
[node2][DEBUG ]   libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
[node2][DEBUG ]   python-flask python-itsdangerous python-rados python-rbd python-werkzeug
[node2][DEBUG ] Suggested packages:
[node2][DEBUG ]   javascript-common python-flask-doc ipython python-genshi python-lxml
[node2][DEBUG ]   python-greenlet python-redis python-pylibmc python-memcache
[node2][DEBUG ]   python-werkzeug-doc
[node2][DEBUG ] Recommended packages:
[node2][DEBUG ]   ceph-fs-common ceph-fuse python-blinker python-openssl python-pyinotify
[node2][DEBUG ] The following NEW packages will be installed:
[node2][DEBUG ]   ceph ceph-common ceph-mds cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
[node2][DEBUG ]   libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
[node2][DEBUG ]   libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
[node2][DEBUG ]   python-flask python-itsdangerous python-rados python-rbd python-werkzeug
[node2][DEBUG ]   radosgw
[node2][DEBUG ] 0 upgraded, 25 newly installed, 0 to remove and 39 not upgraded.
[node2][DEBUG ] Need to get 0 B/39.7 MB of archives.
[node2][DEBUG ] After this operation, 174 MB of additional disk space will be used.
[node2][DEBUG ] Selecting previously unselected package libboost-program-options1.54.0:amd64.
[node2][DEBUG ] (Reading database ... 36458 files and directories currently installed.)
[node2][DEBUG ] Preparing to unpack .../libboost-program-options1.54.0_1.54.0-4ubuntu3.1_amd64.deb ...
[node2][DEBUG ] Unpacking libboost-program-options1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Selecting previously unselected package libboost-random1.54.0:amd64.
[node2][DEBUG ] Preparing to unpack .../libboost-random1.54.0_1.54.0-4ubuntu3.1_amd64.deb ...
[node2][DEBUG ] Unpacking libboost-random1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Selecting previously unselected package libsnappy1.
[node2][DEBUG ] Preparing to unpack .../libsnappy1_1.1.0-1ubuntu1_amd64.deb ...
[node2][DEBUG ] Unpacking libsnappy1 (1.1.0-1ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package libleveldb1:amd64.
[node2][DEBUG ] Preparing to unpack .../libleveldb1_1.15.0-2_amd64.deb ...
[node2][DEBUG ] Unpacking libleveldb1:amd64 (1.15.0-2) ...
[node2][DEBUG ] Selecting previously unselected package libunwind8.
[node2][DEBUG ] Preparing to unpack .../libunwind8_1.1-2.2ubuntu3_amd64.deb ...
[node2][DEBUG ] Unpacking libunwind8 (1.1-2.2ubuntu3) ...
[node2][DEBUG ] Selecting previously unselected package libbabeltrace1:amd64.
[node2][DEBUG ] Preparing to unpack .../libbabeltrace1_1.2.1-2_amd64.deb ...
[node2][DEBUG ] Unpacking libbabeltrace1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Selecting previously unselected package libbabeltrace-ctf1:amd64.
[node2][DEBUG ] Preparing to unpack .../libbabeltrace-ctf1_1.2.1-2_amd64.deb ...
[node2][DEBUG ] Unpacking libbabeltrace-ctf1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Selecting previously unselected package libradosstriper1.
[node2][DEBUG ] Preparing to unpack .../libradosstriper1_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking libradosstriper1 (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package python-rados.
[node2][DEBUG ] Preparing to unpack .../python-rados_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking python-rados (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libcephfs1.
[node2][DEBUG ] Preparing to unpack .../libcephfs1_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking libcephfs1 (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package python-cephfs.
[node2][DEBUG ] Preparing to unpack .../python-cephfs_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking python-cephfs (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package python-rbd.
[node2][DEBUG ] Preparing to unpack .../python-rbd_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking python-rbd (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package ceph-common.
[node2][DEBUG ] Preparing to unpack .../ceph-common_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libcryptsetup4.
[node2][DEBUG ] Preparing to unpack .../libcryptsetup4_2%3a1.6.1-1ubuntu1_amd64.deb ...
[node2][DEBUG ] Unpacking libcryptsetup4 (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package cryptsetup-bin.
[node2][DEBUG ] Preparing to unpack .../cryptsetup-bin_2%3a1.6.1-1ubuntu1_amd64.deb ...
[node2][DEBUG ] Unpacking cryptsetup-bin (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package libjs-jquery.
[node2][DEBUG ] Preparing to unpack .../libjs-jquery_1.7.2+dfsg-2ubuntu1_all.deb ...
[node2][DEBUG ] Unpacking libjs-jquery (1.7.2+dfsg-2ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package python-werkzeug.
[node2][DEBUG ] Preparing to unpack .../python-werkzeug_0.9.4+dfsg-1.1ubuntu2_all.deb ...
[node2][DEBUG ] Unpacking python-werkzeug (0.9.4+dfsg-1.1ubuntu2) ...
[node2][DEBUG ] Selecting previously unselected package python-itsdangerous.
[node2][DEBUG ] Preparing to unpack .../python-itsdangerous_0.22+dfsg1-1build1_all.deb ...
[node2][DEBUG ] Unpacking python-itsdangerous (0.22+dfsg1-1build1) ...
[node2][DEBUG ] Selecting previously unselected package python-flask.
[node2][DEBUG ] Preparing to unpack .../python-flask_0.10.1-2build1_all.deb ...
[node2][DEBUG ] Unpacking python-flask (0.10.1-2build1) ...
[node2][DEBUG ] Selecting previously unselected package libtcmalloc-minimal4.
[node2][DEBUG ] Preparing to unpack .../libtcmalloc-minimal4_2.1-2ubuntu1.1_amd64.deb ...
[node2][DEBUG ] Unpacking libtcmalloc-minimal4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Selecting previously unselected package libgoogle-perftools4.
[node2][DEBUG ] Preparing to unpack .../libgoogle-perftools4_2.1-2ubuntu1.1_amd64.deb ...
[node2][DEBUG ] Unpacking libgoogle-perftools4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Selecting previously unselected package ceph.
[node2][DEBUG ] Preparing to unpack .../ceph_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package ceph-mds.
[node2][DEBUG ] Preparing to unpack .../ceph-mds_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libfcgi0ldbl.
[node2][DEBUG ] Preparing to unpack .../libfcgi0ldbl_2.4.0-8.1ubuntu5_amd64.deb ...
[node2][DEBUG ] Unpacking libfcgi0ldbl (2.4.0-8.1ubuntu5) ...
[node2][DEBUG ] Selecting previously unselected package radosgw.
[node2][DEBUG ] Preparing to unpack .../radosgw_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking radosgw (9.2.1-1trusty) ...
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[node2][DEBUG ] Setting up libboost-program-options1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Setting up libboost-random1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Setting up libsnappy1 (1.1.0-1ubuntu1) ...
[node2][DEBUG ] Setting up libleveldb1:amd64 (1.15.0-2) ...
[node2][DEBUG ] Setting up libunwind8 (1.1-2.2ubuntu3) ...
[node2][DEBUG ] Setting up libbabeltrace1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Setting up libbabeltrace-ctf1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Setting up libradosstriper1 (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up python-rados (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up libcephfs1 (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up python-cephfs (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up python-rbd (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Setting system user ceph properties....done
[node2][WARNIN] usermod: no changes
[node2][DEBUG ] Setting up libcryptsetup4 (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Setting up cryptsetup-bin (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Setting up libjs-jquery (1.7.2+dfsg-2ubuntu1) ...
[node2][DEBUG ] Setting up python-werkzeug (0.9.4+dfsg-1.1ubuntu2) ...
[node2][DEBUG ] Setting up python-itsdangerous (0.22+dfsg1-1build1) ...
[node2][DEBUG ] Setting up python-flask (0.10.1-2build1) ...
[node2][DEBUG ] Setting up libtcmalloc-minimal4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Setting up libgoogle-perftools4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Setting up libfcgi0ldbl (2.4.0-8.1ubuntu5) ...
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][DEBUG ] Setting up radosgw (9.2.1-1trusty) ...
[node2][DEBUG ] radosgw-all start/running
[node2][DEBUG ] Setting up ceph (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-all start/running
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][DEBUG ] Setting up ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-mds-all start/running
[node2][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.7) ...
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][INFO  ] Running command: sudo ceph --version
[node2][DEBUG ] ceph version 9.2.1 (752b6a3020c3de74e07d2a8b4c5e48dab5a6b6fd)
megdc@node1:~/ceph-cluster$ ceph-deploy disk list node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy disk list node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : list
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa9ada18638>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function disk at 0x7fa9ade7c578>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', None, None)]
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Listing disks on node2...
[node2][DEBUG ] find the location of an executable
[node2][INFO  ] Running command: sudo /usr/sbin/ceph-disk list
[node2][DEBUG ] /dev/loop0 other, unknown
[node2][DEBUG ] /dev/loop1 other, unknown
[node2][DEBUG ] /dev/loop2 other, unknown
[node2][DEBUG ] /dev/loop3 other, unknown
[node2][DEBUG ] /dev/loop4 other, unknown
[node2][DEBUG ] /dev/loop5 other, unknown
[node2][DEBUG ] /dev/loop6 other, unknown
[node2][DEBUG ] /dev/loop7 other, unknown
[node2][DEBUG ] /dev/md0 other, ext3, mounted on /boot
[node2][DEBUG ] /dev/md1 swap, swap
[node2][DEBUG ] /dev/md2 other, ext4, mounted on /
[node2][DEBUG ] /dev/ram0 other, unknown
[node2][DEBUG ] /dev/ram1 other, unknown
[node2][DEBUG ] /dev/ram10 other, unknown
[node2][DEBUG ] /dev/ram11 other, unknown
[node2][DEBUG ] /dev/ram12 other, unknown
[node2][DEBUG ] /dev/ram13 other, unknown
[node2][DEBUG ] /dev/ram14 other, unknown
[node2][DEBUG ] /dev/ram15 other, unknown
[node2][DEBUG ] /dev/ram2 other, unknown
[node2][DEBUG ] /dev/ram3 other, unknown
[node2][DEBUG ] /dev/ram4 other, unknown
[node2][DEBUG ] /dev/ram5 other, unknown
[node2][DEBUG ] /dev/ram6 other, unknown
[node2][DEBUG ] /dev/ram7 other, unknown
[node2][DEBUG ] /dev/ram8 other, unknown
[node2][DEBUG ] /dev/ram9 other, unknown
[node2][DEBUG ] /dev/sda :
[node2][DEBUG ]  /dev/sda1 other, linux_raid_member
[node2][DEBUG ]  /dev/sda2 other, linux_raid_member
[node2][DEBUG ]  /dev/sda3 other, linux_raid_member
[node2][DEBUG ] /dev/sdb :
[node2][DEBUG ]  /dev/sdb1 other, linux_raid_member
[node2][DEBUG ]  /dev/sdb2 other, linux_raid_member
[node2][DEBUG ]  /dev/sdb3 other, linux_raid_member
[node2][DEBUG ] /dev/sdc :
[node2][DEBUG ]  /dev/sdc1 other, ext4, mounted on /storage3
[node2][DEBUG ] /dev/sdd :
[node2][DEBUG ]  /dev/sdd1 other, ext4, mounted on /storage4
megdc@node1:~/ceph-cluster$ ceph-deploy --fs-type ext4 osd prepare node2:/storage3/osd node2:/storage4/osd
usage: ceph-deploy [-h] [-v | -q] [--version] [--username USERNAME]
                   [--overwrite-conf] [--cluster NAME] [--ceph-conf CEPH_CONF]
                   COMMAND ...
ceph-deploy: error: argument COMMAND: invalid choice: 'ext4' (choose from 'new', 'install', 'rgw', 'mon', 'mds', 'gatherkeys', 'disk', 'osd', 'admin', 'repo', 'config', 'uninstall', 'purge', 'purgedata', 'calamari', 'forgetkeys', 'pkg')
megdc@node1:~/ceph-cluster$ ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd --fs-type ext4
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd --fs-type ext4
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7ede5d4d40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : ext4
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f7edea33500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to node2
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[node2][WARNIN] osd keyring does not exist yet, creating one
[node2][DEBUG ] create a keyring file
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage3/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type ext4 -- /storage3/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_ext4
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_ext4
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_ext4
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_ext4
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Data dir /storage3/osd already exists
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage4/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type ext4 -- /storage4/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_ext4
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_ext4
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_ext4
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_ext4
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Data dir /storage4/osd already exists
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
megdc@node1:~/ceph-cluster$ ceph-deploy osd activate node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy osd activate node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : activate
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f4609be9d40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f460a048500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.osd][DEBUG ] Activating cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] activating host node2 disk /storage3/osd
[ceph_deploy.osd][DEBUG ] will use init type: upstart
[node2][INFO  ] Running command: sudo ceph-disk -v activate --mark-init upstart --mount /storage3/osd
[node2][WARNIN] DEBUG:ceph-disk:Cluster uuid is 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] Traceback (most recent call last):
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 3589, in <module>
[node2][WARNIN]     main(sys.argv[1:])
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 3543, in main
[node2][WARNIN]     args.func(args)
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 2446, in main_activate
[node2][WARNIN]     init=args.mark_init,
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 2272, in activate_dir
[node2][WARNIN]     (osd_id, cluster) = activate(path, activate_key_template, init)
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 2345, in activate
[node2][WARNIN]     raise Error('No cluster conf found in ' + SYSCONFDIR + ' with fsid %s' % ceph_fsid)
[node2][WARNIN] __main__.Error: Error: No cluster conf found in /etc/ceph with fsid 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
[node2][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: ceph-disk -v activate --mark-init upstart --mount /storage3/osd

megdc@node1:~/ceph-cluster$ ceph-deploy osd activate node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy osd activate node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : activate
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f04e23f1d40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f04e2850500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.osd][DEBUG ] Activating cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] activating host node2 disk /storage3/osd
[ceph_deploy.osd][DEBUG ] will use init type: upstart
[node2][INFO  ] Running command: sudo ceph-disk -v activate --mark-init upstart --mount /storage3/osd
[node2][WARNIN] DEBUG:ceph-disk:Cluster uuid is 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] DEBUG:ceph-disk:Cluster name is ceph
[node2][WARNIN] DEBUG:ceph-disk:OSD uuid is 85169284-fab7-40df-acdb-928436978d82
[node2][WARNIN] DEBUG:ceph-disk:Allocating OSD id...
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 85169284-fab7-40df-acdb-928436978d82
[node2][WARNIN] INFO:ceph-disk:Running command: /bin/chown -R ceph:ceph /storage3/osd/whoami.22366.tmp
[node2][WARNIN] DEBUG:ceph-disk:OSD id is 2
[node2][WARNIN] DEBUG:ceph-disk:Initializing OSD...
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /storage3/osd/activate.monmap
[node2][WARNIN] got monmap epoch 1
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster ceph --mkfs --mkkey -i 2 --monmap /storage3/osd/activate.monmap --osd-data /storage3/osd --osd-journal /storage3/osd/journal --osd-uuid 85169284-fab7-40df-acdb-928436978d82 --keyring /storage3/osd/keyring --setuser ceph --setgroup ceph
[node2][WARNIN] 2016-05-19 15:13:43.143204 7fa4a9da8940 -1 asok(0x563e4efa6200) AdminSocketConfigObs::init: failed: AdminSocket::bind_and_listen: failed to bind the UNIX domain socket to '/var/run/ceph/ceph-osd.2.asok': (13) Permission denied
[node2][WARNIN] 2016-05-19 15:13:43.198135 7fa4a9da8940 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway
[node2][WARNIN] 2016-05-19 15:13:43.526165 7fa4a9da8940 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway
[node2][WARNIN] 2016-05-19 15:13:43.530615 7fa4a9da8940 -1 filestore(/storage3/osd) could not find -1/23c2fcde/osd_superblock/0 in index: (2) No such file or directory
[node2][WARNIN] 2016-05-19 15:13:43.718410 7fa4a9da8940 -1 created object store /storage3/osd journal /storage3/osd/journal for osd.2 fsid 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
[node2][WARNIN] 2016-05-19 15:13:43.718456 7fa4a9da8940 -1 auth: error reading file: /storage3/osd/keyring: can't open /storage3/osd/keyring: (2) No such file or directory
[node2][WARNIN] 2016-05-19 15:13:43.718590 7fa4a9da8940 -1 created new key in keyring /storage3/osd/keyring
[node2][WARNIN] DEBUG:ceph-disk:Marking with init system upstart
[node2][WARNIN] DEBUG:ceph-disk:Authorizing OSD key...
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.2 -i /storage3/osd/keyring osd allow * mon allow profile osd
[node2][WARNIN] Error EINVAL: entity osd.2 exists but key does not match
[node2][WARNIN] Traceback (most recent call last):
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 3589, in <module>
[node2][WARNIN]     main(sys.argv[1:])
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 3543, in main
[node2][WARNIN]     args.func(args)
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 2446, in main_activate
[node2][WARNIN]     init=args.mark_init,
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 2272, in activate_dir
[node2][WARNIN]     (osd_id, cluster) = activate(path, activate_key_template, init)
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 2406, in activate
[node2][WARNIN]     keyring=keyring,
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 1988, in auth_key
[node2][WARNIN]     'mon', 'allow profile osd',
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 350, in command_check_call
[node2][WARNIN]     return subprocess.check_call(arguments)
[node2][WARNIN]   File "/usr/lib/python2.7/subprocess.py", line 540, in check_call
[node2][WARNIN]     raise CalledProcessError(retcode, cmd)
[node2][WARNIN] subprocess.CalledProcessError: Command '['/usr/bin/ceph', '--cluster', 'ceph', '--name', 'client.bootstrap-osd', '--keyring', '/var/lib/ceph/bootstrap-osd/ceph.keyring', 'auth', 'add', 'osd.2', '-i', '/storage3/osd/keyring', 'osd', 'allow *', 'mon', 'allow profile osd']' returned non-zero exit status 22
[node2][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: ceph-disk -v activate --mark-init upstart --mount /storage3/osd

megdc@node1:~/ceph-cluster$ sudo cat /storage1/osd/keyring 
[osd.1]
	key = AQC+ayNXRujBORAACiIcwec73KEwOf1R91hCNQ==
megdc@node1:~/ceph-cluster$ sudo cat /storage2/osd/keyring 
[osd.0]
	key = AQC3ayNXHfdIKBAAFGaYQHEXAIgJeGkZsDttrA==
megdc@node1:~/ceph-cluster$ ceph-deploy osd activate node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy osd activate node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : activate
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f33963a8d40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f3396807500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.osd][DEBUG ] Activating cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] activating host node2 disk /storage3/osd
[ceph_deploy.osd][DEBUG ] will use init type: upstart
[node2][INFO  ] Running command: sudo ceph-disk -v activate --mark-init upstart --mount /storage3/osd
[node2][WARNIN] DEBUG:ceph-disk:Cluster uuid is 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] DEBUG:ceph-disk:Cluster name is ceph
[node2][WARNIN] DEBUG:ceph-disk:OSD uuid is 85169284-fab7-40df-acdb-928436978d82
[node2][WARNIN] DEBUG:ceph-disk:OSD id is 2
[node2][WARNIN] DEBUG:ceph-disk:Marking with init system upstart
[node2][WARNIN] DEBUG:ceph-disk:Authorizing OSD key...
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.2 -i /storage3/osd/keyring osd allow * mon allow profile osd
[node2][WARNIN] Error EINVAL: entity osd.2 exists but key does not match
[node2][WARNIN] Traceback (most recent call last):
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 3589, in <module>
[node2][WARNIN]     main(sys.argv[1:])
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 3543, in main
[node2][WARNIN]     args.func(args)
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 2446, in main_activate
[node2][WARNIN]     init=args.mark_init,
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 2272, in activate_dir
[node2][WARNIN]     (osd_id, cluster) = activate(path, activate_key_template, init)
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 2406, in activate
[node2][WARNIN]     keyring=keyring,
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 1988, in auth_key
[node2][WARNIN]     'mon', 'allow profile osd',
[node2][WARNIN]   File "/usr/sbin/ceph-disk", line 350, in command_check_call
[node2][WARNIN]     return subprocess.check_call(arguments)
[node2][WARNIN]   File "/usr/lib/python2.7/subprocess.py", line 540, in check_call
[node2][WARNIN]     raise CalledProcessError(retcode, cmd)
[node2][WARNIN] subprocess.CalledProcessError: Command '['/usr/bin/ceph', '--cluster', 'ceph', '--name', 'client.bootstrap-osd', '--keyring', '/var/lib/ceph/bootstrap-osd/ceph.keyring', 'auth', 'add', 'osd.2', '-i', '/storage3/osd/keyring', 'osd', 'allow *', 'mon', 'allow profile osd']' returned non-zero exit status 22
[node2][ERROR ] RuntimeError: command returned non-zero exit status: 1
[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: ceph-disk -v activate --mark-init upstart --mount /storage3/osd

megdc@node1:~/ceph-cluster$ ceph-deploy uninstall node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy uninstall node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdbce6cec68>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  func                          : <function uninstall at 0x7fdbcef5d0c8>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[ceph_deploy.install][INFO  ] like: librbd1 and librados2
[ceph_deploy.install][DEBUG ] Uninstalling on cluster ceph hosts node2
[ceph_deploy.install][DEBUG ] Detecting platform for host node2 ...
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] Uninstalling Ceph on node2
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove ceph ceph-mds ceph-common ceph-fs-common radosgw
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[node2][DEBUG ] The following packages were automatically installed and are no longer required:
[node2][DEBUG ]   cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
[node2][DEBUG ]   libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
[node2][DEBUG ]   libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
[node2][DEBUG ]   python-flask python-itsdangerous python-rados python-rbd python-werkzeug
[node2][DEBUG ] Use 'apt-get autoremove' to remove them.
[node2][DEBUG ] The following packages will be REMOVED:
[node2][DEBUG ]   ceph ceph-common ceph-mds radosgw
[node2][DEBUG ] 0 upgraded, 0 newly installed, 4 to remove and 39 not upgraded.
[node2][DEBUG ] After this operation, 151 MB disk space will be freed.
[node2][DEBUG ] (Reading database ... 37108 files and directories currently installed.)
[node2][DEBUG ] Removing ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-mds-all stop/waiting
[node2][DEBUG ] Removing ceph (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-all stop/waiting
[node2][DEBUG ] Removing radosgw (9.2.1-1trusty) ...
[node2][WARNIN] stop: Unknown instance: 
[node2][DEBUG ] Removing ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[node2][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.7) ...
megdc@node1:~/ceph-cluster$ ceph-deploy purgedata node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy purgedata node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f58c7b46998>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f58c844e1b8>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts node2
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] purging data on node2
[node2][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[node2][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
megdc@node1:~/ceph-cluster$ ceph-deploy purge node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy purge node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f623c80c320>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f623d114140>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[ceph_deploy.install][INFO  ] like: librbd1 and librados2
[ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts node2
[ceph_deploy.install][DEBUG ] Detecting platform for host node2 ...
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] Purging Ceph on node2
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[node2][DEBUG ] The following packages were automatically installed and are no longer required:
[node2][DEBUG ]   cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
[node2][DEBUG ]   libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
[node2][DEBUG ]   libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
[node2][DEBUG ]   python-flask python-itsdangerous python-rados python-rbd python-werkzeug
[node2][DEBUG ] Use 'apt-get autoremove' to remove them.
[node2][DEBUG ] The following packages will be REMOVED:
[node2][DEBUG ]   ceph* ceph-common* ceph-mds* radosgw*
[node2][DEBUG ] 0 upgraded, 0 newly installed, 4 to remove and 39 not upgraded.
[node2][DEBUG ] After this operation, 0 B of additional disk space will be used.
[node2][DEBUG ] (Reading database ... 36818 files and directories currently installed.)
[node2][DEBUG ] Removing ceph (9.2.1-1trusty) ...
[node2][DEBUG ] Purging configuration files for ceph (9.2.1-1trusty) ...
[node2][DEBUG ] Removing ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Purging configuration files for ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Removing ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] Purging configuration files for ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] Removing radosgw (9.2.1-1trusty) ...
[node2][DEBUG ] Purging configuration files for radosgw (9.2.1-1trusty) ...
megdc@node1:~/ceph-cluster$ ceph-deploy install node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy install node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  testing                       : None
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc242a630e0>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  dev_commit                    : None
[ceph_deploy.cli][INFO  ]  install_mds                   : False
[ceph_deploy.cli][INFO  ]  stable                        : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  adjust_repos                  : True
[ceph_deploy.cli][INFO  ]  func                          : <function install at 0x7fc24331bde8>
[ceph_deploy.cli][INFO  ]  install_all                   : False
[ceph_deploy.cli][INFO  ]  repo                          : False
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  install_rgw                   : False
[ceph_deploy.cli][INFO  ]  install_tests                 : False
[ceph_deploy.cli][INFO  ]  repo_url                      : None
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  install_osd                   : False
[ceph_deploy.cli][INFO  ]  version_kind                  : stable
[ceph_deploy.cli][INFO  ]  install_common                : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  dev                           : master
[ceph_deploy.cli][INFO  ]  local_mirror                  : None
[ceph_deploy.cli][INFO  ]  release                       : None
[ceph_deploy.cli][INFO  ]  install_mon                   : False
[ceph_deploy.cli][INFO  ]  gpg_url                       : None
[ceph_deploy.install][DEBUG ] Installing stable version infernalis on cluster ceph hosts node2
[ceph_deploy.install][DEBUG ] Detecting platform for host node2 ...
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] installing Ceph on node2
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates apt-transport-https
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] apt-transport-https is already the newest version.
[node2][DEBUG ] ca-certificates is already the newest version.
[node2][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.
[node2][INFO  ] Running command: sudo wget -O release.asc https://download.ceph.com/keys/release.asc
[node2][WARNIN] --2016-05-19 15:23:09--  https://download.ceph.com/keys/release.asc
[node2][WARNIN] Resolving download.ceph.com (download.ceph.com)... 2607:f298:6050:51f3:f816:3eff:fe71:9135, 173.236.253.173
[node2][WARNIN] Connecting to download.ceph.com (download.ceph.com)|2607:f298:6050:51f3:f816:3eff:fe71:9135|:443... connected.
[node2][WARNIN] HTTP request sent, awaiting response... 200 OK
[node2][WARNIN] Length: 1645 (1.6K) [application/octet-stream]
[node2][WARNIN] Saving to: ‘release.asc’
[node2][WARNIN] 
[node2][WARNIN]      0K .                                                     100%  381M=0s
[node2][WARNIN] 
[node2][WARNIN] 2016-05-19 15:23:10 (381 MB/s) - ‘release.asc’ saved [1645/1645]
[node2][WARNIN] 
[node2][INFO  ] Running command: sudo apt-key add release.asc
[node2][DEBUG ] OK
[node2][DEBUG ] add deb repo to /etc/apt/sources.list.d/
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q update
[node2][DEBUG ] Ign http://mirror.hetzner.de trusty InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty Release.gpg
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty Release
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/universe amd64 Packages
[node2][DEBUG ] Ign http://de.archive.ubuntu.com trusty InRelease
[node2][DEBUG ] Get:1 http://security.ubuntu.com trusty-security InRelease [65.9 kB]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates InRelease
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/multiverse amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/restricted i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/universe i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/multiverse i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty Release.gpg
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/main Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/restricted Sources
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/universe amd64 Packages
[node2][DEBUG ] Get:2 http://security.ubuntu.com trusty-security/main Sources [116 kB]
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/multiverse amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/restricted i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/universe Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/multiverse Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/universe i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/multiverse i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/universe amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/multiverse amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/restricted i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/universe i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/multiverse i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/universe amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/multiverse amd64 Packages
[node2][DEBUG ] Get:3 http://security.ubuntu.com trusty-security/restricted Sources [4,035 B]
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/restricted i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/universe i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/multiverse i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/restricted amd64 Packages
[node2][DEBUG ] Get:4 http://security.ubuntu.com trusty-security/universe Sources [36.2 kB]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/universe amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/multiverse amd64 Packages
[node2][DEBUG ] Get:5 http://security.ubuntu.com trusty-security/multiverse Sources [2,760 B]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/main i386 Packages
[node2][DEBUG ] Get:6 http://security.ubuntu.com trusty-security/main amd64 Packages [480 kB]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/restricted i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/universe i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/multiverse i386 Packages
[node2][DEBUG ] Ign http://downloads.opennebula.org stable InRelease
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty Release
[node2][DEBUG ] Get:7 http://security.ubuntu.com trusty-security/restricted amd64 Packages [13.0 kB]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/main Sources
[node2][DEBUG ] Get:8 http://security.ubuntu.com trusty-security/universe amd64 Packages [128 kB]
[node2][DEBUG ] Get:9 http://security.ubuntu.com trusty-security/multiverse amd64 Packages [4,978 B]
[node2][DEBUG ] Hit http://get.megam.io trusty InRelease
[node2][DEBUG ] Get:10 http://security.ubuntu.com trusty-security/main i386 Packages [454 kB]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/restricted Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/universe Sources
[node2][DEBUG ] Get:11 http://security.ubuntu.com trusty-security/restricted i386 Packages [12.7 kB]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/multiverse Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/main amd64 Packages
[node2][DEBUG ] Get:12 http://security.ubuntu.com trusty-security/universe i386 Packages [129 kB]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/restricted amd64 Packages
[node2][DEBUG ] Get:13 http://security.ubuntu.com trusty-security/multiverse i386 Packages [5,168 B]
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/universe amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/multiverse amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/main i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/restricted i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/universe i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/multiverse i386 Packages
[node2][DEBUG ] Hit http://get.megam.io trusty/testing amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/main Sources
[node2][DEBUG ] Hit http://downloads.opennebula.org stable Release.gpg
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/restricted Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/universe Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/multiverse Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/main amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/restricted amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/universe amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/multiverse amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/main i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/restricted i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/universe i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/multiverse i386 Packages
[node2][DEBUG ] Hit https://download.ceph.com trusty InRelease
[node2][DEBUG ] Hit http://downloads.opennebula.org stable Release
[node2][DEBUG ] Hit https://download.ceph.com trusty/main amd64 Packages
[node2][DEBUG ] Hit https://download.ceph.com trusty/main i386 Packages
[node2][DEBUG ] Hit http://downloads.opennebula.org stable/opennebula amd64 Packages
[node2][DEBUG ] Hit http://downloads.opennebula.org stable/opennebula i386 Packages
[node2][DEBUG ] Fetched 1,451 kB in 3s (471 kB/s)
[node2][DEBUG ] Reading package lists...
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew ceph ceph-mds radosgw
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] The following extra packages will be installed:
[node2][DEBUG ]   ceph-common cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
[node2][DEBUG ]   libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
[node2][DEBUG ]   libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
[node2][DEBUG ]   python-flask python-itsdangerous python-rados python-rbd python-werkzeug
[node2][DEBUG ] Suggested packages:
[node2][DEBUG ]   javascript-common python-flask-doc ipython python-genshi python-lxml
[node2][DEBUG ]   python-greenlet python-redis python-pylibmc python-memcache
[node2][DEBUG ]   python-werkzeug-doc
[node2][DEBUG ] Recommended packages:
[node2][DEBUG ]   ceph-fs-common ceph-fuse python-blinker python-openssl python-pyinotify
[node2][DEBUG ] The following NEW packages will be installed:
[node2][DEBUG ]   ceph ceph-common ceph-mds cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
[node2][DEBUG ]   libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
[node2][DEBUG ]   libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
[node2][DEBUG ]   python-flask python-itsdangerous python-rados python-rbd python-werkzeug
[node2][DEBUG ]   radosgw
[node2][DEBUG ] 0 upgraded, 25 newly installed, 0 to remove and 39 not upgraded.
[node2][DEBUG ] Need to get 0 B/39.7 MB of archives.
[node2][DEBUG ] After this operation, 174 MB of additional disk space will be used.
[node2][DEBUG ] Selecting previously unselected package libboost-program-options1.54.0:amd64.
[node2][DEBUG ] (Reading database ... 36453 files and directories currently installed.)
[node2][DEBUG ] Preparing to unpack .../libboost-program-options1.54.0_1.54.0-4ubuntu3.1_amd64.deb ...
[node2][DEBUG ] Unpacking libboost-program-options1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Selecting previously unselected package libboost-random1.54.0:amd64.
[node2][DEBUG ] Preparing to unpack .../libboost-random1.54.0_1.54.0-4ubuntu3.1_amd64.deb ...
[node2][DEBUG ] Unpacking libboost-random1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Selecting previously unselected package libsnappy1.
[node2][DEBUG ] Preparing to unpack .../libsnappy1_1.1.0-1ubuntu1_amd64.deb ...
[node2][DEBUG ] Unpacking libsnappy1 (1.1.0-1ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package libleveldb1:amd64.
[node2][DEBUG ] Preparing to unpack .../libleveldb1_1.15.0-2_amd64.deb ...
[node2][DEBUG ] Unpacking libleveldb1:amd64 (1.15.0-2) ...
[node2][DEBUG ] Selecting previously unselected package libunwind8.
[node2][DEBUG ] Preparing to unpack .../libunwind8_1.1-2.2ubuntu3_amd64.deb ...
[node2][DEBUG ] Unpacking libunwind8 (1.1-2.2ubuntu3) ...
[node2][DEBUG ] Selecting previously unselected package libbabeltrace1:amd64.
[node2][DEBUG ] Preparing to unpack .../libbabeltrace1_1.2.1-2_amd64.deb ...
[node2][DEBUG ] Unpacking libbabeltrace1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Selecting previously unselected package libbabeltrace-ctf1:amd64.
[node2][DEBUG ] Preparing to unpack .../libbabeltrace-ctf1_1.2.1-2_amd64.deb ...
[node2][DEBUG ] Unpacking libbabeltrace-ctf1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Selecting previously unselected package libradosstriper1.
[node2][DEBUG ] Preparing to unpack .../libradosstriper1_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking libradosstriper1 (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package python-rados.
[node2][DEBUG ] Preparing to unpack .../python-rados_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking python-rados (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libcephfs1.
[node2][DEBUG ] Preparing to unpack .../libcephfs1_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking libcephfs1 (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package python-cephfs.
[node2][DEBUG ] Preparing to unpack .../python-cephfs_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking python-cephfs (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package python-rbd.
[node2][DEBUG ] Preparing to unpack .../python-rbd_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking python-rbd (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package ceph-common.
[node2][DEBUG ] Preparing to unpack .../ceph-common_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libcryptsetup4.
[node2][DEBUG ] Preparing to unpack .../libcryptsetup4_2%3a1.6.1-1ubuntu1_amd64.deb ...
[node2][DEBUG ] Unpacking libcryptsetup4 (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package cryptsetup-bin.
[node2][DEBUG ] Preparing to unpack .../cryptsetup-bin_2%3a1.6.1-1ubuntu1_amd64.deb ...
[node2][DEBUG ] Unpacking cryptsetup-bin (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package libjs-jquery.
[node2][DEBUG ] Preparing to unpack .../libjs-jquery_1.7.2+dfsg-2ubuntu1_all.deb ...
[node2][DEBUG ] Unpacking libjs-jquery (1.7.2+dfsg-2ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package python-werkzeug.
[node2][DEBUG ] Preparing to unpack .../python-werkzeug_0.9.4+dfsg-1.1ubuntu2_all.deb ...
[node2][DEBUG ] Unpacking python-werkzeug (0.9.4+dfsg-1.1ubuntu2) ...
[node2][DEBUG ] Selecting previously unselected package python-itsdangerous.
[node2][DEBUG ] Preparing to unpack .../python-itsdangerous_0.22+dfsg1-1build1_all.deb ...
[node2][DEBUG ] Unpacking python-itsdangerous (0.22+dfsg1-1build1) ...
[node2][DEBUG ] Selecting previously unselected package python-flask.
[node2][DEBUG ] Preparing to unpack .../python-flask_0.10.1-2build1_all.deb ...
[node2][DEBUG ] Unpacking python-flask (0.10.1-2build1) ...
[node2][DEBUG ] Selecting previously unselected package libtcmalloc-minimal4.
[node2][DEBUG ] Preparing to unpack .../libtcmalloc-minimal4_2.1-2ubuntu1.1_amd64.deb ...
[node2][DEBUG ] Unpacking libtcmalloc-minimal4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Selecting previously unselected package libgoogle-perftools4.
[node2][DEBUG ] Preparing to unpack .../libgoogle-perftools4_2.1-2ubuntu1.1_amd64.deb ...
[node2][DEBUG ] Unpacking libgoogle-perftools4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Selecting previously unselected package ceph.
[node2][DEBUG ] Preparing to unpack .../ceph_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package ceph-mds.
[node2][DEBUG ] Preparing to unpack .../ceph-mds_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libfcgi0ldbl.
[node2][DEBUG ] Preparing to unpack .../libfcgi0ldbl_2.4.0-8.1ubuntu5_amd64.deb ...
[node2][DEBUG ] Unpacking libfcgi0ldbl (2.4.0-8.1ubuntu5) ...
[node2][DEBUG ] Selecting previously unselected package radosgw.
[node2][DEBUG ] Preparing to unpack .../radosgw_9.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking radosgw (9.2.1-1trusty) ...
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[node2][DEBUG ] Setting up libboost-program-options1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Setting up libboost-random1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Setting up libsnappy1 (1.1.0-1ubuntu1) ...
[node2][DEBUG ] Setting up libleveldb1:amd64 (1.15.0-2) ...
[node2][DEBUG ] Setting up libunwind8 (1.1-2.2ubuntu3) ...
[node2][DEBUG ] Setting up libbabeltrace1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Setting up libbabeltrace-ctf1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Setting up libradosstriper1 (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up python-rados (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up libcephfs1 (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up python-cephfs (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up python-rbd (9.2.1-1trusty) ...
[node2][DEBUG ] Setting up ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Setting system user ceph properties....done
[node2][WARNIN] usermod: no changes
[node2][DEBUG ] Setting up libcryptsetup4 (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Setting up cryptsetup-bin (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Setting up libjs-jquery (1.7.2+dfsg-2ubuntu1) ...
[node2][DEBUG ] Setting up python-werkzeug (0.9.4+dfsg-1.1ubuntu2) ...
[node2][DEBUG ] Setting up python-itsdangerous (0.22+dfsg1-1build1) ...
[node2][DEBUG ] Setting up python-flask (0.10.1-2build1) ...
[node2][DEBUG ] Setting up libtcmalloc-minimal4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Setting up libgoogle-perftools4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Setting up libfcgi0ldbl (2.4.0-8.1ubuntu5) ...
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][DEBUG ] Setting up radosgw (9.2.1-1trusty) ...
[node2][DEBUG ] radosgw-all start/running
[node2][DEBUG ] Setting up ceph (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-all start/running
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][DEBUG ] Setting up ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-mds-all start/running
[node2][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.7) ...
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][INFO  ] Running command: sudo ceph --version
[node2][DEBUG ] ceph version 9.2.1 (752b6a3020c3de74e07d2a8b4c5e48dab5a6b6fd)
megdc@node1:~/ceph-cluster$ ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd --fs-type ext4
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd --fs-type ext4
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5182f89d40>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : ext4
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f51833e8500>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to node2
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[node2][WARNIN] osd keyring does not exist yet, creating one
[node2][DEBUG ] create a keyring file
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage3/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type ext4 -- /storage3/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_ext4
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_ext4
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_ext4
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_ext4
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Data dir /storage3/osd already exists
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage4/osd journal None activate False
[node2][INFO  ] Running command: sudo ceph-disk -v prepare --cluster ceph --fs-type ext4 -- /storage4/osd
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_ext4
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_ext4
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_ext4
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_ext4
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size
[node2][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type
[node2][WARNIN] DEBUG:ceph-disk:Data dir /storage4/osd already exists
[node2][INFO  ] checking OSD status...
[node2][INFO  ] Running command: sudo ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
megdc@node1:~/ceph-cluster$ ceph-deploy uninstall node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy uninstall node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f43cb5e8c68>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  func                          : <function uninstall at 0x7f43cbe770c8>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[ceph_deploy.install][INFO  ] like: librbd1 and librados2
[ceph_deploy.install][DEBUG ] Uninstalling on cluster ceph hosts node2
[ceph_deploy.install][DEBUG ] Detecting platform for host node2 ...
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] Uninstalling Ceph on node2
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove ceph ceph-mds ceph-common ceph-fs-common radosgw
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[node2][DEBUG ] The following packages were automatically installed and are no longer required:
[node2][DEBUG ]   cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
[node2][DEBUG ]   libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
[node2][DEBUG ]   libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
[node2][DEBUG ]   python-flask python-itsdangerous python-rados python-rbd python-werkzeug
[node2][DEBUG ] Use 'apt-get autoremove' to remove them.
[node2][DEBUG ] The following packages will be REMOVED:
[node2][DEBUG ]   ceph ceph-common ceph-mds radosgw
[node2][DEBUG ] 0 upgraded, 0 newly installed, 4 to remove and 39 not upgraded.
[node2][DEBUG ] After this operation, 151 MB disk space will be freed.
[node2][DEBUG ] (Reading database ... 37108 files and directories currently installed.)
[node2][DEBUG ] Removing ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-mds-all stop/waiting
[node2][DEBUG ] Removing ceph (9.2.1-1trusty) ...
[node2][DEBUG ] ceph-all stop/waiting
[node2][DEBUG ] Removing radosgw (9.2.1-1trusty) ...
[node2][WARNIN] stop: Unknown instance: 
[node2][DEBUG ] Removing ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[node2][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.7) ...
megdc@node1:~/ceph-cluster$ ceph-deploy purgedata node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy purgedata node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3846b22998>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  func                          : <function purgedata at 0x7f384742a1b8>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.install][DEBUG ] Purging data from cluster ceph hosts node2
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] purging data on node2
[node2][INFO  ] Running command: sudo rm -rf --one-file-system -- /var/lib/ceph
[node2][INFO  ] Running command: sudo rm -rf --one-file-system -- /etc/ceph/
megdc@node1:~/ceph-cluster$ ceph-deploy purge node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.31): /usr/bin/ceph-deploy purge node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f0cbe16f320>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  func                          : <function purge at 0x7f0cbea77140>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.install][INFO  ] note that some dependencies *will not* be removed because they can cause issues with qemu-kvm
[ceph_deploy.install][INFO  ] like: librbd1 and librados2
[ceph_deploy.install][DEBUG ] Purging on cluster ceph hosts node2
[ceph_deploy.install][DEBUG ] Detecting platform for host node2 ...
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] Purging Ceph on node2
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q -f --force-yes remove --purge ceph ceph-mds ceph-common ceph-fs-common radosgw
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] Package 'ceph-fs-common' is not installed, so not removed
[node2][DEBUG ] The following packages were automatically installed and are no longer required:
[node2][DEBUG ]   cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
[node2][DEBUG ]   libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
[node2][DEBUG ]   libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
[node2][DEBUG ]   python-flask python-itsdangerous python-rados python-rbd python-werkzeug
[node2][DEBUG ] Use 'apt-get autoremove' to remove them.
[node2][DEBUG ] The following packages will be REMOVED:
[node2][DEBUG ]   ceph* ceph-common* ceph-mds* radosgw*
[node2][DEBUG ] 0 upgraded, 0 newly installed, 4 to remove and 39 not upgraded.
[node2][DEBUG ] After this operation, 0 B of additional disk space will be used.
[node2][DEBUG ] (Reading database ... 36818 files and directories currently installed.)
[node2][DEBUG ] Removing ceph (9.2.1-1trusty) ...
[node2][DEBUG ] Purging configuration files for ceph (9.2.1-1trusty) ...
[node2][DEBUG ] Removing ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Purging configuration files for ceph-common (9.2.1-1trusty) ...
[node2][DEBUG ] Removing ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] Purging configuration files for ceph-mds (9.2.1-1trusty) ...
[node2][DEBUG ] Removing radosgw (9.2.1-1trusty) ...
[node2][DEBUG ] Purging configuration files for radosgw (9.2.1-1trusty) ...
megdc@node1:~/ceph-cluster$ sudo apt-get -y remove ceph-deploy ceph-common ceph-mds ceph
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following packages were automatically installed and are no longer required:
  cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
  libboost-program-options1.54.0 libboost-random1.54.0 libcephfs1
  libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
  libradosstriper1 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
  python-flask python-itsdangerous python-jinja2 python-markupsafe
  python-pkg-resources python-rados python-rbd python-setuptools
  python-werkzeug
Use 'apt-get autoremove' to remove them.
The following packages will be REMOVED:
  ceph ceph-common ceph-deploy ceph-mds radosgw
0 upgraded, 0 newly installed, 5 to remove and 11 not upgraded.
After this operation, 152 MB disk space will be freed.
(Reading database ... 37404 files and directories currently installed.)
Removing ceph-mds (9.2.1-1trusty) ...
ceph-mds-all stop/waiting
=== mds.a === 
Stopping Ceph mds.a on node1...done
Removing ceph (9.2.1-1trusty) ...
ceph-all stop/waiting
=== mds.a === 
Stopping Ceph mds.a on node1...done
=== mon.a === 
Stopping Ceph mon.a on node1...done
Removing radosgw (9.2.1-1trusty) ...
stop: Unknown instance: 
Removing ceph-common (9.2.1-1trusty) ...
Removing ceph-deploy (1.5.31) ...
Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
Processing triggers for libc-bin (2.19-0ubuntu6.7) ...
megdc@node1:~/ceph-cluster$ sudo apt-get install ceph-deploy ceph-common ceph-mds ceph
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following package was automatically installed and is no longer required:
  libfcgi0ldbl
Use 'apt-get autoremove' to remove it.
The following extra packages will be installed:
  ceph-fs-common ceph-fuse
The following NEW packages will be installed:
  ceph ceph-common ceph-deploy ceph-fs-common ceph-fuse ceph-mds
0 upgraded, 6 newly installed, 0 to remove and 11 not upgraded.
Need to get 92.5 kB/31.4 MB of archives.
After this operation, 146 MB of additional disk space will be used.
Do you want to continue? [Y/n] y
Get:1 https://download.ceph.com/debian-infernalis/ trusty/main ceph-deploy all 1.5.33 [92.5 kB]
Fetched 92.5 kB in 1s (64.5 kB/s)
Selecting previously unselected package ceph-common.
(Reading database ... 36953 files and directories currently installed.)
Preparing to unpack .../ceph-common_9.2.1-1trusty_amd64.deb ...
Unpacking ceph-common (9.2.1-1trusty) ...
Selecting previously unselected package ceph.
Preparing to unpack .../ceph_9.2.1-1trusty_amd64.deb ...
Unpacking ceph (9.2.1-1trusty) ...
Selecting previously unselected package ceph-deploy.
Preparing to unpack .../ceph-deploy_1.5.33_all.deb ...
Unpacking ceph-deploy (1.5.33) ...
Selecting previously unselected package ceph-fs-common.
Preparing to unpack .../ceph-fs-common_9.2.1-1trusty_amd64.deb ...
Unpacking ceph-fs-common (9.2.1-1trusty) ...
Selecting previously unselected package ceph-fuse.
Preparing to unpack .../ceph-fuse_9.2.1-1trusty_amd64.deb ...
Unpacking ceph-fuse (9.2.1-1trusty) ...
Selecting previously unselected package ceph-mds.
Preparing to unpack .../ceph-mds_9.2.1-1trusty_amd64.deb ...
Unpacking ceph-mds (9.2.1-1trusty) ...
Processing triggers for ureadahead (0.100.0-16) ...
Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
Setting up ceph-common (9.2.1-1trusty) ...
Setting system user ceph properties..usermod: no changes
..done
Fixing /var/run/ceph ownership....done
Setting up ceph (9.2.1-1trusty) ...
ceph-all start/running
Setting up ceph-deploy (1.5.33) ...
Setting up ceph-fs-common (9.2.1-1trusty) ...
Setting up ceph-fuse (9.2.1-1trusty) ...
Setting up ceph-mds (9.2.1-1trusty) ...
start: Job is already running: ceph-mds-all
Processing triggers for libc-bin (2.19-0ubuntu6.7) ...
megdc@node1:~/ceph-cluster$ ceph status
    cluster 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
     health HEALTH_OK
     monmap e1: 1 mons at {node1=136.243.49.217:6789/0}
            election epoch 1, quorum 0 node1
     osdmap e145: 3 osds: 2 up, 2 in
            flags sortbitwise
      pgmap v421437: 466 pgs, 16 pools, 73483 MB data, 18597 objects
            153 GB used, 10377 GB / 11089 GB avail
                 466 active+clean
megdc@node1:~/ceph-cluster$ ceph osd tree 
ID WEIGHT   TYPE NAME               UP/DOWN REWEIGHT PRIMARY-AFFINITY 
-5        0 rack unknownrack                                          
-4        0     host 138.201.21.215                                   
-1 21.64975 root default                                              
-2 10.82977     host node1                                            
 0  5.41489         osd.0                up  1.00000          1.00000 
 1  5.41489         osd.1                up  1.00000          1.00000 
-3 10.81998     host node2                                            
 2  5.40999         osd.2              down        0          1.00000 
 3  5.40999         osd.3               DNE        0                  
megdc@node1:~/ceph-cluster$ ceph-deploy install -h
usage: ceph-deploy install [-h] [--stable [CODENAME] | --release [CODENAME] |
                           --testing | --dev [BRANCH_OR_TAG] | --dev-commit
                           [COMMIT]] [--mon] [--mds] [--rgw] [--osd] [--tests]
                           [--cli] [--all]
                           [--adjust-repos | --no-adjust-repos | --repo]
                           [--local-mirror [LOCAL_MIRROR]]
                           [--repo-url [REPO_URL]] [--gpg-url [GPG_URL]]
                           HOST [HOST ...]

Install Ceph packages on remote hosts.

positional arguments:
  HOST                  hosts to install on

optional arguments:
  -h, --help            show this help message and exit
  --stable [CODENAME]   [DEPRECATED] install a release known as CODENAME (done
                        by default) (default: None)
  --release [CODENAME]  install a release known as CODENAME (done by default)
                        (default: None)
  --testing             install the latest development release
  --dev [BRANCH_OR_TAG]
                        install a bleeding edge build from Git branch or tag
                        (default: master)
  --dev-commit [COMMIT]
                        install a bleeding edge build from Git commit
  --mon                 install the mon component only
  --mds                 install the mds component only
  --rgw                 install the rgw component only
  --osd                 install the osd component only
  --tests               install the testing components
  --cli, --common       install the common component only
  --all                 install all Ceph components (mon, osd, mds, rgw)
                        except tests. This is the default
  --adjust-repos        install packages modifying source repos
  --no-adjust-repos     install packages without modifying source repos
  --repo                install repo files only (skips package installation)
  --local-mirror [LOCAL_MIRROR]
                        Fetch packages and push them to hosts for a local repo
                        mirror
  --repo-url [REPO_URL]
                        specify a repo URL that mirrors/contains Ceph packages
  --gpg-url [GPG_URL]   specify a GPG key URL to be used with custom repos
                        (defaults to ceph.com)
megdc@node1:~/ceph-cluster$ ceph-deploy install --mds --cli --osd --common node2
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.33): /usr/bin/ceph-deploy install --mds --cli --osd --common node2
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  testing                       : None
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa3b66db050>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  dev_commit                    : None
[ceph_deploy.cli][INFO  ]  install_mds                   : True
[ceph_deploy.cli][INFO  ]  stable                        : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  adjust_repos                  : True
[ceph_deploy.cli][INFO  ]  func                          : <function install at 0x7fa3b6f95e60>
[ceph_deploy.cli][INFO  ]  install_all                   : False
[ceph_deploy.cli][INFO  ]  repo                          : False
[ceph_deploy.cli][INFO  ]  host                          : ['node2']
[ceph_deploy.cli][INFO  ]  install_rgw                   : False
[ceph_deploy.cli][INFO  ]  install_tests                 : False
[ceph_deploy.cli][INFO  ]  repo_url                      : None
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  install_osd                   : True
[ceph_deploy.cli][INFO  ]  version_kind                  : stable
[ceph_deploy.cli][INFO  ]  install_common                : True
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  dev                           : master
[ceph_deploy.cli][INFO  ]  local_mirror                  : None
[ceph_deploy.cli][INFO  ]  release                       : None
[ceph_deploy.cli][INFO  ]  install_mon                   : False
[ceph_deploy.cli][INFO  ]  gpg_url                       : None
[ceph_deploy.install][DEBUG ] Installing stable version jewel on cluster ceph hosts node2
[ceph_deploy.install][DEBUG ] Detecting platform for host node2 ...
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[node2][INFO  ] Running command: sudo /sbin/initctl version
[ceph_deploy.install][INFO  ] Distro info: Ubuntu 14.04 trusty
[node2][INFO  ] installing Ceph on node2
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates apt-transport-https
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] apt-transport-https is already the newest version.
[node2][DEBUG ] ca-certificates is already the newest version.
[node2][DEBUG ] 0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.
[node2][INFO  ] Running command: sudo wget -O release.asc https://download.ceph.com/keys/release.asc
[node2][WARNIN] --2016-05-19 15:34:45--  https://download.ceph.com/keys/release.asc
[node2][WARNIN] Resolving download.ceph.com (download.ceph.com)... 2607:f298:6050:51f3:f816:3eff:fe71:9135, 173.236.253.173
[node2][WARNIN] Connecting to download.ceph.com (download.ceph.com)|2607:f298:6050:51f3:f816:3eff:fe71:9135|:443... connected.
[node2][WARNIN] HTTP request sent, awaiting response... 200 OK
[node2][WARNIN] Length: 1645 (1.6K) [application/octet-stream]
[node2][WARNIN] Saving to: ‘release.asc’
[node2][WARNIN] 
[node2][WARNIN]      0K .                                                     100%  238M=0s
[node2][WARNIN] 
[node2][WARNIN] 2016-05-19 15:34:45 (238 MB/s) - ‘release.asc’ saved [1645/1645]
[node2][WARNIN] 
[node2][INFO  ] Running command: sudo apt-key add release.asc
[node2][DEBUG ] OK
[node2][DEBUG ] add deb repo to /etc/apt/sources.list.d/
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q update
[node2][DEBUG ] Ign http://mirror.hetzner.de trusty InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty Release.gpg
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty Release
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/universe amd64 Packages
[node2][DEBUG ] Ign http://de.archive.ubuntu.com trusty InRelease
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates InRelease
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/multiverse amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/restricted i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/universe i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-backports/multiverse i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty Release.gpg
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/main Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/restricted Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/universe Sources
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/universe amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/multiverse amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/restricted i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/multiverse Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/main amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/universe i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-updates/multiverse i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/main amd64 Packages
[node2][DEBUG ] Hit http://security.ubuntu.com trusty-security InRelease
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/universe amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/multiverse amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/restricted i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/universe i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty-security/multiverse i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/main amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/restricted amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/universe amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/multiverse amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/main i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/restricted i386 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/universe i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/universe amd64 Packages
[node2][DEBUG ] Hit http://mirror.hetzner.de trusty/multiverse i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/multiverse amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/main i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/restricted i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/universe i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-updates/multiverse i386 Packages
[node2][DEBUG ] Ign http://downloads.opennebula.org stable InRelease
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty Release
[node2][DEBUG ] Hit http://security.ubuntu.com trusty-security/main Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/main Sources
[node2][DEBUG ] Hit http://get.megam.io trusty InRelease
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/restricted Sources
[node2][DEBUG ] Hit http://security.ubuntu.com trusty-security/restricted Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/universe Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/multiverse Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/main amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/restricted amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/universe amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/multiverse amd64 Packages
[node2][DEBUG ] Hit http://security.ubuntu.com trusty-security/universe Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/main i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/restricted i386 Packages
[node2][DEBUG ] Hit http://get.megam.io trusty/testing amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/universe i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty-backports/multiverse i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/main Sources
[node2][DEBUG ] Hit http://downloads.opennebula.org stable Release.gpg
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/restricted Sources
[node2][DEBUG ] Hit http://security.ubuntu.com trusty-security/multiverse Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/universe Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/multiverse Sources
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/main amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/restricted amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/universe amd64 Packages
[node2][DEBUG ] Hit http://security.ubuntu.com trusty-security/main amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/multiverse amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/main i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/restricted i386 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/universe i386 Packages
[node2][DEBUG ] Hit http://security.ubuntu.com trusty-security/restricted amd64 Packages
[node2][DEBUG ] Hit http://de.archive.ubuntu.com trusty/multiverse i386 Packages
[node2][DEBUG ] Get:1 https://download.ceph.com trusty InRelease
[node2][DEBUG ] Hit http://downloads.opennebula.org stable Release
[node2][DEBUG ] Hit http://security.ubuntu.com trusty-security/universe amd64 Packages
[node2][DEBUG ] Get:2 https://download.ceph.com trusty/main amd64 Packages
[node2][DEBUG ] Hit http://security.ubuntu.com trusty-security/multiverse amd64 Packages
[node2][DEBUG ] Get:3 https://download.ceph.com trusty/main i386 Packages
[node2][DEBUG ] Hit http://security.ubuntu.com trusty-security/main i386 Packages
[node2][DEBUG ] Hit http://downloads.opennebula.org stable/opennebula amd64 Packages
[node2][DEBUG ] Hit http://security.ubuntu.com trusty-security/restricted i386 Packages
[node2][DEBUG ] Hit http://security.ubuntu.com trusty-security/universe i386 Packages
[node2][DEBUG ] Hit http://security.ubuntu.com trusty-security/multiverse i386 Packages
[node2][DEBUG ] Hit http://downloads.opennebula.org stable/opennebula i386 Packages
[node2][DEBUG ] Fetched 17.2 kB in 3s (5,478 B/s)
[node2][DEBUG ] Reading package lists...
[node2][INFO  ] Running command: sudo env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew ceph ceph-mds ceph-common
[node2][DEBUG ] Reading package lists...
[node2][DEBUG ] Building dependency tree...
[node2][DEBUG ] Reading state information...
[node2][DEBUG ] The following extra packages will be installed:
[node2][DEBUG ]   ceph-base ceph-mon ceph-osd cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
[node2][DEBUG ]   libboost-program-options1.54.0 libboost-random1.54.0 libboost-regex1.54.0
[node2][DEBUG ]   libcephfs1 libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery
[node2][DEBUG ]   libleveldb1 librados2 libradosstriper1 librbd1 librgw2 libsnappy1
[node2][DEBUG ]   libtcmalloc-minimal4 libunwind8 python-cephfs python-flask
[node2][DEBUG ]   python-itsdangerous python-rados python-rbd python-werkzeug
[node2][DEBUG ] Suggested packages:
[node2][DEBUG ]   javascript-common python-flask-doc ipython python-genshi python-lxml
[node2][DEBUG ]   python-greenlet python-redis python-pylibmc python-memcache
[node2][DEBUG ]   python-werkzeug-doc
[node2][DEBUG ] Recommended packages:
[node2][DEBUG ]   ceph-fs-common ceph-fuse python-blinker python-openssl python-pyinotify
[node2][DEBUG ] The following NEW packages will be installed:
[node2][DEBUG ]   ceph ceph-base ceph-common ceph-mds ceph-mon ceph-osd cryptsetup-bin
[node2][DEBUG ]   libbabeltrace-ctf1 libbabeltrace1 libboost-program-options1.54.0
[node2][DEBUG ]   libboost-random1.54.0 libboost-regex1.54.0 libcephfs1 libcryptsetup4
[node2][DEBUG ]   libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1 libradosstriper1
[node2][DEBUG ]   librgw2 libsnappy1 libtcmalloc-minimal4 libunwind8 python-cephfs
[node2][DEBUG ]   python-flask python-itsdangerous python-rados python-rbd python-werkzeug
[node2][DEBUG ] The following packages will be upgraded:
[node2][DEBUG ]   librados2 librbd1
[node2][DEBUG ] 2 upgraded, 29 newly installed, 0 to remove and 39 not upgraded.
[node2][DEBUG ] Need to get 105 MB/107 MB of archives.
[node2][DEBUG ] After this operation, 491 MB of additional disk space will be used.
[node2][DEBUG ] Get:1 http://mirror.hetzner.de/ubuntu/packages/ trusty-updates/main libboost-regex1.54.0 amd64 1.54.0-4ubuntu3.1 [261 kB]
[node2][DEBUG ] Get:2 https://download.ceph.com/debian-jewel/ trusty/main librbd1 amd64 10.2.1-1trusty [3,072 kB]
[node2][DEBUG ] Get:3 https://download.ceph.com/debian-jewel/ trusty/main librados2 amd64 10.2.1-1trusty [2,643 kB]
[node2][DEBUG ] Get:4 https://download.ceph.com/debian-jewel/ trusty/main libradosstriper1 amd64 10.2.1-1trusty [2,716 kB]
[node2][DEBUG ] Get:5 https://download.ceph.com/debian-jewel/ trusty/main librgw2 amd64 10.2.1-1trusty [3,650 kB]
[node2][DEBUG ] Get:6 https://download.ceph.com/debian-jewel/ trusty/main python-rados amd64 10.2.1-1trusty [1,559 kB]
[node2][DEBUG ] Get:7 https://download.ceph.com/debian-jewel/ trusty/main libcephfs1 amd64 10.2.1-1trusty [2,740 kB]
[node2][DEBUG ] Get:8 https://download.ceph.com/debian-jewel/ trusty/main python-cephfs amd64 10.2.1-1trusty [1,196 kB]
[node2][DEBUG ] Get:9 https://download.ceph.com/debian-jewel/ trusty/main python-rbd amd64 10.2.1-1trusty [1,204 kB]
[node2][DEBUG ] Get:10 https://download.ceph.com/debian-jewel/ trusty/main ceph-common amd64 10.2.1-1trusty [12.7 MB]
[node2][DEBUG ] Get:11 https://download.ceph.com/debian-jewel/ trusty/main ceph-base amd64 10.2.1-1trusty [54.6 MB]
[node2][DEBUG ] Get:12 https://download.ceph.com/debian-jewel/ trusty/main ceph-mon amd64 10.2.1-1trusty [3,576 kB]
[node2][DEBUG ] Get:13 https://download.ceph.com/debian-jewel/ trusty/main ceph-osd amd64 10.2.1-1trusty [11.0 MB]
[node2][DEBUG ] Get:14 https://download.ceph.com/debian-jewel/ trusty/main ceph amd64 10.2.1-1trusty [994 kB]
[node2][DEBUG ] Get:15 https://download.ceph.com/debian-jewel/ trusty/main ceph-mds amd64 10.2.1-1trusty [3,587 kB]
Extracting templates from packages: 100%
[node2][DEBUG ] Fetched 105 MB in 8s (12.8 MB/s)
[node2][DEBUG ] Selecting previously unselected package libboost-program-options1.54.0:amd64.
[node2][DEBUG ] (Reading database ... 36453 files and directories currently installed.)
[node2][DEBUG ] Preparing to unpack .../libboost-program-options1.54.0_1.54.0-4ubuntu3.1_amd64.deb ...
[node2][DEBUG ] Unpacking libboost-program-options1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Selecting previously unselected package libboost-random1.54.0:amd64.
[node2][DEBUG ] Preparing to unpack .../libboost-random1.54.0_1.54.0-4ubuntu3.1_amd64.deb ...
[node2][DEBUG ] Unpacking libboost-random1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Selecting previously unselected package libboost-regex1.54.0:amd64.
[node2][DEBUG ] Preparing to unpack .../libboost-regex1.54.0_1.54.0-4ubuntu3.1_amd64.deb ...
[node2][DEBUG ] Unpacking libboost-regex1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Selecting previously unselected package libsnappy1.
[node2][DEBUG ] Preparing to unpack .../libsnappy1_1.1.0-1ubuntu1_amd64.deb ...
[node2][DEBUG ] Unpacking libsnappy1 (1.1.0-1ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package libleveldb1:amd64.
[node2][DEBUG ] Preparing to unpack .../libleveldb1_1.15.0-2_amd64.deb ...
[node2][DEBUG ] Unpacking libleveldb1:amd64 (1.15.0-2) ...
[node2][DEBUG ] Selecting previously unselected package libunwind8.
[node2][DEBUG ] Preparing to unpack .../libunwind8_1.1-2.2ubuntu3_amd64.deb ...
[node2][DEBUG ] Unpacking libunwind8 (1.1-2.2ubuntu3) ...
[node2][DEBUG ] Selecting previously unselected package libbabeltrace1:amd64.
[node2][DEBUG ] Preparing to unpack .../libbabeltrace1_1.2.1-2_amd64.deb ...
[node2][DEBUG ] Unpacking libbabeltrace1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Selecting previously unselected package libbabeltrace-ctf1:amd64.
[node2][DEBUG ] Preparing to unpack .../libbabeltrace-ctf1_1.2.1-2_amd64.deb ...
[node2][DEBUG ] Unpacking libbabeltrace-ctf1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Preparing to unpack .../librbd1_10.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking librbd1 (10.2.1-1trusty) over (9.2.1-1trusty) ...
[node2][DEBUG ] Preparing to unpack .../librados2_10.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking librados2 (10.2.1-1trusty) over (9.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libtcmalloc-minimal4.
[node2][DEBUG ] Preparing to unpack .../libtcmalloc-minimal4_2.1-2ubuntu1.1_amd64.deb ...
[node2][DEBUG ] Unpacking libtcmalloc-minimal4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Selecting previously unselected package libgoogle-perftools4.
[node2][DEBUG ] Preparing to unpack .../libgoogle-perftools4_2.1-2ubuntu1.1_amd64.deb ...
[node2][DEBUG ] Unpacking libgoogle-perftools4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Selecting previously unselected package libradosstriper1.
[node2][DEBUG ] Preparing to unpack .../libradosstriper1_10.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking libradosstriper1 (10.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libfcgi0ldbl.
[node2][DEBUG ] Preparing to unpack .../libfcgi0ldbl_2.4.0-8.1ubuntu5_amd64.deb ...
[node2][DEBUG ] Unpacking libfcgi0ldbl (2.4.0-8.1ubuntu5) ...
[node2][DEBUG ] Selecting previously unselected package librgw2.
[node2][DEBUG ] Preparing to unpack .../librgw2_10.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking librgw2 (10.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package python-rados.
[node2][DEBUG ] Preparing to unpack .../python-rados_10.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking python-rados (10.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libcephfs1.
[node2][DEBUG ] Preparing to unpack .../libcephfs1_10.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking libcephfs1 (10.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package python-cephfs.
[node2][DEBUG ] Preparing to unpack .../python-cephfs_10.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking python-cephfs (10.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package python-rbd.
[node2][DEBUG ] Preparing to unpack .../python-rbd_10.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking python-rbd (10.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package ceph-common.
[node2][DEBUG ] Preparing to unpack .../ceph-common_10.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph-common (10.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libcryptsetup4.
[node2][DEBUG ] Preparing to unpack .../libcryptsetup4_2%3a1.6.1-1ubuntu1_amd64.deb ...
[node2][DEBUG ] Unpacking libcryptsetup4 (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package cryptsetup-bin.
[node2][DEBUG ] Preparing to unpack .../cryptsetup-bin_2%3a1.6.1-1ubuntu1_amd64.deb ...
[node2][DEBUG ] Unpacking cryptsetup-bin (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package ceph-base.
[node2][DEBUG ] Preparing to unpack .../ceph-base_10.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph-base (10.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package libjs-jquery.
[node2][DEBUG ] Preparing to unpack .../libjs-jquery_1.7.2+dfsg-2ubuntu1_all.deb ...
[node2][DEBUG ] Unpacking libjs-jquery (1.7.2+dfsg-2ubuntu1) ...
[node2][DEBUG ] Selecting previously unselected package python-werkzeug.
[node2][DEBUG ] Preparing to unpack .../python-werkzeug_0.9.4+dfsg-1.1ubuntu2_all.deb ...
[node2][DEBUG ] Unpacking python-werkzeug (0.9.4+dfsg-1.1ubuntu2) ...
[node2][DEBUG ] Selecting previously unselected package python-itsdangerous.
[node2][DEBUG ] Preparing to unpack .../python-itsdangerous_0.22+dfsg1-1build1_all.deb ...
[node2][DEBUG ] Unpacking python-itsdangerous (0.22+dfsg1-1build1) ...
[node2][DEBUG ] Selecting previously unselected package python-flask.
[node2][DEBUG ] Preparing to unpack .../python-flask_0.10.1-2build1_all.deb ...
[node2][DEBUG ] Unpacking python-flask (0.10.1-2build1) ...
[node2][DEBUG ] Selecting previously unselected package ceph-mon.
[node2][DEBUG ] Preparing to unpack .../ceph-mon_10.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph-mon (10.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package ceph-osd.
[node2][DEBUG ] Preparing to unpack .../ceph-osd_10.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph-osd (10.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package ceph.
[node2][DEBUG ] Preparing to unpack .../ceph_10.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph (10.2.1-1trusty) ...
[node2][DEBUG ] Selecting previously unselected package ceph-mds.
[node2][DEBUG ] Preparing to unpack .../ceph-mds_10.2.1-1trusty_amd64.deb ...
[node2][DEBUG ] Unpacking ceph-mds (10.2.1-1trusty) ...
[node2][DEBUG ] Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][DEBUG ] Setting up libboost-program-options1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Setting up libboost-random1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Setting up libboost-regex1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
[node2][DEBUG ] Setting up libsnappy1 (1.1.0-1ubuntu1) ...
[node2][DEBUG ] Setting up libleveldb1:amd64 (1.15.0-2) ...
[node2][DEBUG ] Setting up libunwind8 (1.1-2.2ubuntu3) ...
[node2][DEBUG ] Setting up libbabeltrace1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Setting up libbabeltrace-ctf1:amd64 (1.2.1-2) ...
[node2][DEBUG ] Setting up librados2 (10.2.1-1trusty) ...
[node2][DEBUG ] Setting up librbd1 (10.2.1-1trusty) ...
[node2][DEBUG ] Setting up libtcmalloc-minimal4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Setting up libgoogle-perftools4 (2.1-2ubuntu1.1) ...
[node2][DEBUG ] Setting up libradosstriper1 (10.2.1-1trusty) ...
[node2][DEBUG ] Setting up libfcgi0ldbl (2.4.0-8.1ubuntu5) ...
[node2][DEBUG ] Setting up librgw2 (10.2.1-1trusty) ...
[node2][DEBUG ] Setting up python-rados (10.2.1-1trusty) ...
[node2][DEBUG ] Setting up libcephfs1 (10.2.1-1trusty) ...
[node2][DEBUG ] Setting up python-cephfs (10.2.1-1trusty) ...
[node2][DEBUG ] Setting up python-rbd (10.2.1-1trusty) ...
[node2][DEBUG ] Setting up ceph-common (10.2.1-1trusty) ...
[node2][DEBUG ] Setting system user ceph properties....done
[node2][WARNIN] usermod: no changes
[node2][DEBUG ] Setting up libcryptsetup4 (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Setting up cryptsetup-bin (2:1.6.1-1ubuntu1) ...
[node2][DEBUG ] Setting up libjs-jquery (1.7.2+dfsg-2ubuntu1) ...
[node2][DEBUG ] Setting up python-werkzeug (0.9.4+dfsg-1.1ubuntu2) ...
[node2][DEBUG ] Setting up python-itsdangerous (0.22+dfsg1-1build1) ...
[node2][DEBUG ] Setting up python-flask (0.10.1-2build1) ...
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][DEBUG ] Setting up ceph-base (10.2.1-1trusty) ...
[node2][DEBUG ] ceph-all start/running
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][DEBUG ] Setting up ceph-mds (10.2.1-1trusty) ...
[node2][DEBUG ] ceph-mds-all start/running
[node2][DEBUG ] Setting up ceph-mon (10.2.1-1trusty) ...
[node2][DEBUG ] ceph-mon-all start/running
[node2][DEBUG ] Setting up ceph-osd (10.2.1-1trusty) ...
[node2][DEBUG ] ceph-osd-all start/running
[node2][DEBUG ] Processing triggers for ureadahead (0.100.0-16) ...
[node2][DEBUG ] Setting up ceph (10.2.1-1trusty) ...
[node2][DEBUG ] Processing triggers for libc-bin (2.19-0ubuntu6.7) ...
[node2][INFO  ] Running command: sudo ceph --version
[node2][DEBUG ] ceph version 10.2.1 (3a66dd4f30852819c1bdaa8ec23c795d4ad77269)
megdc@node1:~/ceph-cluster$ ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd --fs-type ext4
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.33): /usr/bin/ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd --fs-type ext4
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  bluestore                     : None
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : prepare
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc95aedbdd0>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  fs_type                       : ext4
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7fc95b33c578>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[node2][INFO  ] Running command: sudo /sbin/initctl version
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Deploying osd to node2
[node2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[node2][WARNIN] osd keyring does not exist yet, creating one
[node2][DEBUG ] create a keyring file
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage3/osd journal None activate False
[node2][DEBUG ] find the location of an executable
[node2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type ext4 -- /storage3/osd
[node2][WARNIN] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] populate_data_path: Preparing osd data dir /storage3/osd
[node2][WARNIN] command: Running command: /bin/chown -R ceph:ceph /storage3/osd/ceph_fsid.32123.tmp
[node2][WARNIN] command: Running command: /bin/chown -R ceph:ceph /storage3/osd/fsid.32123.tmp
[node2][WARNIN] command: Running command: /bin/chown -R ceph:ceph /storage3/osd/magic.32123.tmp
[node2][INFO  ] checking OSD status...
[node2][DEBUG ] find the location of an executable
[node2][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[node2][INFO  ] Running command: sudo /sbin/initctl version
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] Preparing host node2 disk /storage4/osd journal None activate False
[node2][DEBUG ] find the location of an executable
[node2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type ext4 -- /storage4/osd
[node2][WARNIN] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[node2][WARNIN] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[node2][WARNIN] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[node2][WARNIN] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[node2][WARNIN] populate_data_path: Preparing osd data dir /storage4/osd
[node2][WARNIN] command: Running command: /bin/chown -R ceph:ceph /storage4/osd/ceph_fsid.32205.tmp
[node2][WARNIN] command: Running command: /bin/chown -R ceph:ceph /storage4/osd/fsid.32205.tmp
[node2][WARNIN] command: Running command: /bin/chown -R ceph:ceph /storage4/osd/magic.32205.tmp
[node2][INFO  ] checking OSD status...
[node2][DEBUG ] find the location of an executable
[node2][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host node2 is now ready for osd use.
megdc@node1:~/ceph-cluster$ ceph-deploy osd activate node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/megdc/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.33): /usr/bin/ceph-deploy osd activate node2:/storage3/osd node2:/storage4/osd
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  subcommand                    : activate
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f54e883fdd0>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f54e8ca0578>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  disk                          : [('node2', '/storage3/osd', None), ('node2', '/storage4/osd', None)]
[ceph_deploy.osd][DEBUG ] Activating cluster ceph disks node2:/storage3/osd: node2:/storage4/osd:
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[node2][INFO  ] Running command: sudo /sbin/initctl version
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] activating host node2 disk /storage3/osd
[ceph_deploy.osd][DEBUG ] will use init type: upstart
[node2][DEBUG ] find the location of an executable
[node2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init upstart --mount /storage3/osd
[node2][WARNIN] main_activate: path = /storage3/osd
[node2][WARNIN] activate: Cluster uuid is 271ea3ed-a709-4dae-8612-e172945efc72
[node2][WARNIN] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] activate: Cluster name is ceph
[node2][WARNIN] activate: OSD uuid is cbaf7edf-8cdc-406b-bd03-9f0fdd8cf470
[node2][WARNIN] allocate_osd_id: Allocating OSD id...
[node2][WARNIN] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise cbaf7edf-8cdc-406b-bd03-9f0fdd8cf470
[node2][WARNIN] No data was received after 300 seconds, disconnecting...
[node2][INFO  ] checking OSD status...
[node2][DEBUG ] find the location of an executable
[node2][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[node2][DEBUG ] connection detected need for sudo
[node2][DEBUG ] connected to host: node2 
[node2][DEBUG ] detect platform information from remote host
[node2][DEBUG ] detect machine type
[node2][DEBUG ] find the location of an executable
[node2][INFO  ] Running command: sudo /sbin/initctl version
[node2][DEBUG ] find the location of an executable
[ceph_deploy.osd][INFO  ] Distro info: Ubuntu 14.04 trusty
[ceph_deploy.osd][DEBUG ] activating host node2 disk /storage4/osd
[ceph_deploy.osd][DEBUG ] will use init type: upstart
[node2][DEBUG ] find the location of an executable
[node2][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init upstart --mount /storage4/osd
[node2][WARNIN] main_activate: path = /storage4/osd
[node2][WARNIN] activate: Cluster uuid is 271ea3ed-a709-4dae-8612-e172945efc72
[node2][WARNIN] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[node2][WARNIN] activate: Cluster name is ceph
[node2][WARNIN] activate: OSD uuid is c618e994-9909-4f2a-9342-7a2fd098ad25
[node2][WARNIN] allocate_osd_id: Allocating OSD id...
[node2][WARNIN] command: Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise c618e994-9909-4f2a-9342-7a2fd098ad25
[node2][WARNIN] No data was received after 300 seconds, disconnecting...
[node2][INFO  ] checking OSD status...
[node2][DEBUG ] find the location of an executable
[node2][INFO  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
megdc@node1:~/ceph-cluster$ ceph status
    cluster 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
     health HEALTH_OK
     monmap e1: 1 mons at {node1=136.243.49.217:6789/0}
            election epoch 1, quorum 0 node1
     osdmap e145: 3 osds: 2 up, 2 in
            flags sortbitwise
      pgmap v421437: 466 pgs, 16 pools, 73483 MB data, 18597 objects
            153 GB used, 10377 GB / 11089 GB avail
                 466 active+clean
megdc@node1:~/ceph-cluster$ ls
ceph.bootstrap-mds.keyring  ceph.client.admin.keyring    ceph.conf             ceph.mon.keyring    release.asc  tmpU4w_cD
ceph.bootstrap-osd.keyring  ceph.client.libvirt.keyring  ceph-deploy-ceph.log  client.libvirt.key  s3.py        tmpuAA45G
ceph.bootstrap-rgw.keyring  ceph.client.radosgw.keyring  ceph.log              rbdmap              secret.xml   uid
megdc@node1:~/ceph-cluster$ scp /etc/ceph/*libvirt.keyring megdc@node2:/home/megdc/
ceph.client.libvirt.keyring                                                                                  100%  165     0.2KB/s   00:00    
megdc@node1:~/ceph-cluster$ scp /home/megdc/ceph-cluster/ceph.bootstrap-osd.keyring megdc@node2:/home/megdc/ceph.keyring
ceph.bootstrap-osd.keyring                                                                                   100%   71     0.1KB/s   00:00    
megdc@node1:~/ceph-cluster$ cat ceph.bootstrap-osd.keyring 
[client.bootstrap-osd]
	key = AQCoayNXrPpFIhAAfcNRmR6pjmem2GspWwlwtQ==
megdc@node1:~/ceph-cluster$ sudo cat /etc/ceph/ceph.bootstrap-osd.keyring 
[client.bootstrap-osd]
	key = AQCoayNXrPpFIhAAfcNRmR6pjmem2GspWwlwtQ==
megdc@node1:~/ceph-cluster$ scp ./*.keyring megdc@node2:/home/megdc/
ceph.bootstrap-mds.keyring                                                                                   100%   71     0.1KB/s   00:00    
ceph.bootstrap-osd.keyring                                                                                   100%   71     0.1KB/s   00:00    
ceph.bootstrap-rgw.keyring                                                                                   100%   71     0.1KB/s   00:00    
ceph.client.admin.keyring                                                                                    100%   63     0.1KB/s   00:00    
ceph.client.libvirt.keyring                                                                                  100%  165     0.2KB/s   00:00    
ceph.client.radosgw.keyring                                                                                  100%  119     0.1KB/s   00:00    
ceph.mon.keyring                                                                                             100%   73     0.1KB/s   00:00    
megdc@node1:~/ceph-cluster$ ssh node2 'sudo mv *.keyring /etc/ceph/'
megdc@node1:~/ceph-cluster$ ceph status
    cluster 6773ed61-b6c1-48a4-9be2-c6840e0de9e7
     health HEALTH_WARN
            466 pgs stale
            466 pgs stuck stale
            2/2 in osds are down
     monmap e1: 1 mons at {node1=136.243.49.217:6789/0}
            election epoch 1, quorum 0 node1
     osdmap e146: 3 osds: 0 up, 2 in
            flags sortbitwise
      pgmap v421438: 466 pgs, 16 pools, 73483 MB data, 18597 objects
            153 GB used, 10377 GB / 11089 GB avail
                 466 stale+active+clean
megdc@node1:~/ceph-cluster$ ceph osd tree
ID WEIGHT   TYPE NAME               UP/DOWN REWEIGHT PRIMARY-AFFINITY 
-5        0 rack unknownrack                                          
-4        0     host 138.201.21.215                                   
-1 21.64975 root default                                              
-2 10.82977     host node1                                            
 0  5.41489         osd.0              down  1.00000          1.00000 
 1  5.41489         osd.1              down  1.00000          1.00000 
-3 10.81998     host node2                                            
 2  5.40999         osd.2              down        0          1.00000 
 3  5.40999         osd.3               DNE        0                  
megdc@node1:~/ceph-cluster$ 


ceph-common : Depends: librbd1 (= 9.2.1-1trusty) but 10.2.1-1trusty is to be installed

Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following packages were automatically installed and are no longer required:
  augeas-lenses cgroup-lite ipset ipxe-qemu libaio1 libasound2 libasound2-data
  libasyncns0 libaugeas0 libbluetooth3 libboost-random1.54.0
  libboost-system1.54.0 libboost-thread1.54.0 libbrlapi0.6 libcaca0 libfdt1
  libflac8 libipset3 libjpeg-turbo8 libjpeg8 liblttng-ust-ctl2 liblttng-ust0
  libnetcf1 libnl-route-3-200 libnspr4 libnss3 libnss3-nssdb libogg0
  libpciaccess0 libpixman-1-0 libpulse0 librados2 libruby1.9.1 libsdl1.2debian
  libseccomp2 libsndfile1 libspice-server1 liburcu1 libusbredirparser1
  libvorbis0a libvorbisenc2 libx11-6 libx11-data libxen-4.4 libxenstore3.0
  libxext6 libxslt1.1 libyaml-0-2 netcat-openbsd opennebula-common
  qemu-keymaps qemu-system-common ruby ruby1.9.1 seabios vlan
Use 'apt-get autoremove' to remove them.
The following packages will be REMOVED:
  librbd1 libvirt-bin opennebula-node qemu-kvm qemu-system-x86 qemu-utils
0 upgraded, 0 newly installed, 6 to remove and 0 not upgraded.
After this operation, 35.7 MB disk space will be freed.






