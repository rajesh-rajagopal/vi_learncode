Script started on Mon 23 May 2016 12:42:13 PM EAT
]0;megdc@node1: ~megdc@node1:~$ cdc[K ceph-cluster/
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ 
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ 
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ sudo apt-get update
0% [Working]            Ign http://mirror.hetzner.de trusty InRelease
            4% [Connecting to de.archive.ubuntu.com (141.76.1.200)] [Connecting to security                                                                               Hit http://mirror.hetzner.de trusty-backports InRelease
8% [Connecting to de.archive.ubuntu.com (141.76.1.200)] [Connecting to security                                                                               Hit http://mirror.hetzner.de trusty-updates InRelease
                                                                               Hit http://mirror.hetzner.de trusty-security InRelease
8% [Connecting to de.archive.ubuntu.com (141.76.1.200)] [Connecting to security8% [InRelease gpgv 65.9 kB] [Waiting for headers] [Connecting to de.archive.ubu                                                                               Hit http://mirror.hetzner.de trusty Release.gpg
14% [InRelease gpgv 65.9 kB] [Connecting to de.archive.ubuntu.com (141.76.1.200                                                                               Hit http://mirror.hetzner.de trusty Release
14% [InRelease gpgv 65.9 kB] [Connecting to de.archive.ubuntu.com (141.76.1.20014% [Waiting for headers] [Waiting for headers] [Connecting to get.megam.io] [C14% [InRelease gpgv 65.9 kB] [Waiting for headers] [Waiting for headers] [Conne                                                                               Hit http://mirror.hetzner.de trusty-backports/main amd64 Packages
14% [InRelease gpgv 65.9 kB] [Waiting for headers] [Waiting for headers] [Conne                                                                               Hit http://mirror.hetzner.de trusty-backports/restricted amd64 Packages
                                                                               Hit http://mirror.hetzner.de trusty-backports/universe amd64 Packages
                                                                               Ign http://de.archive.ubuntu.com trusty InRelease
17% [InRelease gpgv 65.9 kB] [Waiting for headers] [Waiting for headers] [Waiti16% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connecti                                                                               Hit http://security.ubuntu.com trusty-security InRelease
100% [Waiting for headers] [Waiting for headers] [Connecting to get.megam.io (1100% [InRelease gpgv 65.9 kB] [Waiting for headers] [Waiting for headers] [Wait100% [Packages 0 B] [InRelease gpgv 65.9 kB] [Waiting for headers] [Waiting for100% [InRelease gpgv 65.9 kB] [Waiting for headers] [Waiting for headers] [Wait100% [Packages 220 kB] [InRelease gpgv 65.9 kB] [Waiting for headers] [Waiting 100% [InRelease gpgv 65.9 kB] [Waiting for headers] [Waiting for headers] [Wait                                                                               Get:1 http://de.archive.ubuntu.com trusty-updates InRelease [65.9 kB]
85% [InRelease gpgv 65.9 kB] [Waiting for headers] [1 InRelease 14.3 kB/65.9 kB85% [Waiting for headers] [1 InRelease 14.3 kB/65.9 kB 22%] [Waiting for header85% [Release gpgv 58.5 kB] [Waiting for headers] [1 InRelease 14.3 kB/65.9 kB 285% [Waiting for headers] [1 InRelease 14.3 kB/65.9 kB 22%] [Waiting for header85% [InRelease gpgv 65.9 kB] [Waiting for headers] [1 InRelease 14.3 kB/65.9 kB                                                                               Hit http://mirror.hetzner.de trusty-backports/multiverse amd64 Packages
93% [InRelease gpgv 65.9 kB] [1 InRelease 43.2 kB/65.9 kB 66%] [Waiting for hea93% [Packages 3,396 B] [InRelease gpgv 65.9 kB] [Waiting for headers] [1 InRele                                                                               Hit http://mirror.hetzner.de trusty-backports/main i386 Packages
93% [InRelease gpgv 65.9 kB] [Waiting for headers] [1 InRelease 43.2 kB/65.9 kB93% [Packages 57.5 kB] [InRelease gpgv 65.9 kB] [Waiting for headers] [1 InRele                                                                               Hit http://mirror.hetzner.de trusty-backports/restricted i386 Packages
93% [Packages 57.5 kB] [InRelease gpgv 65.9 kB] [1 InRelease 43.2 kB/65.9 kB 66                                                                               Hit http://mirror.hetzner.de trusty-backports/universe i386 Packages
93% [Packages 57.5 kB] [InRelease gpgv 65.9 kB] [1 InRelease 43.2 kB/65.9 kB 66                                                                               Hit http://mirror.hetzner.de trusty-backports/multiverse i386 Packages
93% [Packages 57.5 kB] [InRelease gpgv 65.9 kB] [1 InRelease 43.2 kB/65.9 kB 6694% [InRelease gpgv 65.9 kB] [Waiting for headers] [1 InRelease 43.2 kB/65.9 kB94% [Packages 0 B] [InRelease gpgv 65.9 kB] [Waiting for headers] [1 InRelease 94% [InRelease gpgv 65.9 kB] [Waiting for headers] [1 InRelease 43.2 kB/65.9 kB94% [Packages 219 kB] [InRelease gpgv 65.9 kB] [Waiting for headers] [1 InRelea94% [Packages 219 kB] [Waiting for headers] [1 InRelease 43.2 kB/65.9 kB 66%] [96% [Waiting for headers] [1 InRelease 43.2 kB/65.9 kB 66%] [Waiting for header96% [Packages 3,341 B] [Waiting for headers] [1 InRelease 43.2 kB/65.9 kB 66%] 96% [Waiting for headers] [1 InRelease 43.2 kB/65.9 kB 66%] [Waiting for header100% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connect100% [1 InRelease gpgv 65.9 kB] [Waiting for headers] [Waiting for headers] [Wa100% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting                                                                               Hit http://security.ubuntu.com trusty-security/main Sources
100% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connect100% [Sources 588 kB] [Waiting for headers] [Waiting for headers] [Waiting for                                                                                Hit http://de.archive.ubuntu.com trusty-backports InRelease
100% [Sources 588 kB] [Waiting for headers] [Waiting for headers] [Connecting t100% [Sources 588 kB] [InRelease gpgv 65.9 kB] [Waiting for headers] [Waiting f100% [Sources 588 kB] [Waiting for headers] [Waiting for headers] [Waiting for                                                                                Hit http://de.archive.ubuntu.com trusty Release.gpg
100% [Sources 588 kB] [Waiting for headers] [Waiting for headers] [Waiting for                                                                                Hit http://mirror.hetzner.de trusty-updates/main amd64 Packages
100% [Sources 588 kB] [Waiting for headers] [Waiting for headers] [Waiting for                                                                                Hit http://mirror.hetzner.de trusty-updates/restricted amd64 Packages
100% [Sources 588 kB] [Waiting for headers] [Waiting for headers] [Waiting for 100% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting                                                                               Hit http://mirror.hetzner.de trusty-updates/universe amd64 Packages
100% [Packages 5,186 kB] [Waiting for headers] [Waiting for headers] [Waiting f                                                                               Hit http://mirror.hetzner.de trusty-updates/multiverse amd64 Packages
100% [Packages 5,186 kB] [Waiting for headers] [Waiting for headers] [Waiting f                                                                               Hit http://mirror.hetzner.de trusty-updates/main i386 Packages
100% [Packages 5,186 kB] [Waiting for headers] [Waiting for headers] [Waiting f                                                                               Hit http://mirror.hetzner.de trusty-updates/restricted i386 Packages
100% [Packages 5,186 kB] [Waiting for headers] [Waiting for headers] [Waiting f                                                                               Hit http://security.ubuntu.com trusty-security/restricted Sources
100% [Packages 5,186 kB] [Waiting for headers] [Waiting for headers] [Waiting f                                                                               Get:2 http://de.archive.ubuntu.com trusty-updates/main Sources [275 kB]
82% [Packages 5,186 kB] [Waiting for headers] [2 Sources 12.8 kB/275 kB 5%] [Wa                                                                               Hit http://security.ubuntu.com trusty-security/universe Sources
86% [Packages 5,186 kB] [Waiting for headers] [2 Sources 66.3 kB/275 kB 24%] [W                                                                               Hit http://mirror.hetzner.de trusty-updates/universe i386 Packages
95% [Packages 5,186 kB] [2 Sources 200 kB/275 kB 73%] [Waiting for headers] [Wa                                                                               Hit http://mirror.hetzner.de trusty-updates/multiverse i386 Packages
95% [Packages 5,186 kB] [2 Sources 200 kB/275 kB 73%] [Waiting for headers] [Wa                                                                               Hit http://mirror.hetzner.de trusty-security/main amd64 Packages
95% [Packages 5,186 kB] [2 Sources 200 kB/275 kB 73%] [Waiting for headers] [Wa                                                                               Hit http://mirror.hetzner.de trusty-security/restricted amd64 Packages
95% [Packages 5,186 kB] [2 Sources 200 kB/275 kB 73%] [Waiting for headers] [Wa                                                                               Hit http://mirror.hetzner.de trusty-security/universe amd64 Packages
95% [Packages 5,186 kB] [2 Sources 200 kB/275 kB 73%] [Waiting for headers] [Wa                                                                               Hit http://mirror.hetzner.de trusty-security/multiverse amd64 Packages
95% [Packages 5,186 kB] [2 Sources 200 kB/275 kB 73%] [Waiting for headers] [Wa                                                                               Hit http://mirror.hetzner.de trusty-security/main i386 Packages
95% [Packages 5,186 kB] [2 Sources 200 kB/275 kB 73%] [Waiting for headers] [Wa                                                                               Hit http://mirror.hetzner.de trusty-security/restricted i386 Packages
95% [Packages 5,186 kB] [2 Sources 200 kB/275 kB 73%] [Waiting for headers] [Wa                                                                               Hit http://mirror.hetzner.de trusty-security/universe i386 Packages
95% [Packages 5,186 kB] [2 Sources 200 kB/275 kB 73%] [Waiting for headers] [Wa                                                                               Hit http://mirror.hetzner.de trusty-security/multiverse i386 Packages
95% [Packages 5,186 kB] [Waiting for headers] [2 Sources 200 kB/275 kB 73%] [Wa                                                                               Hit http://security.ubuntu.com trusty-security/multiverse Sources
95% [Packages 5,186 kB] [Waiting for headers] [2 Sources 200 kB/275 kB 73%] [Wa                                                                               Hit http://mirror.hetzner.de trusty/main amd64 Packages
95% [Packages 5,186 kB] [2 Sources 200 kB/275 kB 73%] [Waiting for headers] [Wa                                                                               Hit http://mirror.hetzner.de trusty/restricted amd64 Packages
95% [Packages 5,186 kB] [2 Sources 200 kB/275 kB 73%] [Waiting for headers] [Wa                                                                               Hit http://mirror.hetzner.de trusty/universe amd64 Packages
95% [Packages 5,186 kB] [2 Sources 200 kB/275 kB 73%] [Waiting for headers] [Wa                                                                               Hit http://mirror.hetzner.de trusty/multiverse amd64 Packages
95% [Packages 5,186 kB] [2 Sources 200 kB/275 kB 73%] [Waiting for headers] [Wa                                                                               Hit http://mirror.hetzner.de trusty/main i386 Packages
95% [Packages 5,186 kB] [2 Sources 200 kB/275 kB 73%] [Waiting for headers] [Wa                                                                               Hit http://mirror.hetzner.de trusty/restricted i386 Packages
95% [Packages 5,186 kB] [2 Sources 200 kB/275 kB 73%] [Waiting for headers] [Wa                                                                               Hit http://mirror.hetzner.de trusty/universe i386 Packages
95% [Packages 5,186 kB] [2 Sources 200 kB/275 kB 73%] [Waiting for headers] [Wa                                                                               Hit http://mirror.hetzner.de trusty/multiverse i386 Packages
95% [Packages 5,186 kB] [2 Sources 200 kB/275 kB 73%] [Waiting for headers] [Wa100% [Packages 5,186 kB] [Waiting for headers] [Waiting for headers] [Connectin100% [2 Sources bzip2 0 B] [Packages 5,186 kB] [Waiting for headers] [Waiting f                                                                               Hit http://security.ubuntu.com trusty-security/main amd64 Packages
100% [2 Sources bzip2 0 B] [Packages 5,186 kB] [Waiting for headers] [Waiting f                                                                               Get:3 http://de.archive.ubuntu.com trusty-updates/restricted Sources [5,352 B]
100% [2 Sources bzip2 0 B] [Packages 5,186 kB] [3 Sources 5,352 B/5,352 B 100%]100% [2 Sources bzip2 0 B] [Packages 5,186 kB] [Waiting for headers] [Waiting f                                                                               Hit http://security.ubuntu.com trusty-security/restricted amd64 Packages
100% [2 Sources bzip2 0 B] [Packages 5,186 kB] [Waiting for headers] [Waiting f                                                                               Get:4 http://de.archive.ubuntu.com trusty-updates/universe Sources [154 kB]
92% [2 Sources bzip2 0 B] [Packages 5,186 kB] [4 Sources 22.9 kB/154 kB 15%] [W100% [2 Sources bzip2 0 B] [Packages 5,186 kB] [Waiting for headers] [Waiting f                                                                               Hit http://security.ubuntu.com trusty-security/universe amd64 Packages
100% [2 Sources bzip2 0 B] [Packages 5,186 kB] [Waiting for headers] [Waiting f                                                                               Get:5 http://de.archive.ubuntu.com trusty-updates/multiverse Sources [5,939 B]
100% [2 Sources bzip2 0 B] [Packages 5,186 kB] [5 Sources 4,082 B/5,939 B 69%] 100% [2 Sources bzip2 0 B] [Packages 5,186 kB] [Waiting for headers] [Waiting f100% [2 Sources bzip2 0 B] [Waiting for headers] [Waiting for headers] [Waiting100% [2 Sources bzip2 0 B] [Packages 223 kB] [Waiting for headers] [Waiting for100% [2 Sources bzip2 0 B] [Waiting for headers] [Waiting for headers] [Waiting100% [2 Sources bzip2 0 B] [Packages 2,143 kB] [Waiting for headers] [Waiting f                                                                               Hit http://security.ubuntu.com trusty-security/multiverse amd64 Packages
100% [2 Sources bzip2 0 B] [Packages 2,143 kB] [Waiting for headers] [Waiting f                                                                               Get:6 http://de.archive.ubuntu.com trusty-updates/main amd64 Packages [768 kB]
90% [2 Sources bzip2 0 B] [Packages 2,143 kB] [6 Packages 15.5 kB/768 kB 2%] [W                                                                               Hit http://security.ubuntu.com trusty-security/main i386 Packages
93% [2 Sources bzip2 0 B] [Packages 2,143 kB] [6 Packages 212 kB/768 kB 28%] [W96% [2 Sources bzip2 0 B] [6 Packages 330 kB/768 kB 43%] [Waiting for headers] 96% [2 Sources bzip2 0 B] [Packages 66.3 kB] [6 Packages 330 kB/768 kB 43%] [Wa96% [2 Sources bzip2 0 B] [6 Packages 330 kB/768 kB 43%] [Waiting for headers] 96% [2 Sources bzip2 0 B] [Packages 4,947 kB] [6 Packages 330 kB/768 kB 43%] [W                                                                               Hit http://security.ubuntu.com trusty-security/restricted i386 Packages
99% [2 Sources bzip2 0 B] [Packages 4,947 kB] [6 Packages 666 kB/768 kB 87%] [W100% [Packages 4,947 kB] [6 Packages 731 kB/768 kB 95%] [Waiting for headers] [100% [3 Sources bzip2 0 B] [Packages 4,947 kB] [6 Packages 731 kB/768 kB 95%] [100% [Packages 4,947 kB] [6 Packages 758 kB/768 kB 99%] [Waiting for headers] [100% [4 Sources bzip2 0 B] [Packages 4,947 kB] [6 Packages 758 kB/768 kB 99%] [100% [4 Sources bzip2 0 B] [Packages 4,947 kB] [Waiting for headers] [Waiting f                                                                               Hit http://security.ubuntu.com trusty-security/universe i386 Packages
100% [4 Sources bzip2 0 B] [Packages 4,947 kB] [Waiting for headers] [Waiting f                                                                               Get:7 http://de.archive.ubuntu.com trusty-updates/restricted amd64 Packages [15.9 kB]
100% [4 Sources bzip2 0 B] [Packages 4,947 kB] [7 Packages 15.9 kB/15.9 kB 100%100% [4 Sources bzip2 0 B] [Packages 4,947 kB] [Waiting for headers] [Waiting f                                                                               Hit http://security.ubuntu.com trusty-security/multiverse i386 Packages
100% [4 Sources bzip2 0 B] [Packages 4,947 kB] [Waiting for headers] [Waiting f                                                                               Get:8 http://de.archive.ubuntu.com trusty-updates/universe amd64 Packages [360 kB]
97% [4 Sources bzip2 0 B] [Packages 4,947 kB] [8 Packages 17.5 kB/360 kB 5%] [W100% [4 Sources bzip2 0 B] [Packages 4,947 kB] [Waiting for headers] [Waiting f100% [4 Sources bzip2 0 B] [Waiting for headers] [Waiting for headers] [Waiting100% [4 Sources bzip2 0 B] [Packages 221 kB] [Waiting for headers] [Waiting for100% [4 Sources bzip2 0 B] [Waiting for headers] [Waiting for headers] [Waiting100% [4 Sources bzip2 0 B] [Sources 23.0 kB] [Waiting for headers] [Waiting for100% [4 Sources bzip2 0 B] [Waiting for headers] [Waiting for headers] [Waiting100% [4 Sources bzip2 0 B] [Sources 148 kB] [Waiting for headers] [Waiting for 100% [4 Sources bzip2 0 B] [Waiting for headers] [Waiting for headers] [Waiting100% [4 Sources bzip2 0 B] [Packages 2,148 kB] [Waiting for headers] [Waiting f                                                                               Get:9 http://de.archive.ubuntu.com trusty-updates/multiverse amd64 Packages [13.2 kB]
100% [4 Sources bzip2 0 B] [Packages 2,148 kB] [9 Packages 13.2 kB/13.2 kB 100%100% [Packages 2,148 kB] [9 Packages 13.2 kB/13.2 kB 100%] [Waiting for headers100% [Packages 2,148 kB] [Waiting for headers] [Waiting for headers] [Waiting f100% [5 Sources bzip2 0 B] [Packages 2,148 kB] [Waiting for headers] [Waiting f100% [Packages 2,148 kB] [Waiting for headers] [Waiting for headers] [Waiting f100% [6 Packages bzip2 0 B] [Packages 2,148 kB] [Waiting for headers] [Waiting                                                                                Hit http://get.megam.io trusty InRelease
100% [6 Packages bzip2 0 B] [Packages 2,148 kB] [Waiting for headers] [Waiting 100% [6 Packages bzip2 0 B] [Packages 2,148 kB] [InRelease gpgv 5,059 B] [Waiti100% [6 Packages bzip2 0 B] [InRelease gpgv 5,059 B] [Waiting for headers] [Wai100% [6 Packages bzip2 0 B] [Packages 68.2 kB] [InRelease gpgv 5,059 B] [Waitin100% [6 Packages bzip2 0 B] [InRelease gpgv 5,059 B] [Waiting for headers] [Wai                                                                               Get:10 http://de.archive.ubuntu.com trusty-updates/main i386 Packages [737 kB]
96% [6 Packages bzip2 0 B] [InRelease gpgv 5,059 B] [10 Packages 22.9 kB/737 kB96% [6 Packages bzip2 0 B] [Packages 3,304 kB] [InRelease gpgv 5,059 B] [10 Pac97% [6 Packages bzip2 0 B] [Packages 3,304 kB] [10 Packages 88.4 kB/737 kB 12%]100% [6 Packages bzip2 0 B] [Packages 3,304 kB] [Waiting for headers] [Waiting                                                                                Get:11 http://de.archive.ubuntu.com trusty-updates/restricted i386 Packages [15.6 kB]
100% [6 Packages bzip2 0 B] [Packages 3,304 kB] [11 Packages 15.6 kB/15.6 kB 10100% [6 Packages bzip2 0 B] [Packages 3,304 kB] [Waiting for headers] [Waiting                                                                                Ign http://downloads.opennebula.org stable InRelease
100% [6 Packages bzip2 0 B] [Packages 3,304 kB] [Waiting for headers] [Waiting                                                                                Get:12 http://de.archive.ubuntu.com trusty-updates/universe i386 Packages [360 kB]
98% [6 Packages bzip2 0 B] [Packages 3,304 kB] [12 Packages 5,344 B/360 kB 1%] 100% [6 Packages bzip2 0 B] [Packages 3,304 kB] [Waiting for headers] [Connecti100% [6 Packages bzip2 0 B] [Waiting for headers] [Waiting for headers] [Connec100% [6 Packages bzip2 0 B] [Packages 188 kB] [Waiting for headers] [Waiting fo                                                                               Get:13 http://de.archive.ubuntu.com trusty-updates/multiverse i386 Packages [13.6 kB]
100% [6 Packages bzip2 0 B] [Packages 188 kB] [13 Packages 13.6 kB/13.6 kB 100%100% [6 Packages bzip2 0 B] [Packages 188 kB] [Waiting for headers] [Connecting100% [6 Packages bzip2 0 B] [Waiting for headers] [Waiting for headers] [Connec100% [6 Packages bzip2 0 B] [Packages 766 kB] [Waiting for headers] [Waiting fo100% [6 Packages bzip2 0 B] [Waiting for headers] [Waiting for headers] [Connec100% [6 Packages bzip2 0 B] [Packages 20.0 kB] [Waiting for headers] [Waiting f100% [6 Packages bzip2 0 B] [Waiting for headers] [Waiting for headers] [Connec100% [6 Packages bzip2 0 B] [Packages 3,074 kB] [Waiting for headers] [Waiting                                                                                Hit http://de.archive.ubuntu.com trusty-backports/main Sources
100% [6 Packages bzip2 0 B] [Packages 3,074 kB] [Waiting for headers] [Connecti                                                                               Hit http://de.archive.ubuntu.com trusty-backports/restricted Sources
100% [6 Packages bzip2 0 B] [Packages 3,074 kB] [Waiting for headers] [Connecti100% [6 Packages bzip2 0 B] [Waiting for headers] [Waiting for headers] [Connec100% [6 Packages bzip2 0 B] [Packages 186 kB] [Waiting for headers] [Waiting fo100% [6 Packages bzip2 0 B] [Waiting for headers] [Waiting for headers] [Connec100% [6 Packages bzip2 0 B] [Packages 765 kB] [Waiting for headers] [Waiting fo                                                                               Hit http://get.megam.io trusty/testing amd64 Packages
100% [6 Packages bzip2 0 B] [Packages 765 kB] [Waiting for headers] [Connecting100% [6 Packages bzip2 0 B] [Waiting for headers] [Connecting to downloads.open100% [6 Packages bzip2 0 B] [Packages 20.9 kB] [Waiting for headers] [Connectin100% [6 Packages bzip2 0 B] [Waiting for headers] [Connecting to downloads.open100% [6 Packages bzip2 0 B] [Sources 7,860 B] [Waiting for headers] [Connecting100% [6 Packages bzip2 0 B] [Waiting for headers] [Connecting to downloads.open100% [6 Packages bzip2 0 B] [Packages 8,235 kB] [Waiting for headers] [Connecti                                                                               Hit http://de.archive.ubuntu.com trusty-backports/universe Sources
                                                                               100% [6 Packages bzip2 0 B] [Packages 8,235 kB] [Waiting for headers]                                                                     Hit http://de.archive.ubuntu.com trusty-backports/multiverse Sources
100% [6 Packages bzip2 0 B] [Packages 8,235 kB] [Waiting for headers]                                                                     100% [Packages 8,235 kB] [Waiting for headers] [Waiting for headers]                                                                    100% [7 Packages bzip2 0 B] [Packages 8,235 kB] [Waiting for headers] [Waiting                                                                                100% [Packages 8,235 kB] [Waiting for headers] [Waiting for headers]                                                                    100% [8 Packages bzip2 0 B] [Packages 8,235 kB] [Waiting for headers] [Waiting                                                                                Hit http://de.archive.ubuntu.com trusty-backports/main amd64 Packages
                                                                               100% [8 Packages bzip2 0 B] [Packages 8,235 kB] [Waiting for headers]                                                                     Hit http://de.archive.ubuntu.com trusty-backports/restricted amd64 Packages
100% [8 Packages bzip2 0 B] [Packages 8,235 kB] [Waiting for headers]                                                                     Hit http://de.archive.ubuntu.com trusty-backports/universe amd64 Packages
100% [8 Packages bzip2 0 B] [Packages 8,235 kB] [Waiting for headers]                                                                     Hit http://de.archive.ubuntu.com trusty-backports/multiverse amd64 Packages
100% [8 Packages bzip2 0 B] [Packages 8,235 kB] [Waiting for headers]                                                                     100% [8 Packages bzip2 0 B] [Waiting for headers] [Waiting for headers]                                                                       100% [8 Packages bzip2 0 B] [Packages 184 kB] [Waiting for headers] [Waiting fo                                                                               100% [8 Packages bzip2 0 B] [Waiting for headers] [Waiting for headers]                                                                       100% [8 Packages bzip2 0 B] [Packages 31.7 MB] [Waiting for headers] [Waiting f                                                                               Hit http://de.archive.ubuntu.com trusty-backports/main i386 Packages
                                                                               100% [8 Packages bzip2 0 B] [Packages 31.7 MB] [Waiting for headers]                                                                    100% [Packages 31.7 MB] [Waiting for headers] [Waiting for headers]                                                                   100% [9 Packages bzip2 0 B] [Packages 31.7 MB] [Waiting for headers] [Waiting f                                                                               100% [Packages 31.7 MB] [Waiting for headers] [Waiting for headers]                                                                   100% [10 Packages bzip2 0 B] [Packages 31.7 MB] [Waiting for headers] [Waiting                                                                                Hit http://de.archive.ubuntu.com trusty-backports/restricted i386 Packages
                                                                               100% [10 Packages bzip2 0 B] [Packages 31.7 MB] [Waiting for headers]                                                                     Hit http://downloads.opennebula.org stable Release.gpg
100% [10 Packages bzip2 0 B] [Packages 31.7 MB] [Waiting for headers]                                                                     Hit http://de.archive.ubuntu.com trusty-backports/universe i386 Packages
                                                                     100% [10 Packages bzip2 0 B] [Packages 31.7 MB] [Connecting to downloads.openne                                                                               Hit http://de.archive.ubuntu.com trusty-backports/multiverse i386 Packages
100% [10 Packages bzip2 0 B] [Packages 31.7 MB] [Connecting to downloads.openne                                                                               Hit http://de.archive.ubuntu.com trusty Release
100% [10 Packages bzip2 0 B] [Packages 31.7 MB] [Connecting to downloads.openne100% [10 Packages bzip2 0 B] [Packages 31.7 MB] [Release gpgv 58.5 kB] [Connect100% [10 Packages bzip2 0 B] [Packages 31.7 MB] [Connecting to downloads.openne                                                                               Hit http://de.archive.ubuntu.com trusty/main Sources
100% [10 Packages bzip2 0 B] [Packages 31.7 MB] [Connecting to downloads.openne                                                                               Hit http://de.archive.ubuntu.com trusty/restricted Sources
100% [10 Packages bzip2 0 B] [Packages 31.7 MB] [Connecting to downloads.openne                                                                               Hit http://de.archive.ubuntu.com trusty/universe Sources
100% [10 Packages bzip2 0 B] [Packages 31.7 MB] [Connecting to downloads.openne                                                                               Hit http://de.archive.ubuntu.com trusty/multiverse Sources
100% [10 Packages bzip2 0 B] [Packages 31.7 MB] [Connecting to downloads.openne                                                                               Hit http://de.archive.ubuntu.com trusty/main amd64 Packages
100% [10 Packages bzip2 0 B] [Packages 31.7 MB] [Connecting to downloads.openne                                                                               Hit http://de.archive.ubuntu.com trusty/restricted amd64 Packages
                                                                               100% [10 Packages bzip2 0 B] [Packages 31.7 MB] [Waiting for headers]                                                                     100% [Packages 31.7 MB] [Waiting for headers] [Waiting for headers]                                                                   100% [11 Packages bzip2 0 B] [Packages 31.7 MB] [Waiting for headers] [Waiting                                                                                100% [Packages 31.7 MB] [Waiting for headers] [Waiting for headers]                                                                   100% [12 Packages bzip2 0 B] [Packages 31.7 MB] [Waiting for headers] [Waiting                                                                                Hit http://de.archive.ubuntu.com trusty/universe amd64 Packages
                                                                               100% [12 Packages bzip2 0 B] [Packages 31.7 MB] [Waiting for headers]                                                                     Hit http://de.archive.ubuntu.com trusty/multiverse amd64 Packages
100% [12 Packages bzip2 0 B] [Packages 31.7 MB] [Waiting for headers]                                                                     100% [Packages 31.7 MB] [Waiting for headers] [Waiting for headers]                                                                   100% [13 Packages bzip2 0 B] [Packages 31.7 MB] [Waiting for headers] [Waiting                                                                                100% [Packages 31.7 MB] [Waiting for headers] [Waiting for headers]                                                                   Hit http://de.archive.ubuntu.com trusty/main i386 Packages
                                                                   100% [Packages 31.7 MB] [Waiting for headers]                                             Hit http://de.archive.ubuntu.com trusty/restricted i386 Packages
100% [Packages 31.7 MB] [Waiting for headers]                                             Hit http://downloads.opennebula.org stable Release
100% [Packages 31.7 MB] [Waiting for headers]                                             100% [Packages 31.7 MB] [Release gpgv 1,107 B] [Waiting for headers]                                                                    100% [Packages 31.7 MB] [Waiting for headers]                                             Hit http://de.archive.ubuntu.com trusty/universe i386 Packages
                                             100% [Packages 31.7 MB] [Connecting to downloads.opennebula.org (173.255.246.10                                                                               Hit http://de.archive.ubuntu.com trusty/multiverse i386 Packages
100% [Packages 31.7 MB] [Connecting to downloads.opennebula.org (173.255.246.10                                                                               100% [Connecting to downloads.opennebula.org (173.255.246.101)]                                                               100% [Packages 664 kB] [Connecting to downloads.opennebula.org (173.255.246.101                                                                               100% [Connecting to downloads.opennebula.org (173.255.246.101)]                                                               100% [Packages 8,205 kB] [Connecting to downloads.opennebula.org (173.255.246.1                                                                               Hit https://download.ceph.com trusty InRelease
100% [Packages 8,205 kB] [Connecting to downloads.opennebula.org (173.255.246.1100% [Packages 8,205 kB] [InRelease gpgv 7,810 B] [Connecting to downloads.open100% [Packages 8,205 kB] [Connecting to downloads.opennebula.org (173.255.246.1                                                                               100% [Waiting for headers]                          100% [Packages 185 kB] [Waiting for headers]                                            100% [Waiting for headers]                          100% [Packages 31.7 MB] [Waiting for headers]                                             Hit https://download.ceph.com trusty/main amd64 Packages
100% [Packages 31.7 MB] [Waiting for headers]                                             Hit http://downloads.opennebula.org stable/opennebula amd64 Packages
                                             100% [Packages 31.7 MB]                       Hit https://download.ceph.com trusty/main i386 Packages
                       100% [Packages 31.7 MB] [Connecting to downloads.opennebula.org (173.255.246.10                                                                               100% [Waiting for headers]                          100% [Packages 674 kB] [Waiting for headers]                                            100% [Waiting for headers]                          100% [Packages 3,304 kB] [Waiting for headers]                                              Hit http://downloads.opennebula.org stable/opennebula i386 Packages
                                              100% [Packages 3,304 kB]                        100% [Working]              100% [Packages 188 kB]                      100% [Working]              100% [Packages 766 kB]                      100% [Working]              100% [Packages 20.0 kB]                       100% [Working]              100% [Packages 3,074 kB]                        100% [Working]              100% [Packages 186 kB]                      100% [Working]              100% [Packages 765 kB]                      100% [Working]              100% [Packages 20.9 kB]                       100% [Working]              100% [Sources 33.6 kB]                      100% [Working]              100% [Sources 0 B]                  100% [Working]              100% [Packages 11.3 kB]                       100% [Working]              100% [Sources 141 kB]                     100% [Working]              100% [Sources 4,444 B]                      100% [Working]              100% [Packages 57.6 kB]                       100% [Working]              100% [Packages 0 B]                   100% [Working]              100% [Packages 220 kB]                      100% [Working]              100% [Packages 3,396 B]                       100% [Working]              100% [Packages 57.5 kB]                       100% [Working]              100% [Packages 0 B]                   100% [Working]              100% [Packages 219 kB]                      100% [Working]              100% [Packages 3,341 B]                       100% [Working]              100% [Sources 5,000 kB]                       100% [Working]              100% [Sources 22.9 kB]                      100% [Working]              100% [Sources 27.9 MB]                      100% [Working]              100% [Sources 711 kB]                     100% [Working]              100% [Packages 8,235 kB]                        100% [Working]              100% [Packages 184 kB]                      100% [Working]              100% [Packages 31.7 MB]                       100% [Working]              100% [Packages 664 kB]                      100% [Working]              100% [Packages 8,205 kB]                        100% [Working]              100% [Packages 185 kB]                      100% [Working]              100% [Packages 31.7 MB]                       100% [Working]              100% [Packages 674 kB]                      100% [Working]              100% [Packages 46.8 kB]                       100% [Working]              100% [Packages 11.4 kB]                       100% [Working]              100% [Packages 1,304 B]                       100% [Working]              100% [Packages 10.0 kB]                       100% [Working]              Fetched 2,790 kB in 3s (905 kB/s)
Reading package lists... 0%Reading package lists... 0%Reading package lists... 1%Reading package lists... 3%Reading package lists... 3%Reading package lists... 3%Reading package lists... 3%Reading package lists... 18%Reading package lists... 18%Reading package lists... 19%Reading package lists... 19%Reading package lists... 23%Reading package lists... 23%Reading package lists... 23%Reading package lists... 23%Reading package lists... 25%Reading package lists... 38%Reading package lists... 38%Reading package lists... 38%Reading package lists... 38%Reading package lists... 38%Reading package lists... 38%Reading package lists... 38%Reading package lists... 38%Reading package lists... 38%Reading package lists... 38%Reading package lists... 38%Reading package lists... 38%Reading package lists... 38%Reading package lists... 38%Reading package lists... 38%Reading package lists... 38%Reading package lists... 38%Reading package lists... 38%Reading package lists... 38%Reading package lists... 38%Reading package lists... 41%Reading package lists... 41%Reading package lists... 41%Reading package lists... 41%Reading package lists... 42%Reading package lists... 42%Reading package lists... 42%Reading package lists... 42%Reading package lists... 44%Reading package lists... 44%Reading package lists... 44%Reading package lists... 44%Reading package lists... 45%Reading package lists... 45%Reading package lists... 45%Reading package lists... 45%Reading package lists... 47%Reading package lists... 47%Reading package lists... 47%Reading package lists... 47%Reading package lists... 47%Reading package lists... 47%Reading package lists... 47%Reading package lists... 47%Reading package lists... 49%Reading package lists... 49%Reading package lists... 49%Reading package lists... 49%Reading package lists... 49%Reading package lists... 49%Reading package lists... 49%Reading package lists... 49%Reading package lists... 53%Reading package lists... 53%Reading package lists... 53%Reading package lists... 53%Reading package lists... 68%Reading package lists... 68%Reading package lists... 69%Reading package lists... 69%Reading package lists... 70%Reading package lists... 73%Reading package lists... 73%Reading package lists... 73%Reading package lists... 73%Reading package lists... 88%Reading package lists... 88%Reading package lists... 88%Reading package lists... 88%Reading package lists... 90%Reading package lists... 90%Reading package lists... 90%Reading package lists... 90%Reading package lists... 91%Reading package lists... 91%Reading package lists... 92%Reading package lists... 92%Reading package lists... 94%Reading package lists... 94%Reading package lists... 94%Reading package lists... 94%Reading package lists... 95%Reading package lists... 95%Reading package lists... 95%Reading package lists... 95%Reading package lists... 95%Reading package lists... 95%Reading package lists... 95%Reading package lists... 95%Reading package lists... 95%Reading package lists... 95%Reading package lists... 95%Reading package lists... 95%Reading package lists... 95%Reading package lists... 95%Reading package lists... 95%Reading package lists... 95%Reading package lists... 95%Reading package lists... 95%Reading package lists... 95%Reading package lists... 95%Reading package lists... 97%Reading package lists... 97%Reading package lists... 97%Reading package lists... 97%Reading package lists... 97%Reading package lists... 97%Reading package lists... 97%Reading package lists... 97%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... Done
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ sudo apt-get install -y ceph ceph-mds ceph-deploy radosgw dnsmasq openssh-server ntp sshpass
Reading package lists... 0%Reading package lists... 100%Reading package lists... Done
Building dependency tree... 0%Building dependency tree... 0%Building dependency tree... 50%Building dependency tree... 50%Building dependency tree       
Reading state information... 0%Reading state information... 0%Reading state information... Done
sshpass is already the newest version.
ntp is already the newest version.
openssh-server is already the newest version.
dnsmasq is already the newest version.
The following extra packages will be installed:
  ceph-base ceph-common ceph-fs-common ceph-fuse ceph-mon ceph-osd
  cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
  libboost-program-options1.54.0 libboost-regex1.54.0 libcephfs1
  libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
  libradosstriper1 librgw2 libsnappy1 libtcmalloc-minimal4 libunwind8
  python-blinker python-cephfs python-flask python-itsdangerous python-jinja2
  python-markupsafe python-openssl python-pkg-resources python-pyinotify
  python-rados python-rbd python-setuptools python-werkzeug
Suggested packages:
  javascript-common python-flask-doc python-jinja2-doc python-openssl-doc
  python-openssl-dbg python-distribute python-distribute-doc
  python-pyinotify-doc ipython python-genshi python-lxml python-greenlet
  python-redis python-pylibmc python-memcache python-werkzeug-doc
The following NEW packages will be installed:
  ceph ceph-base ceph-common ceph-deploy ceph-fs-common ceph-fuse ceph-mds
  ceph-mon ceph-osd cryptsetup-bin libbabeltrace-ctf1 libbabeltrace1
  libboost-program-options1.54.0 libboost-regex1.54.0 libcephfs1
  libcryptsetup4 libfcgi0ldbl libgoogle-perftools4 libjs-jquery libleveldb1
  libradosstriper1 librgw2 libsnappy1 libtcmalloc-minimal4 libunwind8
  python-blinker python-cephfs python-flask python-itsdangerous python-jinja2
  python-markupsafe python-openssl python-pkg-resources python-pyinotify
  python-rados python-rbd python-setuptools python-werkzeug radosgw
0 upgraded, 39 newly installed, 0 to remove and 0 not upgraded.
Need to get 0 B/107 MB of archives.
After this operation, 503 MB of additional disk space will be used.
Extracting templates from packages: 76%Extracting templates from packages: 100%
Selecting previously unselected package libboost-program-options1.54.0:amd64.
(Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 41861 files and directories currently installed.)
Preparing to unpack .../libboost-program-options1.54.0_1.54.0-4ubuntu3.1_amd64.deb ...
Unpacking libboost-program-options1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
Selecting previously unselected package libboost-regex1.54.0:amd64.
Preparing to unpack .../libboost-regex1.54.0_1.54.0-4ubuntu3.1_amd64.deb ...
Unpacking libboost-regex1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
Selecting previously unselected package libsnappy1.
Preparing to unpack .../libsnappy1_1.1.0-1ubuntu1_amd64.deb ...
Unpacking libsnappy1 (1.1.0-1ubuntu1) ...
Selecting previously unselected package libleveldb1:amd64.
Preparing to unpack .../libleveldb1_1.15.0-2_amd64.deb ...
Unpacking libleveldb1:amd64 (1.15.0-2) ...
Selecting previously unselected package libunwind8.
Preparing to unpack .../libunwind8_1.1-2.2ubuntu3_amd64.deb ...
Unpacking libunwind8 (1.1-2.2ubuntu3) ...
Selecting previously unselected package libbabeltrace1:amd64.
Preparing to unpack .../libbabeltrace1_1.2.1-2_amd64.deb ...
Unpacking libbabeltrace1:amd64 (1.2.1-2) ...
Selecting previously unselected package libbabeltrace-ctf1:amd64.
Preparing to unpack .../libbabeltrace-ctf1_1.2.1-2_amd64.deb ...
Unpacking libbabeltrace-ctf1:amd64 (1.2.1-2) ...
Selecting previously unselected package libtcmalloc-minimal4.
Preparing to unpack .../libtcmalloc-minimal4_2.1-2ubuntu1.1_amd64.deb ...
Unpacking libtcmalloc-minimal4 (2.1-2ubuntu1.1) ...
Selecting previously unselected package libgoogle-perftools4.
Preparing to unpack .../libgoogle-perftools4_2.1-2ubuntu1.1_amd64.deb ...
Unpacking libgoogle-perftools4 (2.1-2ubuntu1.1) ...
Selecting previously unselected package libradosstriper1.
Preparing to unpack .../libradosstriper1_10.2.1-1trusty_amd64.deb ...
Unpacking libradosstriper1 (10.2.1-1trusty) ...
Selecting previously unselected package libfcgi0ldbl.
Preparing to unpack .../libfcgi0ldbl_2.4.0-8.1ubuntu5_amd64.deb ...
Unpacking libfcgi0ldbl (2.4.0-8.1ubuntu5) ...
Selecting previously unselected package librgw2.
Preparing to unpack .../librgw2_10.2.1-1trusty_amd64.deb ...
Unpacking librgw2 (10.2.1-1trusty) ...
Selecting previously unselected package python-rados.
Preparing to unpack .../python-rados_10.2.1-1trusty_amd64.deb ...
Unpacking python-rados (10.2.1-1trusty) ...
Selecting previously unselected package libcephfs1.
Preparing to unpack .../libcephfs1_10.2.1-1trusty_amd64.deb ...
Unpacking libcephfs1 (10.2.1-1trusty) ...
Selecting previously unselected package python-cephfs.
Preparing to unpack .../python-cephfs_10.2.1-1trusty_amd64.deb ...
Unpacking python-cephfs (10.2.1-1trusty) ...
Selecting previously unselected package python-rbd.
Preparing to unpack .../python-rbd_10.2.1-1trusty_amd64.deb ...
Unpacking python-rbd (10.2.1-1trusty) ...
Selecting previously unselected package ceph-common.
Preparing to unpack .../ceph-common_10.2.1-1trusty_amd64.deb ...
Unpacking ceph-common (10.2.1-1trusty) ...
dpkg: warning: ceph-common: conffile 'etc/default/ceph' is not a plain file or symlink (= '/etc/default/ceph')
Selecting previously unselected package libcryptsetup4.
Preparing to unpack .../libcryptsetup4_2%3a1.6.1-1ubuntu1_amd64.deb ...
Unpacking libcryptsetup4 (2:1.6.1-1ubuntu1) ...
Selecting previously unselected package cryptsetup-bin.
Preparing to unpack .../cryptsetup-bin_2%3a1.6.1-1ubuntu1_amd64.deb ...
Unpacking cryptsetup-bin (2:1.6.1-1ubuntu1) ...
Selecting previously unselected package python-pkg-resources.
Preparing to unpack .../python-pkg-resources_3.3-1ubuntu2_all.deb ...
Unpacking python-pkg-resources (3.3-1ubuntu2) ...
Selecting previously unselected package ceph-base.
Preparing to unpack .../ceph-base_10.2.1-1trusty_amd64.deb ...
Unpacking ceph-base (10.2.1-1trusty) ...
Selecting previously unselected package libjs-jquery.
Preparing to unpack .../libjs-jquery_1.7.2+dfsg-2ubuntu1_all.deb ...
Unpacking libjs-jquery (1.7.2+dfsg-2ubuntu1) ...
Selecting previously unselected package python-werkzeug.
Preparing to unpack .../python-werkzeug_0.9.4+dfsg-1.1ubuntu2_all.deb ...
Unpacking python-werkzeug (0.9.4+dfsg-1.1ubuntu2) ...
Selecting previously unselected package python-markupsafe.
Preparing to unpack .../python-markupsafe_0.18-1build2_amd64.deb ...
Unpacking python-markupsafe (0.18-1build2) ...
Selecting previously unselected package python-jinja2.
Preparing to unpack .../python-jinja2_2.7.2-2_all.deb ...
Unpacking python-jinja2 (2.7.2-2) ...
Selecting previously unselected package python-itsdangerous.
Preparing to unpack .../python-itsdangerous_0.22+dfsg1-1build1_all.deb ...
Unpacking python-itsdangerous (0.22+dfsg1-1build1) ...
Selecting previously unselected package python-flask.
Preparing to unpack .../python-flask_0.10.1-2build1_all.deb ...
Unpacking python-flask (0.10.1-2build1) ...
Selecting previously unselected package ceph-mon.
Preparing to unpack .../ceph-mon_10.2.1-1trusty_amd64.deb ...
Unpacking ceph-mon (10.2.1-1trusty) ...
Selecting previously unselected package ceph-osd.
Preparing to unpack .../ceph-osd_10.2.1-1trusty_amd64.deb ...
Unpacking ceph-osd (10.2.1-1trusty) ...
Selecting previously unselected package ceph.
Preparing to unpack .../ceph_10.2.1-1trusty_amd64.deb ...
Unpacking ceph (10.2.1-1trusty) ...
Selecting previously unselected package python-setuptools.
Preparing to unpack .../python-setuptools_3.3-1ubuntu2_all.deb ...
Unpacking python-setuptools (3.3-1ubuntu2) ...
Selecting previously unselected package ceph-deploy.
Preparing to unpack .../ceph-deploy_1.5.33_all.deb ...
Unpacking ceph-deploy (1.5.33) ...
Selecting previously unselected package ceph-fs-common.
Preparing to unpack .../ceph-fs-common_10.2.1-1trusty_amd64.deb ...
Unpacking ceph-fs-common (10.2.1-1trusty) ...
Selecting previously unselected package ceph-fuse.
Preparing to unpack .../ceph-fuse_10.2.1-1trusty_amd64.deb ...
Unpacking ceph-fuse (10.2.1-1trusty) ...
Selecting previously unselected package ceph-mds.
Preparing to unpack .../ceph-mds_10.2.1-1trusty_amd64.deb ...
Unpacking ceph-mds (10.2.1-1trusty) ...
Selecting previously unselected package python-blinker.
Preparing to unpack .../python-blinker_1.3.dfsg1-1ubuntu2_all.deb ...
Unpacking python-blinker (1.3.dfsg1-1ubuntu2) ...
Selecting previously unselected package python-openssl.
Preparing to unpack .../python-openssl_0.13-2ubuntu6_amd64.deb ...
Unpacking python-openssl (0.13-2ubuntu6) ...
Selecting previously unselected package python-pyinotify.
Preparing to unpack .../python-pyinotify_0.9.4-1build1_all.deb ...
Unpacking python-pyinotify (0.9.4-1build1) ...
Selecting previously unselected package radosgw.
Preparing to unpack .../radosgw_10.2.1-1trusty_amd64.deb ...
Unpacking radosgw (10.2.1-1trusty) ...
Processing triggers for man-db (2.6.7.1-1ubuntu1) ...
Processing triggers for ureadahead (0.100.0-16) ...
Setting up libboost-program-options1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
Setting up libboost-regex1.54.0:amd64 (1.54.0-4ubuntu3.1) ...
Setting up libsnappy1 (1.1.0-1ubuntu1) ...
Setting up libleveldb1:amd64 (1.15.0-2) ...
Setting up libunwind8 (1.1-2.2ubuntu3) ...
Setting up libbabeltrace1:amd64 (1.2.1-2) ...
Setting up libbabeltrace-ctf1:amd64 (1.2.1-2) ...
Setting up libtcmalloc-minimal4 (2.1-2ubuntu1.1) ...
Setting up libgoogle-perftools4 (2.1-2ubuntu1.1) ...
Setting up libradosstriper1 (10.2.1-1trusty) ...
Setting up libfcgi0ldbl (2.4.0-8.1ubuntu5) ...
Setting up librgw2 (10.2.1-1trusty) ...
Setting up python-rados (10.2.1-1trusty) ...
Setting up libcephfs1 (10.2.1-1trusty) ...
Setting up python-cephfs (10.2.1-1trusty) ...
Setting up python-rbd (10.2.1-1trusty) ...
Setting up ceph-common (10.2.1-1trusty) ...
dpkg: warning: ceph-common: conffile '/etc/default/ceph' is not a plain file or symlink (= '/etc/default/ceph')
Setting system user ceph properties..usermod: no changes
..done
Setting up libcryptsetup4 (2:1.6.1-1ubuntu1) ...
Setting up cryptsetup-bin (2:1.6.1-1ubuntu1) ...
Setting up python-pkg-resources (3.3-1ubuntu2) ...
Setting up libjs-jquery (1.7.2+dfsg-2ubuntu1) ...
Setting up python-werkzeug (0.9.4+dfsg-1.1ubuntu2) ...
Setting up python-markupsafe (0.18-1build2) ...
Setting up python-jinja2 (2.7.2-2) ...
Setting up python-itsdangerous (0.22+dfsg1-1build1) ...
Setting up python-flask (0.10.1-2build1) ...
Setting up python-setuptools (3.3-1ubuntu2) ...
Setting up ceph-deploy (1.5.33) ...
Setting up ceph-fs-common (10.2.1-1trusty) ...
Setting up ceph-fuse (10.2.1-1trusty) ...
Setting up python-blinker (1.3.dfsg1-1ubuntu2) ...
Setting up python-openssl (0.13-2ubuntu6) ...
Setting up python-pyinotify (0.9.4-1build1) ...
Processing triggers for ureadahead (0.100.0-16) ...
Setting up radosgw (10.2.1-1trusty) ...
radosgw-all start/running
Setting up ceph-base (10.2.1-1trusty) ...
ceph-all start/running
Setting up ceph-mon (10.2.1-1trusty) ...
start: Job is already running: ceph-mon-all
Setting up ceph-osd (10.2.1-1trusty) ...
start: Job is already running: ceph-osd-all
Setting up ceph (10.2.1-1trusty) ...
Setting up ceph-mds (10.2.1-1trusty) ...
ceph-mds-all start/running
Processing triggers for libc-bin (2.19-0ubuntu6.7) ...
Processing triggers for ureadahead (0.100.0-16) ...
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ls
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ceph-deploy new node1
[[1mceph_deploy.conf[0m][[1;34mDEBUG[0m ] found configuration file at: /home/megdc/.cephdeploy.conf
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] Invoked (1.5.33): /usr/bin/ceph-deploy new node1
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] ceph-deploy options:
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  username                      : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  func                          : <function new at 0x7f510f519410>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  verbose                       : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  overwrite_conf                : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  quiet                         : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f510f538ea8>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cluster                       : ceph
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  ssh_copykey                   : True
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  mon                           : ['node1']
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  public_network                : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  ceph_conf                     : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cluster_network               : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  default_release               : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  fsid                          : None
[[1mceph_deploy.new[0m][[1;34mDEBUG[0m ] Creating new cluster named ceph
[[1mceph_deploy.new[0m][[1;37mINFO[0m  ] making sure passwordless SSH succeeds
[[1mnode1[0m][[1;34mDEBUG[0m ] connection detected need for sudo
[[1mnode1[0m][[1;34mDEBUG[0m ] connected to host: node1 
[[1mnode1[0m][[1;34mDEBUG[0m ] detect platform information from remote host
[[1mnode1[0m][[1;34mDEBUG[0m ] detect machine type
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/initctl version
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /bin/ip link show
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /bin/ip addr show
[[1mnode1[0m][[1;34mDEBUG[0m ] IP addresses found: ['136.243.49.217']
[[1mceph_deploy.new[0m][[1;34mDEBUG[0m ] Resolving host node1
[[1mceph_deploy.new[0m][[1;34mDEBUG[0m ] Monitor node1 at 2a01:4f8:212:11e8::2
[[1mceph_deploy.new[0m][[1;37mINFO[0m  ] Monitors are IPv6, binding Messenger traffic on IPv6
[[1mceph_deploy.new[0m][[1;34mDEBUG[0m ] Monitor initial members are ['node1']
[[1mceph_deploy.new[0m][[1;34mDEBUG[0m ] Monitor addrs are ['[2a01:4f8:212:11e8::2]']
[[1mceph_deploy.new[0m][[1;34mDEBUG[0m ] Creating a random mon key...
[[1mceph_deploy.new[0m][[1;34mDEBUG[0m ] Writing monitor keyring to ceph.mon.keyring...
[[1mceph_deploy.new[0m][[1;34mDEBUG[0m ] Writing initial config to ceph.conf...
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ls
ceph.conf  ceph-deploy-ceph.log  ceph.mon.keyring
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ cat ceph.conf 
[global]
fsid = 768fbc30-f707-4f89-bdd8-59972cd96ff3
ms_bind_ipv6 = true
mon_initial_members = node1
mon_host = [2a01:4f8:212:11e8::2]
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx

]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ echo "osd_pool_default_size = 2" >> ceph.conf
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ceph-deploy disk zap node1:sdc node1:sdd[K[K[K[K[K[K[K[K[K[K
[[1mceph_deploy.conf[0m][[1;34mDEBUG[0m ] found configuration file at: /home/megdc/.cephdeploy.conf
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] Invoked (1.5.33): /usr/bin/ceph-deploy disk zap node1:sdc
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] ceph-deploy options:
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  username                      : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  verbose                       : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  overwrite_conf                : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  subcommand                    : zap
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  quiet                         : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9ec2c8b5a8>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cluster                       : ceph
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  func                          : <function disk at 0x7f9ec30f05f0>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  ceph_conf                     : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  default_release               : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  disk                          : [('node1', '/dev/sdc', None)]
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] zapping /dev/sdc on node1
[[1mnode1[0m][[1;34mDEBUG[0m ] connection detected need for sudo
[[1mnode1[0m][[1;34mDEBUG[0m ] connected to host: node1 
[[1mnode1[0m][[1;34mDEBUG[0m ] detect platform information from remote host
[[1mnode1[0m][[1;34mDEBUG[0m ] detect machine type
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/initctl version
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mceph_deploy.osd[0m][[1;37mINFO[0m  ] Distro info: Ubuntu 14.04 trusty
[[1mnode1[0m][[1;34mDEBUG[0m ] zeroing last few blocks of device
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdc
[[1mnode1[0m][[1;33mWARNIN[0m] Caution: invalid backup GPT header, but valid main header; regenerating
[[1mnode1[0m][[1;33mWARNIN[0m] backup header from main header.
[[1mnode1[0m][[1;33mWARNIN[0m] 
[[1mnode1[0m][[1;33mWARNIN[0m] Warning! Main and backup partition tables differ! Use the 'c' and 'e' options
[[1mnode1[0m][[1;33mWARNIN[0m] on the recovery & transformation menu to examine the two tables.
[[1mnode1[0m][[1;33mWARNIN[0m] 
[[1mnode1[0m][[1;33mWARNIN[0m] Warning! One or more CRCs don't match. You should repair the disk!
[[1mnode1[0m][[1;33mWARNIN[0m] 
[[1mnode1[0m][[1;34mDEBUG[0m ] ****************************************************************************
[[1mnode1[0m][[1;34mDEBUG[0m ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[[1mnode1[0m][[1;34mDEBUG[0m ] verification and recovery are STRONGLY recommended.
[[1mnode1[0m][[1;34mDEBUG[0m ] ****************************************************************************
[[1mnode1[0m][[1;34mDEBUG[0m ] Warning: The kernel is still using the old partition table.
[[1mnode1[0m][[1;34mDEBUG[0m ] The new table will be used at the next reboot.
[[1mnode1[0m][[1;34mDEBUG[0m ] GPT data structures destroyed! You may now partition the disk using fdisk or
[[1mnode1[0m][[1;34mDEBUG[0m ] other utilities.
[[1mnode1[0m][[1;34mDEBUG[0m ] Creating new GPT entries.
[[1mnode1[0m][[1;34mDEBUG[0m ] Warning: The kernel is still using the old partition table.
[[1mnode1[0m][[1;34mDEBUG[0m ] The new table will be used at the next reboot.
[[1mnode1[0m][[1;34mDEBUG[0m ] The operation has completed successfully.
[[1mnode1[0m][[1;33mWARNIN[0m] No data was received after 300 seconds, disconnecting...
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] Calling partprobe on zapped device /dev/sdc
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/partprobe /dev/sdc
[[1mnode1[0m][[1;33mWARNIN[0m] Error: Partition(s) 1 on /dev/sdc have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[[1mnode1[0m][[1;31mERROR[0m ] RuntimeError: command returned non-zero exit status: 1
[[1mceph_deploy[0m][[1;31mERROR[0m ] RuntimeError: Failed to execute command: /sbin/partprobe /dev/sdc

Error in sys.exitfunc:
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda       8:0    0 223.6G  0 disk  
├─sda1    8:1    0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sda2    8:2    0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sda3    8:3    0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdb       8:16   0 223.6G  0 disk  
├─sdb1    8:17   0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sdb2    8:18   0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sdb3    8:19   0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdc       8:32   0   5.5T  0 disk  
└─sdc1    8:33   0   5.5T  0 part  /storage1
sdd       8:48   0   5.5T  0 disk  
├─sdd1    8:49   0   5.5T  0 part  
└─sdd2    8:50   0     5G  0 part  
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ nano /etc/ceph/[K[K[K[K[Kfstab
[?1049h[1;39r(B[m[4l[?7h[?12l[?25h[?1h=[?1h=[?1h=[39;49m[39;49m(B[m[H[2J(B[0;7m  GNU nano 2.2.6                                   File: /etc/fstab                                                                            [3;1H(B[mproc /proc proc defaults 0 0[4d/dev/md/0 /boot ext3 defaults 0 0[5d/dev/md/1 none swap sw 0 0[6d/dev/md/2 / ext4 defaults 0 0[7d#/dev/sdc1 /storage1 ext4 defaults 0 0[8d#/dev/sdd1 /storage2 ext4 defaults 0 0[37;49H(B[0;7m[ Read 6 lines (Warning: No write permission) ][38d^G(B[m Get Help[38;24H(B[0;7m^O(B[m WriteOut[38;47H(B[0;7m^R(B[m Read File[38;70H(B[0;7m^Y(B[m Prev Page[38;93H(B[0;7m^K(B[m Cut Text[38;116H(B[0;7m^C(B[m Cur Pos[39d(B[0;7m^X(B[m Exit[39;24H(B[0;7m^J(B[m Justify[39;47H(B[0;7m^W(B[m Where Is[39;70H(B[0;7m^V(B[m Next Page[39;93H(B[0;7m^U(B[m UnCut Text[39;116H(B[0;7m^T(B[m To Spell[3d[4d[5d[6d[7d[1;134H(B[0;7mModified[7d(B[m[1P#/dev/sdc1 /storage1 ext4 defaults 0 0#[37d(B[0;7mSave modified buffer (ANSWERING "No" WILL DESTROY CHANGES) ?                                                                                   [38;1H Y(B[m Yes[K[39d(B[0;7m N(B[m No  [39;17H(B[0;7m^C(B[m Cancel[K[37;62H[38d[J[39;143H[39;1H[?1049l[?1l>]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ceph-deploy disk list node1
[[1mceph_deploy.conf[0m][[1;34mDEBUG[0m ] found configuration file at: /home/megdc/.cephdeploy.conf
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] Invoked (1.5.33): /usr/bin/ceph-deploy disk list node1
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] ceph-deploy options:
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  username                      : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  verbose                       : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  overwrite_conf                : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  subcommand                    : list
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  quiet                         : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f567de1e5a8>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cluster                       : ceph
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  func                          : <function disk at 0x7f567e2835f0>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  ceph_conf                     : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  default_release               : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  disk                          : [('node1', None, None)]
[[1mnode1[0m][[1;34mDEBUG[0m ] connection detected need for sudo
[[1mnode1[0m][[1;34mDEBUG[0m ] connected to host: node1 
[[1mnode1[0m][[1;34mDEBUG[0m ] detect platform information from remote host
[[1mnode1[0m][[1;34mDEBUG[0m ] detect machine type
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/initctl version
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mceph_deploy.osd[0m][[1;37mINFO[0m  ] Distro info: Ubuntu 14.04 trusty
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] Listing disks on node1...
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /usr/sbin/ceph-disk list
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop0 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop1 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop2 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop3 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop4 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop5 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop6 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop7 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/md0 other, ext3, mounted on /boot
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/md1 swap, swap
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/md2 other, ext4, mounted on /
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram0 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram1 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram10 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram11 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram12 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram13 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram14 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram15 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram2 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram3 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram4 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram5 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram6 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram7 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram8 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram9 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sda :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sda1 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sda2 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sda3 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sdb :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdb1 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdb2 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdb3 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sdc :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdc1 other, xfs, mounted on /storage1
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sdd :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdd2 other
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdd1 other, xfs
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda       8:0    0 223.6G  0 disk  
├─sda1    8:1    0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sda2    8:2    0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sda3    8:3    0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdb       8:16   0 223.6G  0 disk  
├─sdb1    8:17   0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sdb2    8:18   0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sdb3    8:19   0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdc       8:32   0   5.5T  0 disk  
└─sdc1    8:33   0   5.5T  0 part  /storage1
sdd       8:48   0   5.5T  0 disk  
├─sdd1    8:49   0   5.5T  0 part  
└─sdd2    8:50   0     5G  0 part  
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ lsblkceph-deploy disk list node1[12Pnano /etc/fstabceph-deploy disk list node1lsblk[K[K[K[K[K[Kumount /storage1 
umount: /storage1 is not mounted (according to mtab)
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ nano /etc/mtab[1@s[1@u[1@d[1@o[1@ 
[?1049h[1;39r(B[m[4l[?7h[?12l[?25h[?1h=[?1h=[?1h=[39;49m[39;49m(B[m[H[2J(B[0;7m  GNU nano 2.2.6                                   File: /etc/mtab                                                                             [3;1H(B[m/dev/md2 / ext4 rw 0 0[4dproc /proc proc rw 0 0[5dsysfs /sys sysfs rw,noexec,nosuid,nodev 0 0[6dnone /sys/fs/cgroup tmpfs rw 0 0[7dnone /sys/fs/fuse/connections fusectl rw 0 0[8dnone /sys/kernel/debug debugfs rw 0 0[9dnone /sys/kernel/security securityfs rw 0 0[10dudev /dev devtmpfs rw,mode=0755 0 0[11ddevpts /dev/pts devpts rw,noexec,nosuid,gid=5,mode=0620 0 0[12dtmpfs /run tmpfs rw,noexec,nosuid,size=10%,mode=0755 0 0[13dnone /run/lock tmpfs rw,noexec,nosuid,nodev,size=5242880 0 0[14dnone /run/shm tmpfs rw,nosuid,nodev 0 0[15dnone /run/user tmpfs rw,noexec,nosuid,nodev,size=104857600,mode=0755 0 0[16dnone /sys/fs/pstore pstore rw 0 0[17dtracefs /var/lib/ureadahead/debugfs/tracing tracefs rw,relatime 0 0[18d/dev/md0 /boot ext3 rw 0 0[19dsystemd /sys/fs/cgroup/systemd cgroup rw,noexec,nosuid,nodev,none,name=systemd 0 0[37;64H(B[0;7m[ Read 17 lines ][38d^G(B[m Get Help[38;24H(B[0;7m^O(B[m WriteOut[38;47H(B[0;7m^R(B[m Read File[38;70H(B[0;7m^Y(B[m Prev Page[38;93H(B[0;7m^K(B[m Cut Text[38;116H(B[0;7m^C(B[m Cur Pos[39d(B[0;7m^X(B[m Exit[39;24H(B[0;7m^J(B[m Justify[39;47H(B[0;7m^W(B[m Where Is[39;70H(B[0;7m^V(B[m Next Page[39;93H(B[0;7m^U(B[m UnCut Text[39;116H(B[0;7m^T(B[m To Spell[3d[4d[5d[6d[7d[8d[9d[10d[11d[12d[13d[14d[15d[38d[J[39;143H[39;1H[?1049l[?1l>]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ cat /etc/ma[Ktab.bk 
/dev/md2 / ext4 rw 0 0
proc /proc proc rw 0 0
sysfs /sys sysfs rw,noexec,nosuid,nodev 0 0
none /sys/fs/cgroup tmpfs rw 0 0
none /sys/fs/fuse/connections fusectl rw 0 0
none /sys/kernel/debug debugfs rw 0 0
none /sys/kernel/security securityfs rw 0 0
udev /dev devtmpfs rw,mode=0755 0 0
devpts /dev/pts devpts rw,noexec,nosuid,gid=5,mode=0620 0 0
tmpfs /run tmpfs rw,noexec,nosuid,size=10%,mode=0755 0 0
none /run/lock tmpfs rw,noexec,nosuid,nodev,size=5242880 0 0
none /run/shm tmpfs rw,nosuid,nodev 0 0
none /run/user tmpfs rw,noexec,nosuid,nodev,size=104857600,mode=0755 0 0
none /sys/fs/pstore pstore rw 0 0
tracefs /var/lib/ureadahead/debugfs/tracing tracefs rw,relatime 0 0
/dev/md0 /boot ext3 rw 0 0
systemd /sys/fs/cgroup/systemd cgroup rw,noexec,nosuid,nodev,none,name=systemd 0 0
/dev/sdd1 /var/lib/ceph/osd/ceph-0 xfs rw,noatime,inode64 0 0
/dev/sdc1 /var/lib/ceph/osd/ceph-1 xfs rw,noatime,inode64 0 0
/dev/sdc1 /storage1 xfs rw 0 0
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ 
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ suo[Kdo nano /etc/fstab
[?1049h[1;39r(B[m[4l[?7h[?12l[?25h[?1h=[?1h=[?1h=[39;49m[39;49m(B[m[H[2J(B[0;7m  GNU nano 2.2.6                                   File: /etc/fstab                                                                            [3;1H(B[mproc /proc proc defaults 0 0[4d/dev/md/0 /boot ext3 defaults 0 0[5d/dev/md/1 none swap sw 0 0[6d/dev/md/2 / ext4 defaults 0 0[7d#/dev/sdc1 /storage1 ext4 defaults 0 0[8d#/dev/sdd1 /storage2 ext4 defaults 0 0[37;64H(B[0;7m[ Read 6 lines ][38d^G(B[m Get Help[38;24H(B[0;7m^O(B[m WriteOut[38;47H(B[0;7m^R(B[m Read File[38;70H(B[0;7m^Y(B[m Prev Page[38;93H(B[0;7m^K(B[m Cut Text[38;116H(B[0;7m^C(B[m Cur Pos[39d(B[0;7m^X(B[m Exit[39;24H(B[0;7m^J(B[m Justify[39;47H(B[0;7m^W(B[m Where Is[39;70H(B[0;7m^V(B[m Next Page[39;93H(B[0;7m^U(B[m UnCut Text[39;116H(B[0;7m^T(B[m To Spell[3d[4d[5d[6d[7d[38d[J[39;143H[39;1H[?1049l[?1l>]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda       8:0    0 223.6G  0 disk  
├─sda1    8:1    0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sda2    8:2    0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sda3    8:3    0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdb       8:16   0 223.6G  0 disk  
├─sdb1    8:17   0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sdb2    8:18   0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sdb3    8:19   0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdc       8:32   0   5.5T  0 disk  
└─sdc1    8:33   0   5.5T  0 part  /storage1
sdd       8:48   0   5.5T  0 disk  
├─sdd1    8:49   0   5.5T  0 part  
└─sdd2    8:50   0     5G  0 part  
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ mount
/dev/md2 on / type ext4 (rw)
proc on /proc type proc (rw)
sysfs on /sys type sysfs (rw,noexec,nosuid,nodev)
none on /sys/fs/cgroup type tmpfs (rw)
none on /sys/fs/fuse/connections type fusectl (rw)
none on /sys/kernel/debug type debugfs (rw)
none on /sys/kernel/security type securityfs (rw)
udev on /dev type devtmpfs (rw,mode=0755)
devpts on /dev/pts type devpts (rw,noexec,nosuid,gid=5,mode=0620)
tmpfs on /run type tmpfs (rw,noexec,nosuid,size=10%,mode=0755)
none on /run/lock type tmpfs (rw,noexec,nosuid,nodev,size=5242880)
none on /run/shm type tmpfs (rw,nosuid,nodev)
none on /run/user type tmpfs (rw,noexec,nosuid,nodev,size=104857600,mode=0755)
none on /sys/fs/pstore type pstore (rw)
tracefs on /var/lib/ureadahead/debugfs/tracing type tracefs (rw,relatime)
/dev/md0 on /boot type ext3 (rw)
systemd on /sys/fs/cgroup/systemd type cgroup (rw,noexec,nosuid,nodev,none,name=systemd)
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ blkid
/dev/sda1: UUID="4f9d7593-dca1-c4b0-50e3-f8386e8af941" UUID_SUB="6593a1e9-c428-333f-0b93-7e07b4441753" LABEL="rescue:0" TYPE="linux_raid_member" 
/dev/sda2: UUID="98acdb0b-3496-cdfb-c089-bdc4b009941b" UUID_SUB="3382f202-4376-4445-e814-4ada95dc7ad8" LABEL="rescue:1" TYPE="linux_raid_member" 
/dev/sda3: UUID="c3f2f4af-f155-fb5b-85a9-37336089d036" UUID_SUB="f8a5d4e9-bb86-5d46-b652-5124d34b3fd1" LABEL="rescue:2" TYPE="linux_raid_member" 
/dev/sdb1: UUID="4f9d7593-dca1-c4b0-50e3-f8386e8af941" UUID_SUB="dff6602c-1f3d-55f2-67b3-8a9f5e81e91f" LABEL="rescue:0" TYPE="linux_raid_member" 
/dev/sdb2: UUID="98acdb0b-3496-cdfb-c089-bdc4b009941b" UUID_SUB="10c8437b-44de-2780-151e-537db4adf2fd" LABEL="rescue:1" TYPE="linux_raid_member" 
/dev/sdb3: UUID="c3f2f4af-f155-fb5b-85a9-37336089d036" UUID_SUB="c6d1e62b-a64a-4d63-72a5-b4e70ecd3cb5" LABEL="rescue:2" TYPE="linux_raid_member" 
/dev/sdd1: UUID="efda3f44-31ef-4282-8780-9c25e62fb5a6" TYPE="xfs" 
/dev/md2: UUID="c81b0a14-ee5b-47ed-b402-6e873e126d03" TYPE="ext4" 
/dev/md0: UUID="aa115c94-46d4-4b87-82a3-9bc3b775308a" TYPE="ext3" 
/dev/md1: UUID="d0015543-f580-4914-9370-b4444f35a3b4" TYPE="swap" 
/dev/sdc1: UUID="d6ad8b5f-acc4-4767-93cf-6ed259d0d474" TYPE="xfs" 
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ls
ceph.conf  ceph-deploy-ceph.log  ceph.mon.keyring
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda       8:0    0 223.6G  0 disk  
├─sda1    8:1    0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sda2    8:2    0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sda3    8:3    0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdb       8:16   0 223.6G  0 disk  
├─sdb1    8:17   0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sdb2    8:18   0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sdb3    8:19   0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdc       8:32   0   5.5T  0 disk  
└─sdc1    8:33   0   5.5T  0 part  /storage1
sdd       8:48   0   5.5T  0 disk  
├─sdd1    8:49   0   5.5T  0 part  
└─sdd2    8:50   0     5G  0 part  
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ umount /storage1 
umount: /storage1 is not mounted (according to mtab)
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ n[Knano /[K/etc/mtab [K[Kb.bk 
[?1049h[1;39r(B[m[4l[?7h[?12l[?25h[?1h=[?1h=[?1h=[39;49m[39;49m(B[m[H[2J(B[0;7m  GNU nano 2.2.6                                  File: /etc/mtab.bk                                                                           [3;1H(B[m/dev/md2 / ext4 rw 0 0[4dproc /proc proc rw 0 0[5dsysfs /sys sysfs rw,noexec,nosuid,nodev 0 0[6dnone /sys/fs/cgroup tmpfs rw 0 0[7dnone /sys/fs/fuse/connections fusectl rw 0 0[8dnone /sys/kernel/debug debugfs rw 0 0[9dnone /sys/kernel/security securityfs rw 0 0[10dudev /dev devtmpfs rw,mode=0755 0 0[11ddevpts /dev/pts devpts rw,noexec,nosuid,gid=5,mode=0620 0 0[12dtmpfs /run tmpfs rw,noexec,nosuid,size=10%,mode=0755 0 0[13dnone /run/lock tmpfs rw,noexec,nosuid,nodev,size=5242880 0 0[14dnone /run/shm tmpfs rw,nosuid,nodev 0 0[15dnone /run/user tmpfs rw,noexec,nosuid,nodev,size=104857600,mode=0755 0 0[16dnone /sys/fs/pstore pstore rw 0 0[17dtracefs /var/lib/ureadahead/debugfs/tracing tracefs rw,relatime 0 0[18d/dev/md0 /boot ext3 rw 0 0[19dsystemd /sys/fs/cgroup/systemd cgroup rw,noexec,nosuid,nodev,none,name=systemd 0 0[20d/dev/sdd1 /var/lib/ceph/osd/ceph-0 xfs rw,noatime,inode64 0 0[21d/dev/sdc1 /var/lib/ceph/osd/ceph-1 xfs rw,noatime,inode64 0 0[22d/dev/sdc1 /storage1 xfs rw 0 0[37;48H(B[0;7m[ Read 20 lines (Warning: No write permission) ][38d^G(B[m Get Help[38;24H(B[0;7m^O(B[m WriteOut[38;47H(B[0;7m^R(B[m Read File[38;70H(B[0;7m^Y(B[m Prev Page[38;93H(B[0;7m^K(B[m Cut Text[38;116H(B[0;7m^C(B[m Cur Pos[39d(B[0;7m^X(B[m Exit[39;24H(B[0;7m^J(B[m Justify[39;47H(B[0;7m^W(B[m Where Is[39;70H(B[0;7m^V(B[m Next Page[39;93H(B[0;7m^U(B[m UnCut Text[39;116H(B[0;7m^T(B[m To Spell[3d[4d[5d[6d[7d[8d[9d[10d[11d[12d[13d[14d[15d[16d[17d[18d[38d[J[39;143H[39;1H[?1049l[?1l>]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ history
  759  ceph-deploy --version
  760  sudo apt-get -y install ceph-deploy ceph-common ceph-mds
  761  ceph -v
  762  ceph status
  763  sudo apt-get update
  764  ceph osd tree
  765  sudo start ceph-all
  766  sudo stop ceph-all
  767  sudo start ceph-all
  768  ceph osd tree
  769  sudo restart ceph-osd id=0
  770  sudo /etc/init.d/ceph -a start
  771  ceph status
  772  ceph osd -h
  773  ceph osd up 0
  774  sudo /etc/init.d/ceph -a restart
  775  ceph status
  776  ceph -v
  777  ceph osd set sortbitwise
  778  sudo nano /etc/ceph/ceph.conf 
  779  sudo /etc/init.d/ceph -a restart
  780  ceph status
  781  ceph -v
  782  ceph osd set sortbitwise
  783  sudo /etc/init.d/ceph -a restart
  784  nano /etc/ceph/ceph.conf 
  785  cd /var/run/ceph/
  786  exit
  787  ceph-deploy purgedata node1 node2 master
  788  ping master
  789  sudo nano /etc/hosts
  790  ping master
  791  cat ../.ssh/id_rsa.pub 
  792  ssh master
  793  cd ..
  794  ls -la
  795  cd .ssh/
  796  ls
  797  nano config 
  798  nano ssh_config 
  799  cd ..
  800  ls
  801  cd ceph-cluster/
  802  ls
  803  mv * /root/ceph.bk
  804  sudo mv * /root/ceph.bk
  805  ls
  806  ceph-deploy uninstall node1 node2 master
  807  ceph-deploy purgedata node1 node2 master
  808  ceph-deploy forgetkeys
  809  ceph-deploy purge node1 node2 master
  810  sudo apt-get -y remove ceph-deploy ceph-common ceph-mds ceph
  811  sudo apt-get -y purge ceph-deploy ceph-common ceph-mds ceph
  812  apt-get autoremove
  813  sudo apt-get autoremove
  814  sudo rm -r /run/ceph
  815  sudo rm -r /var/lib/ceph
  816  sudo rm /var/log/upstart/ceph*
  817  sudo rm -rf /home/megdc/ceph-cluster/*
  818  rm -rf /storage1/osd
  819  sudo rm -rf /storage1/osd
  820  sudo rm -rf /storage2/osd
  821  echo deb https://download.ceph.com/debian-infernalis/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list
  822  sudo echo deb https://download.ceph.com/debian-infernalis/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list
  823  apt-get update
  824  sudo apt-get update
  825  sudo apt-get install ceph-common
  826  history
  827  sudo apt-get install ceph-common
  828  sudo apt-get upgrade
  829  sudo apt-get remove libradosstriper1
  830  su oneadmin
  831  sudo apt-get install ceph-common
  832  sudo apt-get remove librbd1
  833  apt-get autoremove
  834  sudo apt-get autoremove
  835  sudo echo deb https://download.ceph.com/debian-jewel/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list
  836  cat /etc/apt/sources.list.d/ceph.list 
  837  apt-get update
  838  sudo apt-get update
  839  sudo apt-get install -y ceph ceph-mds ceph-deploy 
  840  ceph-deploy new node1
  841  ls
  842  cat ceph.conf 
  843  cat ceph.mon.keyring 
  844  ceph-deploy install node1 node2 master
  845  sudo mkdir -p /storage1/osd
  846  sudo mkdir -p /storage2/osd
  847  ceph-deploy mon create-initial
  848  ls
  849  tail -20 ceph-deploy-ceph.log 
  850  echo "osd_pool_default_size = 2" >> ceph.conf
  851  sudo ls /etc/ceph/ceph.conf 
  852  sudo cat /etc/ceph/ceph.conf 
  853  ceph status
  854  ceph health
  855  ls
  856  nano ceph.conf 
  857  ceph-deploy osd -h
  858  ceph-deploy osd prepare -h
  859  ceph-deploy osd active -h
  860  ceph-deploy osd activate -h
  861  ceph-deploy osd --fs-type ext4 prepare node1:/storage1/osd node1:/storage2/osd node2:/storage3/osd node2:/storage4/osd master:/storage5/osd master:/storage6/osd
  862  ceph-deploy osd prepare --fs-type ext4 node1:/storage1/osd node1:/storage2/osd node2:/storage3/osd node2:/storage4/osd master:/storage5/osd master:/storage6/osd
  863  ceph-deploy osd prepare --fs-type ext4 node1:/storage1/osd node1:/storage2/osd
  864  ls /etc/ceph/
  865  sudo cat /etc/ceph/ceph.client.admin.keyring 
  866  ls
  867  cat ceph.client.admin.keyring 
  868  rm -f /etc/ceph/ceph.conf
  869  sudo rm -f /etc/ceph/ceph.conf
  870  ceph-deploy osd prepare --fs-type ext4 node1:/storage1/osd node1:/storage2/osd
  871  cat /etc/ceph/ceph.conf 
  872  ceph-deploy osd --fs-type ext4 prepare node2:/storage3/osd node2:/storage4/osd
  873  ceph-deploy osd prepare --fs-type ext4 node2:/storage3/osd node2:/storage4/osd
  874  ceph-deploy osd prepare --fs-type ext4 master:/storage5/osd master:/storage6/osd
  875  ceph-deploy osd activate node1:/storage/osd node1:/storage/osd
  876  ceph-deploy osd activate node1:/storage1/osd node1:/storage2/osd
  877  ls -la /storage1/osd/
  878  cat /storage1/osd/ceph_fsid 
  879  cat /storage1/osd/fsid 
  880  cat /storage2/osd/fsid 
  881  cat /storage2/osd/ceph_fsid 
  882  cat /etc/os-release 
  883  sudo chown -R ceph:ceph /storage1/osd
  884  sudo chown -R ceph:ceph /storage2/osd
  885  ls -la /storage1/osd/
  886  ceph-deploy osd activate node1:/storage1/osd node1:/storage2/osd
  887  ceph status
  888  sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
  889  ceph-deploy osd activate node2:/storage3/osd osd1:/storage4/osd
  890  ceph-deploy osd activate node2:/storage4/osd
  891  ceph status
  892  ceph-deploy osd activate master:/storage6/osd master:/storage6/osd
  893  ceph-deploy admin node1
  894  cat /etc/ceph/ceph.conf 
  895  sudo chmod +r /etc/ceph/ceph.client.admin.keyring
  896  ceph status
  897  ceph osd tree
  898  ceph-deploy osd activate master:/storage5/osd
  899  ceph status
  900  ceph osd tree
  901  sleep 180
  902  ceph osd pool set rbd pg_num 256
  903  ceph status
  904  ceph ls pool
  905  ceph ls
  906  ceph 
  907  ceph -h
  908  ceph status
  909  sleep 180
  910  ceph osd pool set rbd pg_num 256
  911  sleep 180
  912  ceph osd pool set rbd pg_num 256
  913  sudo echo "osd crush chooseleaf type = 0" >> /ect/ceph/ceph.conf
  914  sudo echo "mon_pg_warn_max_per_osd = 0" >> /ect/ceph/ceph.conf
  915  ls
  916  sudo echo "osd crush chooseleaf type = 0" >> /etc/ceph/ceph.conf
  917  sudo echo "mon_pg_warn_max_per_osd = 0" >> /etc/ceph/ceph.conf
  918  ceph status
  919  ceph health detail
  920  ceph status
  921  ceph pg 0.10
  922  ceph pg 0.10 query
  923  ceph status
  924  ceph osd tree 
  925  ceph status
  926  sudo stop ceph-all
  927  sudo start ceph-all
  928  ceph status
  929  ceph osd pool set rbd pg_num 256
  930  ceph osd crush tunables optimal
  931  ceph osd getcrushmap -o /tmp/crushmap
  932  crushtool -d  /tmp/crushmap -o /tmp/crushmap.txt
  933  nano /tmp/crushmap.txt 
  934  crushtool -c /tmp/crushmap.txt -o /tmp/crushmap --enable-unsafe-tunables
  935  ceph osd setcrushmap -i /tmp/crushmap
  936  ceph status
  937  ceph-deploy uninstall node1 node2 master
  938  ceph-deploy purgedata node1 node2 master
  939  ceph-deploy forgetkeys
  940  ceph-deploy purge node1 node2 master
  941  sudo apt-get -y remove ceph-deploy ceph-common ceph-mds ceph
  942  sudo apt-get -y purge ceph-deploy ceph-common ceph-mds ceph
  943  sudo apt-get -y autoremove
  944  sudo rm -r /run/ceph
  945  sudo rm -r /var/lib/ceph
  946  sudo rm /var/log/upstart/ceph*
  947  sudo rm -rf /home/megdc/ceph-cluster/*
  948  sudo rm -rf /storage1/osd/*
  949  sudo rm -rf /storage2/osd/*
  950  sudo cat /etc/
  951  ls
  952  sudo apt-get install -y ceph ceph-mds ceph-deploy radosgw
  953  ls
  954  ceph-deploy new node1
  955  ls
  956  echo "osd_pool_default_size = 2" >> ceph.conf
  957  sudo echo "osd crush chooseleaf type = 0" >> /etc/ceph/ceph.conf
  958  sudo echo "osd crush chooseleaf type = 0" >> ceph.conf
  959  sudo echo "mon_pg_warn_max_per_osd = 0" >> ceph.conf
  960  cat ceph.conf 
  961  ceph-deploy install node1 node2 master
  962  sudo mkdir -p /storage1/osd
  963  sudo mkdir -p /storage2/osd
  964  sudo chown -R ceph:ceph /storage1/osd
  965  sudo chown -R ceph:ceph /storage2/osd
  966  ceph-deploy osd prepare --fs-type ext4 node1:/storage1/osd node1:/storage2/osd 
  967  ls
  968  ceph-deploy mon create-initial
  969  ceph-deploy osd prepare --fs-type ext4 node1:/storage1/osd node1:/storage2/osd 
  970  ceph-deploy osd prepare --fs-type ext4 node2:/storage3/osd node2:/storage4/osd
  971  ceph-deploy osd prepare --fs-type ext4 master:/storage5/osd master:/storage6/osd
  972  ceph-deploy osd activate node1:/storage1/osd node1:/storage2/osd
  973  ceph-deploy osd activate node2:/storage3/osd node2:/storage4/osd
  974  ceph-deploy osd activate master:/storage5/osd master:/storage6/osd
  975  ceph-deploy admin -h
  976  ceph-deploy -h
  977  ceph-deploy admin node1 node2 master
  978  sleep 240
  979  ceph osd pool set rbd pg_num 256
  980  ls
  981  ls /etc/ceph/
  982  sudo cat /etc/ceph/ceph.client.admin.keyring 
  983  ceph osd pool set rbd pg_num 256
  984  cd /storage1/osd/
  985  ls
  986  cat keyring 
  987  sudo cat keyring 
  988  cd ~/ceph-cluster/
  989  ls
  990  sudo chmod +r /etc/ceph/ceph.client.admin.keyring
  991  ceph osd pool set rbd pg_num 256
  992  ceph status 
  993  sleep 180
  994  ceph status 
  995  ceph-deploy -h
  996  ceph osd pool set rbd pg_num 256
  997  sleep 400
  998  ceph osd pool set rbd pg_num 256
  999  ceph status
 1000  ceph osd pool set rbd pg_num 512
 1001  ceph osd pool set rbd pg_num 256
 1002  ceph status
 1003  ceph osd pool set rbd pg_num 150
 1004  ceph status
 1005  ceph osd -h
 1006  nano /etc/ceph/ceph.conf 
 1007  sudo nano /etc/ceph/ceph.conf 
 1008  ceph status
 1009  sudo nano /etc/ceph/ceph.conf 
 1010  ceph -v
 1011  ceph osd tree
 1012  sudo nano /etc/ceph/ceph.conf 
 1013  cat /etc/network/interfaces
 1014  sudo nano /etc/network/interfaces
 1015  brctl show
 1016  sudo /etc/init.d/networking restart
 1017  ceph status
 1018  ifconfig
 1019  brctl delbr one
 1020  sudo brctl delbr one
 1021  brctl show
 1022  brctl delbr one
 1023  exit
 1024  ls
 1025  ceph-deploy uninstall node1 node2 master
 1026  ceph-deploy purgedata node1 node2 master
 1027  ceph-deploy forgetkeys
 1028  ceph-deploy purge node1 node2 master
 1029  sudo apt-get -y remove ceph-deploy ceph-common ceph-mds ceph
 1030  sudo apt-get -y purge ceph-deploy ceph-common ceph-mds ceph
 1031  sudo apt-get -y autoremove
 1032  sudo rm -r /run/ceph
 1033  sudo rm -r /var/lib/ceph
 1034  sudo rm /var/log/upstart/ceph*
 1035  sudo rm -rf /home/megdc/ceph-cluster/*
 1036  sudo rm -rf /storage1/osd/*
 1037  sudo rm -rf /storage2/osd/*
 1038  sudo apt-get update
 1039  sudo apt-get install -y ceph ceph-mds ceph-deploy radosgw
 1040  su megdc
 1041  cat /home/megdc/.ssh/ssh_config 
 1042  ls -la
 1043  ls -la ../
 1044  cat ../.ssh/config 
 1045  ls -la ../.ssh/
 1046  cat ../.ssh/config 
 1047  ls
 1048  ceph-deploy new node1
 1049  ls
 1050  echo "osd_pool_default_size = 2" >> ceph.conf
 1051  sudo echo "osd crush chooseleaf type = 0" >> ceph.conf
 1052  sudo echo "mon_pg_warn_max_per_osd = 0" >> ceph.conf
 1053  ceph-deploy install node1 node2 master
 1054  sudo mkdir -p /storage1/osd
 1055  sudo mkdir -p /storage2/osd
 1056  sudo chown -R ceph:ceph /storage1/osd
 1057  sudo chown -R ceph:ceph /storage2/osd
 1058  ceph-deploy mon create-initial
 1059  ceph-deploy osd prepare node1:/storage1/osd node1:/storage2/osd
 1060  ceph-deploy osd prepare node2:/storage3/osd node2:/storage4/osd
 1061  ceph-deploy osd prepare master:/storage5/osd master:/storage6/osd
 1062  ceph-deploy osd activate node1:/storage1/osd node1:/storage2/osd
 1063  cat ceph.conf 
 1064  ceph status
 1065  nano /etc/ceph/ceph.conf 
 1066  ceph-deploy osd activate node2:/storage3/osd node2:/storage4/osd
 1067  ceph-deploy osd activate master:/storage5/osd master:/storage6/osd
 1068  cat /etc/ceph/ceph.c
 1069  cat /etc/ceph/ceph.conf 
 1070  ceph-deploy osd activate master:/storage5/osd master:/storage6/osd
 1071  ceph-deploy admin node1 node2 master
 1072  sudo chmod +r /etc/ceph/ceph.client.admin.keyring
 1073  sleep 400
 1074  ceph osd pool set rbd pg_num 256
 1075  ceph status
 1076  ceph osd tree
 1077  ceph osd pool set rbd pg_num 150
 1078  ceph-deploy osd activate master:/storage5/osd master:/storage6/osd
 1079  ceph osd tree
 1080  sudo nano /etc/ceph/ceph.conf 
 1081  ceph status
 1082  ceph osd tree
 1083  ls
 1084  cat ceph.conf 
 1085  cp ceph.conf /etc/ceph/ceph.conf 
 1086  sudo cp ceph.conf /etc/ceph/ceph.conf 
 1087  ceph status
 1088  ceph osd tree 
 1089  sudo stop ceph-all
 1090  sudo start ceph-all
 1091  ceph status
 1092  nano /etc/network/interfaces
 1093  cat /etc/hosts
 1094  nano /etc/hosts
 1095  sudo nano /etc/hosts
 1096  sudo /etc/ceph/ceph.conf
 1097  sudo nano /etc/ceph/ceph.conf
 1098  ceph status
 1099  ping localhost
 1100  ping 136.243.49.217
 1101  sudo nano /etc/ceph/ceph.conf
 1102  ceph status
 1103  ceph
 1104  ls -la
 1105  ls /var/run/ceph/
 1106  sudo ls /var/run/ceph/
 1107  ceph ping mon.node1
 1108  ls -la /etc/ceph/ceph.client.admin.keyring
 1109  chmod +x /etc/ceph/ceph.client.admin.keyring
 1110  sudo chmod +x /etc/ceph/ceph.client.admin.keyring
 1111  ceph ping mon.node1
 1112  ceph -w
 1113  ceph-deploy admin node1
 1114  sudo cp /etc/ceph/ceph.conf /root/
 1115  ceph-deploy admin --overwrite-conf node1
 1116  ceph-deploy admin node1 --overwrite-conf 
 1117  ceph-deploy --overwrite-conf  admin node1 
 1118  ceph status
 1119  chmod +x /etc/ceph/ceph.client.admin.keyring
 1120  sudo chmod +x /etc/ceph/ceph.client.admin.keyring
 1121  ceph status
 1122  sudo cat /etc/ceph/ceph.conf 
 1123  ceph osd tree
 1124  ls
 1125  sudo ls -la /etc/ceph/ceph.conf 
 1126  sudo ls -la /etc/ceph/
 1127  cp -r *.keyring /etc/ceph/
 1128  sudo cp -r *.keyring /etc/ceph/
 1129  ceph status
 1130  sudo ls -la /etc/ceph/
 1131  ceph-deploy uninstall node1 node2 master
 1132  ceph-deploy purgedata node1 node2 master
 1133  ceph-deploy forgetkeys
 1134  ceph-deploy purge node1 node2 master
 1135  sudo apt-get -y autoremove
 1136  sudo apt-get -y remove ceph-deploy ceph-common ceph-mds ceph
 1137  sudo apt-get -y purge ceph-deploy ceph-common ceph-mds ceph
 1138  sudo apt-get -y autoremove
 1139  sudo rm -r /run/ceph
 1140  sudo rm -r /var/lib/ceph
 1141  sudo rm /var/log/upstart/ceph*
 1142  sudo rm -rf /home/megdc/ceph-cluster/*
 1143  sudo rm -rf /storage1/osd
 1144  sudo rm -rf /storage2/osd
 1145  megdc 
 1146  megdc cephinstall -h
 1147  megdc cephinstall --osd osd1=storage1 --osd osd2=storage2
 1148  sudo rm -rf /var/lib/urknall/ceph*
 1149  sudo rm -rf /var/lib/urknall/one*
 1150  megdc cephinstall --osd osd1=storage1 --osd osd2=storage2
 1151  rm -rf /home/megdc/.ssh
 1152  megdc cephinstall --osd osd1=storage1 --osd osd2=storage2
 1153  cd ..
 1154  rm -r /home/megdc/ceph-cluster
 1155  megdc cephinstall --osd osd1=storage1 --osd osd2=storage2
 1156  ceph stauts
 1157  ceph status
 1158  ceph -v
 1159  ceph osd tree
 1160  ceph osd in 0 1
 1161  ceph status
 1162  ceph osd out 0 1
 1163  ceph status
 1164  sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
 1165  cd ceph-cluster/
 1166  ls
 1167  cat /etc/hosts
 1168  sudo nano /etc/hosts
 1169  ceph -v
 1170  ceph -s
 1171  nano /etc/ceph/ceph.conf 
 1172  sudo nano /etc/ceph/ceph.conf 
 1173  sudo stop ceph-all
 1174  sudo start ceph-all
 1175  ceph
 1176  ceph status
 1177  cephfs-data-scan tmap_upgrade
 1178  sudo nano /etc/ceph/ceph.conf 
 1179  sudo stop ceph-all
 1180  sudo start ceph-all
 1181  ceph status
 1182  sudo nano /etc/ceph/ceph.conf 
 1183  ceph osd pool set rbd pg_num 150
 1184  ceph osd pool set rbd pg_num 128
 1185  ceph osd pool set rbd pg_num 100
 1186  sudo nano /etc/ceph/ceph.conf 
 1187  ceph osd pool set rbd pg_num 100
 1188  sudo stop ceph-all
 1189  sudo start ceph-all
 1190  ceph status
 1191  ceph osd tree
 1192  ceph pg dump_stuck stale
 1193  ceph pg dump_stuck inactive
 1194  ceph pg dump_stuck unclean
 1195  ceph health detail
 1196  ceph pg 0.39 query
 1197  sudo nano /etc/ceph/ceph.conf 
 1198  sudo start ceph-all 
 1199  ceph health detail
 1200  ceph status
 1201  ceph pg -h
 1202  ceph pg repair 0.39
 1203  ceph
 1204  ceph health detail
 1205  ceph pg repair 0.17
 1206  ceph -2
 1207  ceph -w
 1208  sudo nano /etc/network/interfaces
 1209  sudo nano /etc/ceph/ceph.conf 
 1210  sudo stop ceph-all
 1211  sudo start ceph-all
 1212  ceph status
 1213  cd ../.ssh/
 1214  ls
 1215  ls -la
 1216  sudo nano config 
 1217  cat id_rsa.pub 
 1218  ssh master
 1219  ssh node2
 1220  cd ../ceph-cluster/
 1221  ls
 1222  ceph status
 1223  ceph osd pool set rbd pg_num 100
 1224  ceph status
 1225  sudo nano /etc/ceph/ceph.conf 
 1226  sudo stop ceph-all
 1227  sudo start ceph-all
 1228  ceph status
 1229  ceph osd pool set rbd pg_num 128 --allow-experimental-feature
 1230  ceph osd pool set rbd pg_num 128
 1231  sudo stop ceph-all
 1232  ceph osd pool set rbd pg_num 128
 1233  sudo start ceph-all
 1234  sudo start ceph-osd id=0
 1235  sudo stop ceph-osd id=0
 1236  sudo stop ceph-osd id=1
 1237  ceph osd pool set rbd pg_num 128
 1238  ceph status
 1239  ceph osd lspools
 1240  ceph osd pool delete rbd
 1241  ceph osd pool delete rbd --yes-i-really-really-mean-it
 1242  ceph osd lspools
 1243  ceph osd pool get rbd
 1244  ceph osd pool get rbd size
 1245  ceph osd pool get rbd size pgp_num
 1246  ceph osd pool get rbd pgp_num
 1247  ceph osd pool delete rbd  rbd --yes-i-really-really-mean-it
 1248  ceph status
 1249  sudo start ceph-osd id=0
 1250  sudo start ceph-osd id=1
 1251  ceph status
 1252  sudo stop ceph-osd id=0
 1253  sudo stop ceph-osd id=1
 1254  ceph status
 1255  ceph osd pool create rbd 300 300
 1256  ceph status
 1257  sudo start ceph-osd id=0
 1258  sudo start ceph-osd id=1
 1259  ceph status
 1260  ps -ef | grep osd
 1261  ls
 1262  ceph-deploy uninstall node1 node2 master
 1263  ceph-deploy purgedata node1 node2 master
 1264  ceph-deploy forgetkeys
 1265  ceph-deploy purge node1 node2 master
 1266  nano ceph-setup.sh
 1267  chmod +x ceph-setup.sh 
 1268  sudo apt-get -y autoremove
 1269  sudo apt-get -y remove ceph-deploy ceph-common ceph-mds ceph
 1270  sudo apt-get -y purge ceph-deploy ceph-common ceph-mds ceph
 1271  sudo apt-get -y autoremove
 1272  sudo rm -r /run/ceph
 1273  sudo rm -r /var/lib/ceph
 1274  sudo rm /var/log/upstart/ceph*
 1275  sudo rm -rf /home/megdc/ceph-cluster/*
 1276  sudo rm -rf /storage1/osd
 1277  sudo rm -rf /storage2/osd
 1278  cd ..
 1279  ls
 1280  installceph.sh
 1281  nano installceph.sh
 1282  chmod +x installceph.sh 
 1283  nano installceph.sh
 1284  ./installceph.sh 
 1285  ceph status
 1286  ceph osd tree
 1287  sudo chown -R ceph:ceph /storage1/osd
 1288  sudo chown -R ceph:ceph /storage2/osd
 1289  sudo mkdir -p /storage1/osd
 1290  sudo mkdir -p /storage2/osd
 1291  sudo chown -R ceph:ceph /storage1/osd
 1292  sudo chown -R ceph:ceph /storage2/osd
 1293  ceph-deploy osd prepare --fs-type ext4 node1:/storage1/osd node1:/storage2/osd
 1294  cd ceph-cluster/
 1295  ceph-deploy osd prepare --fs-type ext4 node1:/storage1/osd node1:/storage2/osd
 1296  ceph-deploy osd activate node1:/storage1/osd node1:/storage2/osd
 1297  ceph status
 1298  sleep 150
 1299  ceph osd pool set rbd pg_num 300
 1300  ceph osd pool set rbd pg_num 256
 1301  sleep 150
 1302  ceph osd pool set rbd pgp_num 256
 1303  ceph status
 1304  sleep 300
 1305  ceph status
 1306  nano /etc/ceph/ceph.conf 
 1307  sudo nano /etc/ceph/ceph.conf 
 1308  ceph status
 1309  sudo nano /etc/ceph/ceph.conf 
 1310  ceph status
 1311  ceph osd tree 
 1312  ceph -h
 1313  sudo scp /etc/ceph/*.keyring megdc@node2:/home/megdc/
 1314  sudo scp /etc/ceph/*.keyring megdc@master:/home/megdc/
 1315  sftp root@master
 1316  ceph status
 1317  sudo nano /etc/network/interfaces
 1318  sudo /etc/init.d/networking restart
 1319  ifconfig
 1320  exit
 1321  pwd
 1322  cd ~
 1323  ceph status
 1324  pwd
 1325  ceph -v
 1326  dpkg --get-selection | grep open
 1327  sudo dpkg --get-selection | grep open
 1328  sudo dpkg --get-selections | grep open
 1329  exit
 1330  cd /home/megdc/ceph-cluster/
 1331  ls
 1332  nano secret.xml 
 1333  cat secret.xml 
 1334  nano secret.xml 
 1335  sudo nano secret.xml 
 1336  exit
 1337  cd ~/ceph-cluster/
 1338  ls
 1339  sudo nano /etc/apache2/apache2.conf
 1340  sudo a2enmod rewrite
 1341  sudo a2enmod fastcgi
 1342  sudo service apache2 start
 1343  sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.radosgw.keyring
 1344  sudo chmod +r /etc/ceph/ceph.client.radosgw.keyring
 1345  sudo ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.admin --gen-key
 1346  sudo ceph-authtool -n client.radosgw.admin --cap osd 'allow rwx' --cap mon 'allow rwx' /etc/ceph/ceph.client.radosgw.keyring
 1347  sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.admin -i /etc/ceph/ceph.client.radosgw.keyring
 1348  sudo scp /etc/ceph/ceph.client.admin.keyring  megdc@slave:/home/megdc
 1349  ssh megdc@slave 'sudo mv ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring'
 1350  cat ../.ssh/id_rsa.pub 
 1351  ceph osd pool create .rgw 32 32
 1352  ceph osd pool create .rgw.root 32 32
 1353  ceph osd pool create .rgw.control 32 32
 1354  ceph osd pool create .rgw.gc 32 32
 1355  ceph osd pool create .rgw.buckets 32 32
 1356  ceph osd pool create .rgw.buckets.index 32 32
 1357  ceph osd pool create .log 32 32
 1358  ceph osd pool create .intent-log 32 32
 1359  ceph osd pool create .usage 32 32
 1360  ceph osd pool create .users 32 32
 1361  ceph osd pool create .users.email 32 32
 1362  ceph osd pool create .users.swift 32 32
 1363  ceph osd pool create .users.uid 32 32
 1364  cd /var/lib/one
 1365  ls
 1366  cd ~/ceph-cluster/
 1367  rados lspools
 1368  ceph status
 1369  sudo nano /etc/ceph/ceph.conf 
 1370  ceph-deploy --overwrite-conf config pull slave
 1371  ceph-deploy --overwrite-conf config push node1 node2 master
 1372  cat /etc/ceph/ceph.conf 
 1373  ceph status
 1374  ls
 1375  cat ceph.conf 
 1376  ifconfig
 1377  sudo nano ceph.conf 
 1378  cat /storage2/osd/ceph_fsid 
 1379  sudo nano ceph.conf 
 1380  sudo cp /etc/ceph/ceph.conf /root/
 1381  sudo cp ceph.conf /etc/ceph/ceph.conf
 1382  ceph status
 1383  ceph-deploy --overwrite-conf config push node2 master
 1384  ceph status
 1385  df -h
 1386  sudo scp /etc/ceph/ceph.client.admin.keyring  megdc@slave:/home/megdc
 1387  ssh megdc@slave 'sudo mv ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring'
 1388  sudo scp /etc/ceph/ceph.conf megdc@slave:/home/megdc
 1389  ssh megdc@slave 'sudo mv ceph.conf /etc/ceph/ceph.conf'
 1390  ceph status
 1391  ceph-deploy new -h
 1392  ls -la
 1393  df -h
 1394  cd /var/lib/one/
 1395  ls
 1396  cd datastores/
 1397  ls
 1398  cd 0/
 1399  ls
 1400  ls -la
 1401  cd 332/
 1402  ls
 1403  ls -la
 1404  ceph status
 1405  cd ~
 1406  ceph status
 1407  df -h
 1408  ls
 1409  ceph-deploy purge slave
 1410  ceph-deploy purgedata slave
 1411  ceph auth del client.radosgw.admin
 1412  ceph-deploy install -h
 1413  ceph-deploy install --rgw slave
 1414  sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.radosgw.keyring
 1415  sudo chmod +r /etc/ceph/ceph.client.radosgw.keyring
 1416  sudo ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.admin --gen-key
 1417  sudo ceph-authtool -n client.radosgw.admin --cap osd 'allow rwx' --cap mon 'allow rwx' /etc/ceph/ceph.client.radosgw.keyring
 1418  sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.admin -i /etc/ceph/ceph.client.radosgw.keyring
 1419  sudo scp /etc/ceph/ceph.client.radosgw.keyring  megdc@slave:/home/megdc/
 1420  ssh megdc@slave 'sudo mv ceph.client.radosgw.keyring /etc/ceph/ceph.client.radosgw.keyring'
 1421  cat /etc/ceph/ceph.conf 
 1422  ceph-deploy --overwrite-conf config pull slave
 1423  sudo scp /etc/ceph/ceph.conf  megdc@slave:/home/megdc/
 1424  ssh megdc@slave 'sudo mv ceph.conf /etc/ceph/ceph.conf'
 1425  ceph-deploy --overwrite-conf config pull slave
 1426  sudo scp /etc/ceph/ceph.client.admin.keyring  megdc@slave:/home/megdc
 1427  ssh megdc@slave 'sudo mv ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring'
 1428  ceph-deploy config push node2 master
 1429  ceph-deploy purge slave
 1430  ceph-deploy purgedata slave
 1431  sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.radosgw.keyring
 1432  sudo chmod +r /etc/ceph/ceph.client.radosgw.keyring
 1433  sudo ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.admin --gen-key
 1434  sudo ceph-authtool -n client.radosgw.admin --cap osd 'allow rwx' --cap mon 'allow rwx' /etc/ceph/ceph.client.radosgw.keyring
 1435  sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.admin -i /etc/ceph/ceph.client.radosgw.keyring
 1436  ceph auth del client.radosgw.admin
 1437  sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.admin -i /etc/ceph/ceph.client.radosgw.keyring
 1438  sudo scp /etc/ceph/ceph.client.radosgw.keyring  megdc@slave:/home/megdc/
 1439  ssh megdc@slave 'sudo mv ceph.client.radosgw.keyring /etc/ceph/ceph.client.radosgw.keyring'
 1440  rados lspools
 1441  cat /etc/ceph/ceph.conf 
 1442  ceph-deploy --overwrite-conf config pull slave
 1443  sudo scp /etc/ceph/ceph.conf  megdc@slave:/home/megdc/
 1444  ssh megdc@slave 'sudo mv ceph.conf /etc/ceph/ceph.conf'
 1445  ceph-deploy --overwrite-conf config push node2 master
 1446  sudo scp /etc/ceph/ceph.client.admin.keyring  megdc@slave:/home/megdc
 1447  ssh megdc@slave 'sudo mv ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring'
 1448  ceph-deploy purge slave
 1449  ceph-deploy purgedata slave
 1450  ceph auth del client.radosgw.admin
 1451  ceph-deploy install --rgw slave
 1452  sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.radosgw.keyring
 1453  sudo chmod +r /etc/ceph/ceph.client.radosgw.keyring
 1454  sudo ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.admin --gen-key
 1455  sudo ceph-authtool -n client.radosgw.admin --cap osd 'allow rwx' --cap mon 'allow rwx' /etc/ceph/ceph.client.radosgw.keyring
 1456  sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.admin -i /etc/ceph/ceph.client.radosgw.keyring
 1457  sudo scp /etc/ceph/ceph.client.radosgw.keyring  megdc@slave:/home/megdc/
 1458  ssh megdc@slave 'sudo mv ceph.client.radosgw.keyring /etc/ceph/ceph.client.radosgw.keyring'
 1459  nano /etc/ceph/ceph.conf
 1460  sudo scp /etc/ceph/ceph.conf  megdc@slave:/home/megdc/
 1461  ssh megdc@slave 'sudo mv ceph.conf /etc/ceph/ceph.conf'
 1462  ceph-deploy --overwrite-conf config pull slave
 1463  ceph-deploy --overwrite-conf config push node2 master
 1464  sudo scp /etc/ceph/ceph.client.admin.keyring  megdc@slave:/home/megdc
 1465  ssh megdc@slave 'sudo mv ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring'
 1466  sudo apt-get install apache2 libapache2-mod-fastcgi
 1467  radosgw-admin bucket list
 1468  nano /etc/ceph/ceph.conf 
 1469  sudo nano /etc/ceph/ceph.conf 
 1470  ceph status
 1471  ceph osd pool create .rgw.meta 32 32
 1472  sudo stop ceph-all
 1473  sudo start ceph-all
 1474  ceph status
 1475  nano /etc/ceph/ceph.conf 
 1476  sudo nano /etc/ceph/ceph.conf 
 1477  ceph status
 1478  hostname -f 
 1479  ceph-deploy purge slave
 1480  ceph-deploy purgedata slave
 1481  ceph auth del client.radosgw.admin
 1482  ceph status
 1483  sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.radosgw.keyring
 1484  sudo chmod +r /etc/ceph/ceph.client.radosgw.keyring
 1485  sudo ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.admin --gen-key
 1486  sudo ceph-authtool -n client.radosgw.admin --cap osd 'allow rwx' --cap mon 'allow rwx' /etc/ceph/ceph.client.radosgw.keyring
 1487  sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.admin -i /etc/ceph/ceph.client.radosgw.keyring
 1488  sudo scp /etc/ceph/ceph.client.radosgw.keyring  megdc@slave:/home/megdc/
 1489  ssh megdc@slave 'sudo mv ceph.client.radosgw.keyring /etc/ceph/ceph.client.radosgw.keyring'
 1490  rados lspools
 1491  nano /etc/ceph/ceph.conf
 1492  ceph-deploy --overwrite-conf config pull slave
 1493  sudo scp /etc/ceph/ceph.conf  megdc@slave:/home/megdc/
 1494  ssh megdc@slave 'sudo mv ceph.conf /etc/ceph/ceph.conf'
 1495  ceph-deploy --overwrite-conf config pull slave
 1496  ls
 1497  cat ceph.conf 
 1498  ceph-deploy --overwrite-conf config push node1 node2 master
 1499  ceph status
 1500  sudo scp /etc/ceph/ceph.client.admin.keyring  megdc@slave:/home/megdc
 1501  ssh megdc@slave 'sudo mv ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring'
 1502  ceph status
 1503  cat ceph.conf 
 1504  nano /etc/ceph/ceph.conf 
 1505  sudo nano /etc/ceph/ceph.conf 
 1506  ceph-deploy purge slave
 1507  ceph-deploy purgedata slave
 1508  ceph auth del client.radosgw.admin
 1509  ceph-deploy install --rgw slave
 1510  ceph-deploy rgw create slave
 1511  cat /etc/ceph/ceph.client.radosgw.keyring
 1512  sudo scp /etc/ceph/ceph.client.radosgw.keyring  megdc@slave:/home/megdc/
 1513  ssh megdc@slave 'sudo mv ceph.client.radosgw.keyring /etc/ceph/ceph.client.radosgw.keyring'
 1514  sudo scp /etc/ceph/ceph.client.admin.keyring  megdc@slave:/home/megdc
 1515  ssh megdc@slave 'sudo mv ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring'
 1516  ceph-auth list
 1517  ceph auth list
 1518  ls
 1519  ls -la  /etc/ceph/
 1520  cat /etc/ceph/ceph.client.radosgw.keyring 
 1521  cat /etc/ceph/ceph.client.admin.keyring 
 1522  cat /etc/ceph/ceph.conf 
 1523  ssh slave
 1524  rados lspools
 1525  nano /etc/ceph/ceph.conf 
 1526  sudo nano /etc/ceph/ceph.conf 
 1527  sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.admin -i /etc/ceph/ceph.client.radosgw.keyring
 1528  sudo scp /etc/ceph/ceph.client.radosgw.keyring  megdc@slave:/home/megdc/
 1529  ssh megdc@slave 'sudo mv ceph.client.radosgw.keyring /etc/ceph/ceph.client.radosgw.keyring'
 1530  cat /etc/ceph/ceph.client.radosgw.keyring 
 1531  ceph status
 1532  sudo stop ceph-all
 1533  sudo start ceph-all
 1534  ceph status
 1535  ls
 1536  sudo scp /etc/ceph/*.keyring  megdc@node1:/home/megdc/
 1537  sudo scp /etc/ceph/*.keyring  megdc@node2:/home/megdc/
 1538  sudo scp /etc/ceph/*.keyring  megdc@master:/home/megdc/
 1539  sftp root@master
 1540  ssh node2 'mv *.keyring /etc/ceph/'
 1541  ssh node2 'sudo mv *.keyring /etc/ceph/'
 1542  rbd lspool
 1543  rbd one lspool
 1544  rbd ls one
 1545  ceph status
 1546  rbd ls rbd
 1547  nano /etc/ceph/ceph.conf 
 1548  sudo nano /etc/ceph/ceph.conf 
 1549  nano ceph.conf 
 1550  ceph-deploy --overwrite-conf config pull slave
 1551  cat ceph.conf 
 1552  cat /etc/ceph/ceph.conf 
 1553  nano ceph.conf 
 1554  ceph-deploy --overwrite-conf config node2 master
 1555  ceph-deploy --overwrite-conf config push node2 master
 1556  ceph status
 1557  sudo stop ceph-all
 1558  sudo start ceph-all
 1559  ceph status
 1560  sudo cd /var/log/ceph/
 1561  cd /var/log/ceph/
 1562  exit
 1563  cd /var/log/ceph/
 1564  ls
 1565  tail -100 ceph.log
 1566  ls
 1567  ceph status
 1568  nano /etc/ceph/ceph.conf 
 1569  ceph-deploy --overwrite-conf config push node1 node2 master
 1570  ceph status
 1571  cat /etc/ceph/ceph.conf 
 1572  nano ceph.conf 
 1573  ceph-deploy --overwrite-conf config push node2 master
 1574  cat /etc/ceph/ceph.conf 
 1575  ceph statsu
 1576  ceph status
 1577  cat ../.cephdeploy.conf
 1578  sudo nano /etc/ceph/ceph.conf 
 1579  ceph status
 1580  sudo stop ceph-all
 1581  sudo start ceph-all
 1582  ceph -w
 1583  ceph osd in 0
 1584  ceph osd in 1 2 3
 1585  ceph status
 1586  sudo start ceph-osd id=0
 1587  sudo start ceph-osd id=1
 1588  sudo start ceph-osd id=2
 1589  sudo start ceph-osd id=3
 1590  ceph status
 1591  ceph osd tree
 1592  sudo ce /var/log/ceph/
 1593  sudo cd /var/log/ceph/
 1594  sudo -s
 1595  cd /home/megdc/
 1596  ls
 1597  ./installceph.sh 
 1598  cd /storage2
 1599  ls
 1600  ls -ls
 1601  cd ../
 1602  cd storage1
 1603  ls -la
 1604  mkdir /storage2/osd
 1605  cd ../
 1606  ls
 1607  cd storage2
 1608  mkdir osd
 1609  sudo mkdir osd
 1610  ls
 1611  ls -la
 1612  chown -R ceph:ceph osd
 1613  sudo chown -R ceph:ceph osd
 1614  ls -ls
 1615  ceph status
 1616  ceph-deploy uninstall node1
 1617  exit
 1618  cd /home/megdc/
 1619  ./installceph.sh 
 1620  cd /storage1
 1621  ls
 1622  ceph status
 1623  exit
 1624  ls
 1625  pwd
 1626  cat ceph-may24 
 1627  lsblk
 1628  cat ceph-may24 
 1629  exit
 1630  cd ~
 1631  ls
 1632  cd ceph-cluster/
 1633  ls
 1634  rbd ls 
 1635  man rbd
 1636  ceph status
 1637  ceph osd pool set rbd pg_num 256
 1638  ceph osd pool set rbd pg_num 128
 1639  ceph status
 1640  ceph health detail
 1641  ceph pg 0.68 query
 1642  ceph pg repair 0.68 
 1643  ceph status
 1644  ceph health detail
 1645  ceph status
 1646  cd /storage1/
 1647  ls
 1648  ls -la
 1649  cd osd/
 1650  ls
 1651  df ./
 1652  ceph-deploy disk zap -h
 1653  lsblk
 1654  sudo ls /dev/s*
 1655  cat /etc/os-release 
 1656  cd ~
 1657  ls
 1658  lsblk
 1659  ceph-deploy
 1660  umount /storage1
 1661  sudo umount /storage1
 1662  sudo umount /storage2
 1663  lsblk
 1664  ls
 1665  cd ceph-cluster/
 1666  ls
 1667  cd ..
 1668  script ceph-may24
 1669  cat ceph-may24 
 1670  clear
 1671  cat ceph-may24 
 1672  exit
 1673  lsblk
 1674  sudo apt-get update
 1675  sudo apt-get install -y ceph ceph-mds ceph-deploy radosgw dnsmasq openssh-server ntp sshpass
 1676  cd ceph-cluster/
 1677  ls
 1678  cat ../.ssh/config 
 1679  lsblk
 1680  ceph-deploy disk zap node1:sdc node2:sdd
 1681  ceph-deploy new megamadhi
 1682  ceph-deploy new node1
 1683  ceph-deploy mon create-initial
 1684  cat ceph.conf
 1685  echo "osd_pool_default_size = 2" >> ceph.conf
 1686  echo "osd crush chooseleaf type = 0" >> ceph.conf
 1687  echo "mon_pg_warn_max_per_osd = 0" >> ceph.conf
 1688  ceph-deploy disk zap node1:sdc node1:sdd
 1689  lsblk
 1690  ceph-deploy osd prepare node1:sdc node1:sdd
 1691  ls
 1692  cat /etc/ceph/ceph.conf 
 1693  lsblk
 1694  ceph-deploy disk zap node1:sdc node1:sdd
 1695  ls
 1696  lsblk
 1697  ceph-deploy disk zap node1:sdd
 1698  lsblk
 1699  sudo fdisk -l
 1700  cat /etc/fstab
 1701  sudo blkid
 1702  lsbk
 1703  lsblk
 1704  ls
 1705  cat ceph.conf 
 1706  nano /etc/ceph/ceph.con
 1707  nano /etc/ceph/ceph.conf
 1708  sudo nano /etc/ceph/ceph.conf
 1709  ceph-deploy osd prepare node1:sdc node1:sdd
 1710  ceph-deploy osd prepare --overwrite-conf node1:sdc node1:sdd
 1711  ceph-deploy osd prepare node1:sdc node1:sdd --overwrite-conf
 1712  ceph-deploy -h
 1713  ceph-deploy --overwrite-conf osd  prepare node1:sdc node1:sdd
 1714  lsblk
 1715  cat /etc/ceph/ceph.conf 
 1716  cat ceph.conf 
 1717  /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd
 1718  sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd
 1719  blkid
 1720  lsblk
 1721  umount /dev/sdd
 1722  cat /etc/mtab 
 1723  umount /var/lib/ceph/osd/ceph-0
 1724  sudo umount /var/lib/ceph/osd/ceph-0
 1725  ls -la /var/lib/ceph/osd/ceph-0
 1726  exit
 1727  cd ~
 1728  ls
 1729  pwd
 1730  script ceph-may24
 1731  lsblk
 1732  fdisk /dev/sdc
 1733  exit
 1734  cd ceph-cluster/
 1735  sudo apt-get update
 1736  sudo apt-get install -y ceph ceph-mds ceph-deploy radosgw dnsmasq openssh-server ntp sshpass
 1737  ls
 1738  ceph-deploy new node1
 1739  ls
 1740  cat ceph.conf 
 1741  echo "osd_pool_default_size = 2" >> ceph.conf
 1742  ceph-deploy disk zap node1:sdc
 1743  lsblk
 1744  nano /etc/fstab
 1745  ceph-deploy disk list node1
 1746  lsblk
 1747  umount /storage1 
 1748  sudo nano /etc/mtab
 1749  cat /etc/mtab.bk 
 1750  sudo nano /etc/fstab
 1751  lsblk
 1752  mount
 1753  blkid
 1754  ls
 1755  lsblk
 1756  umount /storage1 
 1757  nano /etc/mtab.bk 
 1758  history
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ceph-deploy disk zap -h
usage: ceph-deploy disk zap [-h] HOST:DISK [HOST:DISK ...]

positional arguments:
  HOST:DISK   host and disk

optional arguments:
  -h, --help  show this help message and exit
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda       8:0    0 223.6G  0 disk  
├─sda1    8:1    0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sda2    8:2    0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sda3    8:3    0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdb       8:16   0 223.6G  0 disk  
├─sdb1    8:17   0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sdb2    8:18   0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sdb3    8:19   0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdc       8:32   0   5.5T  0 disk  
└─sdc1    8:33   0   5.5T  0 part  /storage1
sdd       8:48   0   5.5T  0 disk  
├─sdd1    8:49   0   5.5T  0 part  
└─sdd2    8:50   0     5G  0 part  
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ 
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ lsblkceph-deploy disk zap -hhistory[Knano /etc/mtab.bk [1Pumount /storage1[Clsblk[K[Kblkumount /storage1 
umount: /storage1 is not mounted (according to mtab)
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ cp /etc/mtab.bk  [1P[1P[1P[1@c[1@a[1@t[1@ 
/dev/md2 / ext4 rw 0 0
proc /proc proc rw 0 0
sysfs /sys sysfs rw,noexec,nosuid,nodev 0 0
none /sys/fs/cgroup tmpfs rw 0 0
none /sys/fs/fuse/connections fusectl rw 0 0
none /sys/kernel/debug debugfs rw 0 0
none /sys/kernel/security securityfs rw 0 0
udev /dev devtmpfs rw,mode=0755 0 0
devpts /dev/pts devpts rw,noexec,nosuid,gid=5,mode=0620 0 0
tmpfs /run tmpfs rw,noexec,nosuid,size=10%,mode=0755 0 0
none /run/lock tmpfs rw,noexec,nosuid,nodev,size=5242880 0 0
none /run/shm tmpfs rw,nosuid,nodev 0 0
none /run/user tmpfs rw,noexec,nosuid,nodev,size=104857600,mode=0755 0 0
none /sys/fs/pstore pstore rw 0 0
tracefs /var/lib/ureadahead/debugfs/tracing tracefs rw,relatime 0 0
/dev/md0 /boot ext3 rw 0 0
systemd /sys/fs/cgroup/systemd cgroup rw,noexec,nosuid,nodev,none,name=systemd 0 0
/dev/sdd1 /var/lib/ceph/osd/ceph-0 xfs rw,noatime,inode64 0 0
/dev/sdc1 /var/lib/ceph/osd/ceph-1 xfs rw,noatime,inode64 0 0
/dev/sdc1 /storage1 xfs rw 0 0
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ cat /etc/e[Kmetab
cat: /etc/metab: No such file or directory
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ cat /etc/metab[K[K[K[K[Kmtab
/dev/md2 / ext4 rw 0 0
proc /proc proc rw 0 0
sysfs /sys sysfs rw,noexec,nosuid,nodev 0 0
none /sys/fs/cgroup tmpfs rw 0 0
none /sys/fs/fuse/connections fusectl rw 0 0
none /sys/kernel/debug debugfs rw 0 0
none /sys/kernel/security securityfs rw 0 0
udev /dev devtmpfs rw,mode=0755 0 0
devpts /dev/pts devpts rw,noexec,nosuid,gid=5,mode=0620 0 0
tmpfs /run tmpfs rw,noexec,nosuid,size=10%,mode=0755 0 0
none /run/lock tmpfs rw,noexec,nosuid,nodev,size=5242880 0 0
none /run/shm tmpfs rw,nosuid,nodev 0 0
none /run/user tmpfs rw,noexec,nosuid,nodev,size=104857600,mode=0755 0 0
none /sys/fs/pstore pstore rw 0 0
tracefs /var/lib/ureadahead/debugfs/tracing tracefs rw,relatime 0 0
/dev/md0 /boot ext3 rw 0 0
systemd /sys/fs/cgroup/systemd cgroup rw,noexec,nosuid,nodev,none,name=systemd 0 0
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ cat /etc/mtabetab[1Ptab[1P[1P[1P /etc/mtabs /etc/mtabu /etc/mtabd /etc/mtabo /etc/mtab[C /etc/mtabn /etc/mtaba /etc/mtabn /etc/mtabo /etc/mtab
[?1049h[1;39r(B[m[4l[?7h[?12l[?25h[?1h=[?1h=[?1h=[39;49m[39;49m(B[m[H[2J(B[0;7m  GNU nano 2.2.6                                   File: /etc/mtab                                                                             [3;1H(B[m/dev/md2 / ext4 rw 0 0[4dproc /proc proc rw 0 0[5dsysfs /sys sysfs rw,noexec,nosuid,nodev 0 0[6dnone /sys/fs/cgroup tmpfs rw 0 0[7dnone /sys/fs/fuse/connections fusectl rw 0 0[8dnone /sys/kernel/debug debugfs rw 0 0[9dnone /sys/kernel/security securityfs rw 0 0[10dudev /dev devtmpfs rw,mode=0755 0 0[11ddevpts /dev/pts devpts rw,noexec,nosuid,gid=5,mode=0620 0 0[12dtmpfs /run tmpfs rw,noexec,nosuid,size=10%,mode=0755 0 0[13dnone /run/lock tmpfs rw,noexec,nosuid,nodev,size=5242880 0 0[14dnone /run/shm tmpfs rw,nosuid,nodev 0 0[15dnone /run/user tmpfs rw,noexec,nosuid,nodev,size=104857600,mode=0755 0 0[16dnone /sys/fs/pstore pstore rw 0 0[17dtracefs /var/lib/ureadahead/debugfs/tracing tracefs rw,relatime 0 0[18d/dev/md0 /boot ext3 rw 0 0[19dsystemd /sys/fs/cgroup/systemd cgroup rw,noexec,nosuid,nodev,none,name=systemd 0 0[37;64H(B[0;7m[ Read 17 lines ][38d^G(B[m Get Help[38;24H(B[0;7m^O(B[m WriteOut[38;47H(B[0;7m^R(B[m Read File[38;70H(B[0;7m^Y(B[m Prev Page[38;93H(B[0;7m^K(B[m Cut Text[38;116H(B[0;7m^C(B[m Cur Pos[39d(B[0;7m^X(B[m Exit[39;24H(B[0;7m^J(B[m Justify[39;47H(B[0;7m^W(B[m Where Is[39;70H(B[0;7m^V(B[m Next Page[39;93H(B[0;7m^U(B[m UnCut Text[39;116H(B[0;7m^T(B[m To Spell[3d[4d[5d[6d[7d[8d[9d[10d[11d[12d[13d[14d[15d[16d[17d[18d[19d[20d[1;134H(B[0;7mModified[20d(B[m/dev/sdd1 /var/lib/ceph/osd/ceph-0 xfs rw,noatime,inode64 0 0[21d/dev/sdc1 /var/lib/ceph/osd/ceph-1 xfs rw,noatime,inode64 0 0[22d/dev/sdc1 /storage1 xfs rw 0 0[37d[K[23d[37d(B[0;7mFile Name to Write: /etc/mtab                                                                                                                  [38;24H(B[m            (B[0;7mM-D(B[m DOS Format         [38;70H (B[0;7mM-A(B[m Append [38;93H             (B[0;7mM-B(B[m Backup File[K[39;2H(B[0;7mC(B[m Cancel[39;24H            (B[0;7mM-M(B[m Mac Format        [39;70H (B[0;7mM-P(B[m Prepend[K[37;30H[1;134H[39;49m(B[0;7m        [37;62H(B[m[1K (B[0;7m[ Wrote 21 lines ](B[m[K[38;24H(B[0;7m^O(B[m WriteOut            (B[0;7m^R(B[m Read File[38;70H(B[0;7m^Y(B[m Prev Page[38;93H(B[0;7m^K(B[m Cut Text            (B[0;7m^C(B[m Cur Pos[39;2H(B[0;7mX(B[m Exit  [39;24H(B[0;7m^J(B[m Justify             (B[0;7m^W(B[m Where Is[39;70H(B[0;7m^V(B[m Next Page[39;93H(B[0;7m^U(B[m UnCut Text[39;116H(B[0;7m^T(B[m To Spell[23d[38d[J[39;143H[39;1H[?1049l[?1l>]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ sudo nano /etc/mtab[6Pcat[C[C[C[C[C[C[C[C[C[C[6@sudo nano[C[C[C[C[C[C[C[C[C[C
[?1049h[1;39r(B[m[4l[?7h[?12l[?25h[?1h=[?1h=[?1h=[39;49m[39;49m(B[m[H[2J(B[0;7m  GNU nano 2.2.6                                   File: /etc/mtab                                                                             [3;1H(B[m/dev/md2 / ext4 rw 0 0[4dproc /proc proc rw 0 0[5dsysfs /sys sysfs rw,noexec,nosuid,nodev 0 0[6dnone /sys/fs/cgroup tmpfs rw 0 0[7dnone /sys/fs/fuse/connections fusectl rw 0 0[8dnone /sys/kernel/debug debugfs rw 0 0[9dnone /sys/kernel/security securityfs rw 0 0[10dudev /dev devtmpfs rw,mode=0755 0 0[11ddevpts /dev/pts devpts rw,noexec,nosuid,gid=5,mode=0620 0 0[12dtmpfs /run tmpfs rw,noexec,nosuid,size=10%,mode=0755 0 0[13dnone /run/lock tmpfs rw,noexec,nosuid,nodev,size=5242880 0 0[14dnone /run/shm tmpfs rw,nosuid,nodev 0 0[15dnone /run/user tmpfs rw,noexec,nosuid,nodev,size=104857600,mode=0755 0 0[16dnone /sys/fs/pstore pstore rw 0 0[17dtracefs /var/lib/ureadahead/debugfs/tracing tracefs rw,relatime 0 0[18d/dev/md0 /boot ext3 rw 0 0[19dsystemd /sys/fs/cgroup/systemd cgroup rw,noexec,nosuid,nodev,none,name=systemd 0 0[20d/dev/sdd1 /var/lib/ceph/osd/ceph-0 xfs rw,noatime,inode64 0 0[21d/dev/sdc1 /var/lib/ceph/osd/ceph-1 xfs rw,noatime,inode64 0 0[22d/dev/sdc1 /storage1 xfs rw 0 0[37;64H(B[0;7m[ Read 21 lines ][38d^G(B[m Get Help[38;24H(B[0;7m^O(B[m WriteOut[38;47H(B[0;7m^R(B[m Read File[38;70H(B[0;7m^Y(B[m Prev Page[38;93H(B[0;7m^K(B[m Cut Text[38;116H(B[0;7m^C(B[m Cur Pos[39d(B[0;7m^X(B[m Exit[39;24H(B[0;7m^J(B[m Justify[39;47H(B[0;7m^W(B[m Where Is[39;70H(B[0;7m^V(B[m Next Page[39;93H(B[0;7m^U(B[m UnCut Text[39;116H(B[0;7m^T(B[m To Spell[3d[4d[5d[6d[7d[8d[9d[10d[11d[12d[13d[14d[15d[16d[17d[18d[19d[20d[20;36r[36;1H
[1;39r[1;134H(B[0;7mModified[20d(B[m[20;36r[36;1H
[1;39r[20;1H[37d(B[0;7mFile Name to Write: /etc/mtab                                                                                                                  [38;24H(B[m            (B[0;7mM-D(B[m DOS Format         [38;70H (B[0;7mM-A(B[m Append [38;93H             (B[0;7mM-B(B[m Backup File[K[39;2H(B[0;7mC(B[m Cancel[39;24H            (B[0;7mM-M(B[m Mac Format        [39;70H (B[0;7mM-P(B[m Prepend[K[37;30H[1;134H[39;49m(B[0;7m        [37;62H(B[m[1K (B[0;7m[ Wrote 19 lines ](B[m[K[38;24H(B[0;7m^O(B[m WriteOut            (B[0;7m^R(B[m Read File[38;70H(B[0;7m^Y(B[m Prev Page[38;93H(B[0;7m^K(B[m Cut Text            (B[0;7m^C(B[m Cur Pos[39;2H(B[0;7mX(B[m Exit  [39;24H(B[0;7m^J(B[m Justify             (B[0;7m^W(B[m Where Is[39;70H(B[0;7m^V(B[m Next Page[39;93H(B[0;7m^U(B[m UnCut Text[39;116H(B[0;7m^T(B[m To Spell[20d[38d[J[39;143H[39;1H[?1049l[?1l>]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ umon[Kunt s[K/storage1 
umount: /storage1 is not in the fstab (and you are not root)
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ umount /storage1 [1@s[C[1@u[1@d[1@o[1@ 
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ l[Klsblk
NAME    MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda       8:0    0 223.6G  0 disk  
├─sda1    8:1    0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sda2    8:2    0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sda3    8:3    0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdb       8:16   0 223.6G  0 disk  
├─sdb1    8:17   0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sdb2    8:18   0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sdb3    8:19   0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdc       8:32   0   5.5T  0 disk  
└─sdc1    8:33   0   5.5T  0 part  
sdd       8:48   0   5.5T  0 disk  
├─sdd1    8:49   0   5.5T  0 part  
└─sdd2    8:50   0     5G  0 part  
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ceph-deploy [K[K[K[K[K[K[K[K[K[K[K[Kceph-deploy disk zap node1:sdc
[[1mceph_deploy.conf[0m][[1;34mDEBUG[0m ] found configuration file at: /home/megdc/.cephdeploy.conf
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] Invoked (1.5.33): /usr/bin/ceph-deploy disk zap node1:sdc
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] ceph-deploy options:
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  username                      : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  verbose                       : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  overwrite_conf                : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  subcommand                    : zap
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  quiet                         : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7c22d1e5a8>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cluster                       : ceph
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  func                          : <function disk at 0x7f7c231835f0>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  ceph_conf                     : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  default_release               : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  disk                          : [('node1', '/dev/sdc', None)]
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] zapping /dev/sdc on node1
[[1mnode1[0m][[1;34mDEBUG[0m ] connection detected need for sudo
[[1mnode1[0m][[1;34mDEBUG[0m ] connected to host: node1 
[[1mnode1[0m][[1;34mDEBUG[0m ] detect platform information from remote host
[[1mnode1[0m][[1;34mDEBUG[0m ] detect machine type
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/initctl version
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mceph_deploy.osd[0m][[1;37mINFO[0m  ] Distro info: Ubuntu 14.04 trusty
[[1mnode1[0m][[1;34mDEBUG[0m ] zeroing last few blocks of device
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdc
[[1mnode1[0m][[1;33mWARNIN[0m] Caution: invalid backup GPT header, but valid main header; regenerating
[[1mnode1[0m][[1;33mWARNIN[0m] backup header from main header.
[[1mnode1[0m][[1;33mWARNIN[0m] 
[[1mnode1[0m][[1;34mDEBUG[0m ] ****************************************************************************
[[1mnode1[0m][[1;34mDEBUG[0m ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[[1mnode1[0m][[1;34mDEBUG[0m ] verification and recovery are STRONGLY recommended.
[[1mnode1[0m][[1;34mDEBUG[0m ] ****************************************************************************
[[1mnode1[0m][[1;34mDEBUG[0m ] GPT data structures destroyed! You may now partition the disk using fdisk or
[[1mnode1[0m][[1;34mDEBUG[0m ] other utilities.
[[1mnode1[0m][[1;34mDEBUG[0m ] Creating new GPT entries.
[[1mnode1[0m][[1;34mDEBUG[0m ] The operation has completed successfully.
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] Calling partprobe on zapped device /dev/sdc
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/partprobe /dev/sdc
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda       8:0    0 223.6G  0 disk  
├─sda1    8:1    0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sda2    8:2    0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sda3    8:3    0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdb       8:16   0 223.6G  0 disk  
├─sdb1    8:17   0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sdb2    8:18   0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sdb3    8:19   0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdc       8:32   0   5.5T  0 disk  
sdd       8:48   0   5.5T  0 disk  
├─sdd1    8:49   0   5.5T  0 part  
└─sdd2    8:50   0     5G  0 part  
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ lsblkceph-deploy disk zap node1:sdc^C
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ceph-deploy osd prepare node1:sdc 
[[1mceph_deploy.conf[0m][[1;34mDEBUG[0m ] found configuration file at: /home/megdc/.cephdeploy.conf
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] Invoked (1.5.33): /usr/bin/ceph-deploy osd prepare node1:sdc
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] ceph-deploy options:
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  username                      : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  disk                          : [('node1', '/dev/sdc', None)]
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  dmcrypt                       : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  verbose                       : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  bluestore                     : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  overwrite_conf                : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  subcommand                    : prepare
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  quiet                         : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f1676e43f80>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cluster                       : ceph
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  fs_type                       : xfs
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  func                          : <function osd at 0x7f16772a5578>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  ceph_conf                     : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  default_release               : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  zap_disk                      : False
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] Preparing cluster ceph disks node1:/dev/sdc:
[[1mceph_deploy[0m][[1;31mERROR[0m ] RuntimeError: bootstrap-osd keyring not found; run 'gatherkeys'

]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ls
ceph.conf  ceph-deploy-ceph.log  ceph.mon.keyring
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ceph-deploy mon create-initial
[[1mceph_deploy.conf[0m][[1;34mDEBUG[0m ] found configuration file at: /home/megdc/.cephdeploy.conf
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] Invoked (1.5.33): /usr/bin/ceph-deploy mon create-initial
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] ceph-deploy options:
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  username                      : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  verbose                       : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  overwrite_conf                : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  subcommand                    : create-initial
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  quiet                         : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f39aff0d5a8>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cluster                       : ceph
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  func                          : <function mon at 0x7f39b03776e0>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  ceph_conf                     : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  default_release               : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  keyrings                      : None
[[1mceph_deploy.mon[0m][[1;34mDEBUG[0m ] Deploying mon, cluster ceph hosts node1
[[1mceph_deploy.mon[0m][[1;34mDEBUG[0m ] detecting platform for host node1 ...
[[1mnode1[0m][[1;34mDEBUG[0m ] connection detected need for sudo
[[1mnode1[0m][[1;34mDEBUG[0m ] connected to host: node1 
[[1mnode1[0m][[1;34mDEBUG[0m ] detect platform information from remote host
[[1mnode1[0m][[1;34mDEBUG[0m ] detect machine type
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/initctl version
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mceph_deploy.mon[0m][[1;37mINFO[0m  ] distro info: Ubuntu 14.04 trusty
[[1mnode1[0m][[1;34mDEBUG[0m ] determining if provided host has same hostname in remote
[[1mnode1[0m][[1;34mDEBUG[0m ] get remote short hostname
[[1mnode1[0m][[1;34mDEBUG[0m ] deploying mon to node1
[[1mnode1[0m][[1;34mDEBUG[0m ] get remote short hostname
[[1mnode1[0m][[1;34mDEBUG[0m ] remote hostname: node1
[[1mnode1[0m][[1;34mDEBUG[0m ] write cluster configuration to /etc/ceph/{cluster}.conf
[[1mnode1[0m][[1;34mDEBUG[0m ] create the mon path if it does not exist
[[1mnode1[0m][[1;34mDEBUG[0m ] checking for done path: /var/lib/ceph/mon/ceph-node1/done
[[1mnode1[0m][[1;34mDEBUG[0m ] done path does not exist: /var/lib/ceph/mon/ceph-node1/done
[[1mnode1[0m][[1;37mINFO[0m  ] creating keyring file: /var/lib/ceph/tmp/ceph-node1.mon.keyring
[[1mnode1[0m][[1;34mDEBUG[0m ] create the monitor keyring file
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo ceph-mon --cluster ceph --mkfs -i node1 --keyring /var/lib/ceph/tmp/ceph-node1.mon.keyring --setuser 64045 --setgroup 64045
[[1mnode1[0m][[1;34mDEBUG[0m ] ceph-mon: mon.noname-a [2a01:4f8:212:11e8::2]:6789/0 is local, renaming to mon.node1
[[1mnode1[0m][[1;34mDEBUG[0m ] ceph-mon: set fsid to 768fbc30-f707-4f89-bdd8-59972cd96ff3
[[1mnode1[0m][[1;34mDEBUG[0m ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-node1 for mon.node1
[[1mnode1[0m][[1;37mINFO[0m  ] unlinking keyring file /var/lib/ceph/tmp/ceph-node1.mon.keyring
[[1mnode1[0m][[1;34mDEBUG[0m ] create a done file to avoid re-doing the mon deployment
[[1mnode1[0m][[1;34mDEBUG[0m ] create the init path if it does not exist
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo initctl emit ceph-mon cluster=ceph id=node1
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.node1.asok mon_status
[[1mnode1[0m][[1;34mDEBUG[0m ] ********************************************************************************
[[1mnode1[0m][[1;34mDEBUG[0m ] status for monitor: mon.node1
[[1mnode1[0m][[1;34mDEBUG[0m ] {
[[1mnode1[0m][[1;34mDEBUG[0m ]   "election_epoch": 3, 
[[1mnode1[0m][[1;34mDEBUG[0m ]   "extra_probe_peers": [], 
[[1mnode1[0m][[1;34mDEBUG[0m ]   "monmap": {
[[1mnode1[0m][[1;34mDEBUG[0m ]     "created": "2016-05-23 13:33:57.617476", 
[[1mnode1[0m][[1;34mDEBUG[0m ]     "epoch": 1, 
[[1mnode1[0m][[1;34mDEBUG[0m ]     "fsid": "768fbc30-f707-4f89-bdd8-59972cd96ff3", 
[[1mnode1[0m][[1;34mDEBUG[0m ]     "modified": "2016-05-23 13:33:57.617476", 
[[1mnode1[0m][[1;34mDEBUG[0m ]     "mons": [
[[1mnode1[0m][[1;34mDEBUG[0m ]       {
[[1mnode1[0m][[1;34mDEBUG[0m ]         "addr": "[2a01:4f8:212:11e8::2]:6789/0", 
[[1mnode1[0m][[1;34mDEBUG[0m ]         "name": "node1", 
[[1mnode1[0m][[1;34mDEBUG[0m ]         "rank": 0
[[1mnode1[0m][[1;34mDEBUG[0m ]       }
[[1mnode1[0m][[1;34mDEBUG[0m ]     ]
[[1mnode1[0m][[1;34mDEBUG[0m ]   }, 
[[1mnode1[0m][[1;34mDEBUG[0m ]   "name": "node1", 
[[1mnode1[0m][[1;34mDEBUG[0m ]   "outside_quorum": [], 
[[1mnode1[0m][[1;34mDEBUG[0m ]   "quorum": [
[[1mnode1[0m][[1;34mDEBUG[0m ]     0
[[1mnode1[0m][[1;34mDEBUG[0m ]   ], 
[[1mnode1[0m][[1;34mDEBUG[0m ]   "rank": 0, 
[[1mnode1[0m][[1;34mDEBUG[0m ]   "state": "leader", 
[[1mnode1[0m][[1;34mDEBUG[0m ]   "sync_provider": []
[[1mnode1[0m][[1;34mDEBUG[0m ] }
[[1mnode1[0m][[1;34mDEBUG[0m ] ********************************************************************************
[[1mnode1[0m][[1;37mINFO[0m  ] monitor: mon.node1 is running
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.node1.asok mon_status
[[1mceph_deploy.mon[0m][[1;37mINFO[0m  ] processing monitor mon.node1
[[1mnode1[0m][[1;34mDEBUG[0m ] connection detected need for sudo
[[1mnode1[0m][[1;34mDEBUG[0m ] connected to host: node1 
[[1mnode1[0m][[1;34mDEBUG[0m ] detect platform information from remote host
[[1mnode1[0m][[1;34mDEBUG[0m ] detect machine type
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/initctl version
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.node1.asok mon_status
[[1mceph_deploy.mon[0m][[1;37mINFO[0m  ] mon.node1 monitor has reached quorum!
[[1mceph_deploy.mon[0m][[1;37mINFO[0m  ] all initial monitors are running and have formed quorum
[[1mceph_deploy.mon[0m][[1;37mINFO[0m  ] Running gatherkeys...
[[1mceph_deploy.gatherkeys[0m][[1;34mDEBUG[0m ] Checking node1 for /etc/ceph/ceph.client.admin.keyring
[[1mnode1[0m][[1;34mDEBUG[0m ] connection detected need for sudo
[[1mnode1[0m][[1;34mDEBUG[0m ] connected to host: node1 
[[1mnode1[0m][[1;34mDEBUG[0m ] detect platform information from remote host
[[1mnode1[0m][[1;34mDEBUG[0m ] detect machine type
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/initctl version
[[1mnode1[0m][[1;34mDEBUG[0m ] fetch remote file
[[1mceph_deploy.gatherkeys[0m][[1;34mDEBUG[0m ] Got ceph.client.admin.keyring key from node1.
[[1mceph_deploy.gatherkeys[0m][[1;34mDEBUG[0m ] Have ceph.mon.keyring
[[1mceph_deploy.gatherkeys[0m][[1;34mDEBUG[0m ] Checking node1 for /var/lib/ceph/bootstrap-osd/ceph.keyring
[[1mnode1[0m][[1;34mDEBUG[0m ] connection detected need for sudo
[[1mnode1[0m][[1;34mDEBUG[0m ] connected to host: node1 
[[1mnode1[0m][[1;34mDEBUG[0m ] detect platform information from remote host
[[1mnode1[0m][[1;34mDEBUG[0m ] detect machine type
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/initctl version
[[1mnode1[0m][[1;34mDEBUG[0m ] fetch remote file
[[1mceph_deploy.gatherkeys[0m][[1;34mDEBUG[0m ] Got ceph.bootstrap-osd.keyring key from node1.
[[1mceph_deploy.gatherkeys[0m][[1;34mDEBUG[0m ] Checking node1 for /var/lib/ceph/bootstrap-mds/ceph.keyring
[[1mnode1[0m][[1;34mDEBUG[0m ] connection detected need for sudo
[[1mnode1[0m][[1;34mDEBUG[0m ] connected to host: node1 
[[1mnode1[0m][[1;34mDEBUG[0m ] detect platform information from remote host
[[1mnode1[0m][[1;34mDEBUG[0m ] detect machine type
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/initctl version
[[1mnode1[0m][[1;34mDEBUG[0m ] fetch remote file
[[1mceph_deploy.gatherkeys[0m][[1;34mDEBUG[0m ] Got ceph.bootstrap-mds.keyring key from node1.
[[1mceph_deploy.gatherkeys[0m][[1;34mDEBUG[0m ] Checking node1 for /var/lib/ceph/bootstrap-rgw/ceph.keyring
[[1mnode1[0m][[1;34mDEBUG[0m ] connection detected need for sudo
[[1mnode1[0m][[1;34mDEBUG[0m ] connected to host: node1 
[[1mnode1[0m][[1;34mDEBUG[0m ] detect platform information from remote host
[[1mnode1[0m][[1;34mDEBUG[0m ] detect machine type
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/initctl version
[[1mnode1[0m][[1;34mDEBUG[0m ] fetch remote file
[[1mceph_deploy.gatherkeys[0m][[1;34mDEBUG[0m ] Got ceph.bootstrap-rgw.keyring key from node1.
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ls
ceph.bootstrap-mds.keyring  ceph.bootstrap-rgw.keyring  ceph.conf             ceph.mon.keyring
ceph.bootstrap-osd.keyring  ceph.client.admin.keyring   ceph-deploy-ceph.log
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda       8:0    0 223.6G  0 disk  
├─sda1    8:1    0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sda2    8:2    0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sda3    8:3    0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdb       8:16   0 223.6G  0 disk  
├─sdb1    8:17   0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sdb2    8:18   0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sdb3    8:19   0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdc       8:32   0   5.5T  0 disk  
sdd       8:48   0   5.5T  0 disk  
├─sdd1    8:49   0   5.5T  0 part  
└─sdd2    8:50   0     5G  0 part  
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ dep[K[K[Kceph-deploy disk list
usage: ceph-deploy disk list [-h] HOST:DISK [HOST:DISK ...]
ceph-deploy disk list: error: too few arguments
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ceph-deploy disk list node1
[[1mceph_deploy.conf[0m][[1;34mDEBUG[0m ] found configuration file at: /home/megdc/.cephdeploy.conf
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] Invoked (1.5.33): /usr/bin/ceph-deploy disk list node1
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] ceph-deploy options:
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  username                      : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  verbose                       : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  overwrite_conf                : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  subcommand                    : list
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  quiet                         : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f145271b5a8>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cluster                       : ceph
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  func                          : <function disk at 0x7f1452b805f0>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  ceph_conf                     : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  default_release               : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  disk                          : [('node1', None, None)]
[[1mnode1[0m][[1;34mDEBUG[0m ] connection detected need for sudo
[[1mnode1[0m][[1;34mDEBUG[0m ] connected to host: node1 
[[1mnode1[0m][[1;34mDEBUG[0m ] detect platform information from remote host
[[1mnode1[0m][[1;34mDEBUG[0m ] detect machine type
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/initctl version
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mceph_deploy.osd[0m][[1;37mINFO[0m  ] Distro info: Ubuntu 14.04 trusty
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] Listing disks on node1...
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /usr/sbin/ceph-disk list
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop0 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop1 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop2 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop3 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop4 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop5 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop6 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop7 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/md0 other, ext3, mounted on /boot
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/md1 swap, swap
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/md2 other, ext4, mounted on /
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram0 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram1 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram10 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram11 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram12 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram13 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram14 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram15 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram2 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram3 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram4 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram5 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram6 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram7 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram8 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram9 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sda :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sda1 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sda2 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sda3 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sdb :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdb1 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdb2 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdb3 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sdc other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sdd :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdd2 other
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdd1 other, xfs
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ceph-deploy osd prepare node1:sdc
[[1mceph_deploy.conf[0m][[1;34mDEBUG[0m ] found configuration file at: /home/megdc/.cephdeploy.conf
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] Invoked (1.5.33): /usr/bin/ceph-deploy osd prepare node1:sdc
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] ceph-deploy options:
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  username                      : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  disk                          : [('node1', '/dev/sdc', None)]
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  dmcrypt                       : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  verbose                       : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  bluestore                     : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  overwrite_conf                : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  subcommand                    : prepare
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  quiet                         : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9c88b89f80>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cluster                       : ceph
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  fs_type                       : xfs
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  func                          : <function osd at 0x7f9c88feb578>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  ceph_conf                     : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  default_release               : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  zap_disk                      : False
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] Preparing cluster ceph disks node1:/dev/sdc:
[[1mnode1[0m][[1;34mDEBUG[0m ] connection detected need for sudo
[[1mnode1[0m][[1;34mDEBUG[0m ] connected to host: node1 
[[1mnode1[0m][[1;34mDEBUG[0m ] detect platform information from remote host
[[1mnode1[0m][[1;34mDEBUG[0m ] detect machine type
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/initctl version
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mceph_deploy.osd[0m][[1;37mINFO[0m  ] Distro info: Ubuntu 14.04 trusty
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] Deploying osd to node1
[[1mnode1[0m][[1;34mDEBUG[0m ] write cluster configuration to /etc/ceph/{cluster}.conf
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] Preparing host node1 disk /dev/sdc journal None activate False
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdc
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdc uuid path is /sys/dev/block/8:32/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] set_type: Will colocate journal with data on /dev/sdc
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdc uuid path is /sys/dev/block/8:32/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdc uuid path is /sys/dev/block/8:32/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdc uuid path is /sys/dev/block/8:32/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdc uuid path is /sys/dev/block/8:32/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdc uuid path is /sys/dev/block/8:32/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] ptype_tobe_for_name: name = journal
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdc uuid path is /sys/dev/block/8:32/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] create_partition: Creating journal partition num 2 size 5120 on /dev/sdc
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:938b0c8c-9b4a-4444-8ce8-e8fdc9703f66 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdc
[[1mnode1[0m][[1;34mDEBUG[0m ] The operation has completed successfully.
[[1mnode1[0m][[1;33mWARNIN[0m] update_partition: Calling partprobe on created device /dev/sdc
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /sbin/partprobe /dev/sdc
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdc uuid path is /sys/dev/block/8:32/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdc uuid path is /sys/dev/block/8:32/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdc2 uuid path is /sys/dev/block/8:34/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/938b0c8c-9b4a-4444-8ce8-e8fdc9703f66
[[1mnode1[0m][[1;33mWARNIN[0m] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/938b0c8c-9b4a-4444-8ce8-e8fdc9703f66
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdc uuid path is /sys/dev/block/8:32/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] set_data_partition: Creating osd partition on /dev/sdc
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdc uuid path is /sys/dev/block/8:32/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] ptype_tobe_for_name: name = data
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdc uuid path is /sys/dev/block/8:32/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] create_partition: Creating data partition num 1 size 0 on /dev/sdc
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:dab5457e-b095-4c0e-8ff4-9252844dab38 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sdc
[[1mnode1[0m][[1;34mDEBUG[0m ] The operation has completed successfully.
[[1mnode1[0m][[1;33mWARNIN[0m] update_partition: Calling partprobe on created device /dev/sdc
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /sbin/partprobe /dev/sdc
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdc uuid path is /sys/dev/block/8:32/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdc uuid path is /sys/dev/block/8:32/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdc1 uuid path is /sys/dev/block/8:33/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] populate_data_path_device: Creating xfs fs on /dev/sdc1
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdc1
[[1mnode1[0m][[1;34mDEBUG[0m ] meta-data=/dev/sdc1              isize=2048   agcount=32, agsize=45744365 blks
[[1mnode1[0m][[1;34mDEBUG[0m ]          =                       sectsz=4096  attr=2, projid32bit=0
[[1mnode1[0m][[1;34mDEBUG[0m ] data     =                       bsize=4096   blocks=1463819665, imaxpct=5
[[1mnode1[0m][[1;34mDEBUG[0m ]          =                       sunit=0      swidth=0 blks
[[1mnode1[0m][[1;34mDEBUG[0m ] naming   =version 2              bsize=4096   ascii-ci=0
[[1mnode1[0m][[1;34mDEBUG[0m ] log      =internal log           bsize=4096   blocks=521728, version=2
[[1mnode1[0m][[1;34mDEBUG[0m ]          =                       sectsz=4096  sunit=1 blks, lazy-count=1
[[1mnode1[0m][[1;34mDEBUG[0m ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[[1mnode1[0m][[1;33mWARNIN[0m] mount: Mounting /dev/sdc1 on /var/lib/ceph/tmp/mnt.biCgdv with options noatime,inode64
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdc1 /var/lib/ceph/tmp/mnt.biCgdv
[[1mnode1[0m][[1;33mWARNIN[0m] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.biCgdv
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.biCgdv/ceph_fsid.20132.tmp
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.biCgdv/fsid.20132.tmp
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.biCgdv/magic.20132.tmp
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.biCgdv/journal_uuid.20132.tmp
[[1mnode1[0m][[1;33mWARNIN[0m] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.biCgdv/journal -> /dev/disk/by-partuuid/938b0c8c-9b4a-4444-8ce8-e8fdc9703f66
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.biCgdv
[[1mnode1[0m][[1;33mWARNIN[0m] unmount: Unmounting /var/lib/ceph/tmp/mnt.biCgdv
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.biCgdv
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdc uuid path is /sys/dev/block/8:32/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdc
[[1mnode1[0m][[1;34mDEBUG[0m ] The operation has completed successfully.
[[1mnode1[0m][[1;33mWARNIN[0m] update_partition: Calling partprobe on prepared device /dev/sdc
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /sbin/partprobe /dev/sdc
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match sdc1
[[1mnode1[0m][[1;37mINFO[0m  ] checking OSD status...
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] Host node1 is now ready for osd use.
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda       8:0    0 223.6G  0 disk  
├─sda1    8:1    0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sda2    8:2    0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sda3    8:3    0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdb       8:16   0 223.6G  0 disk  
├─sdb1    8:17   0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sdb2    8:18   0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sdb3    8:19   0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdc       8:32   0   5.5T  0 disk  
├─sdc1    8:33   0   5.5T  0 part  
└─sdc2    8:34   0     5G  0 part  
sdd       8:48   0   5.5T  0 disk  
├─sdd1    8:49   0   5.5T  0 part  
└─sdd2    8:50   0     5G  0 part  
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ cd /etc/[K[K[K[K[K[K[K[Klsblkceph-deploy osd prepare node1:sdc[6Pdisk list node1
[[1mceph_deploy.conf[0m][[1;34mDEBUG[0m ] found configuration file at: /home/megdc/.cephdeploy.conf
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] Invoked (1.5.33): /usr/bin/ceph-deploy disk list node1
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] ceph-deploy options:
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  username                      : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  verbose                       : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  overwrite_conf                : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  subcommand                    : list
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  quiet                         : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff6962f35a8>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cluster                       : ceph
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  func                          : <function disk at 0x7ff6967585f0>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  ceph_conf                     : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  default_release               : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  disk                          : [('node1', None, None)]
[[1mnode1[0m][[1;34mDEBUG[0m ] connection detected need for sudo
[[1mnode1[0m][[1;34mDEBUG[0m ] connected to host: node1 
[[1mnode1[0m][[1;34mDEBUG[0m ] detect platform information from remote host
[[1mnode1[0m][[1;34mDEBUG[0m ] detect machine type
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/initctl version
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mceph_deploy.osd[0m][[1;37mINFO[0m  ] Distro info: Ubuntu 14.04 trusty
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] Listing disks on node1...
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /usr/sbin/ceph-disk list
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop0 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop1 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop2 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop3 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop4 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop5 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop6 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop7 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/md0 other, ext3, mounted on /boot
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/md1 swap, swap
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/md2 other, ext4, mounted on /
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram0 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram1 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram10 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram11 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram12 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram13 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram14 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram15 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram2 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram3 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram4 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram5 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram6 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram7 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram8 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram9 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sda :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sda1 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sda2 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sda3 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sdb :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdb1 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdb2 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdb3 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sdc :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdc2 ceph journal, for /dev/sdc1
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdc1 ceph data, active, cluster ceph, osd.0, journal /dev/sdc2
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sdd :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdd2 other
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdd1 other, xfs
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda       8:0    0 223.6G  0 disk  
├─sda1    8:1    0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sda2    8:2    0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sda3    8:3    0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdb       8:16   0 223.6G  0 disk  
├─sdb1    8:17   0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sdb2    8:18   0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sdb3    8:19   0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdc       8:32   0   5.5T  0 disk  
├─sdc1    8:33   0   5.5T  0 part  
└─sdc2    8:34   0     5G  0 part  
sdd       8:48   0   5.5T  0 disk  
├─sdd1    8:49   0   5.5T  0 part  
└─sdd2    8:50   0     5G  0 part  
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ l[Kceph-deploy disk zap node1:sdd
[[1mceph_deploy.conf[0m][[1;34mDEBUG[0m ] found configuration file at: /home/megdc/.cephdeploy.conf
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] Invoked (1.5.33): /usr/bin/ceph-deploy disk zap node1:sdd
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] ceph-deploy options:
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  username                      : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  verbose                       : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  overwrite_conf                : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  subcommand                    : zap
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  quiet                         : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7ff499dcf5a8>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cluster                       : ceph
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  func                          : <function disk at 0x7ff49a2345f0>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  ceph_conf                     : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  default_release               : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  disk                          : [('node1', '/dev/sdd', None)]
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] zapping /dev/sdd on node1
[[1mnode1[0m][[1;34mDEBUG[0m ] connection detected need for sudo
[[1mnode1[0m][[1;34mDEBUG[0m ] connected to host: node1 
[[1mnode1[0m][[1;34mDEBUG[0m ] detect platform information from remote host
[[1mnode1[0m][[1;34mDEBUG[0m ] detect machine type
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/initctl version
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mceph_deploy.osd[0m][[1;37mINFO[0m  ] Distro info: Ubuntu 14.04 trusty
[[1mnode1[0m][[1;34mDEBUG[0m ] zeroing last few blocks of device
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /usr/sbin/ceph-disk zap /dev/sdd
[[1mnode1[0m][[1;33mWARNIN[0m] Caution: invalid backup GPT header, but valid main header; regenerating
[[1mnode1[0m][[1;33mWARNIN[0m] backup header from main header.
[[1mnode1[0m][[1;33mWARNIN[0m] 
[[1mnode1[0m][[1;34mDEBUG[0m ] ****************************************************************************
[[1mnode1[0m][[1;34mDEBUG[0m ] Caution: Found protective or hybrid MBR and corrupt GPT. Using GPT, but disk
[[1mnode1[0m][[1;34mDEBUG[0m ] verification and recovery are STRONGLY recommended.
[[1mnode1[0m][[1;34mDEBUG[0m ] ****************************************************************************
[[1mnode1[0m][[1;34mDEBUG[0m ] GPT data structures destroyed! You may now partition the disk using fdisk or
[[1mnode1[0m][[1;34mDEBUG[0m ] other utilities.
[[1mnode1[0m][[1;34mDEBUG[0m ] Creating new GPT entries.
[[1mnode1[0m][[1;34mDEBUG[0m ] The operation has completed successfully.
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] Calling partprobe on zapped device /dev/sdd
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/partprobe /dev/sdd
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda       8:0    0 223.6G  0 disk  
├─sda1    8:1    0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sda2    8:2    0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sda3    8:3    0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdb       8:16   0 223.6G  0 disk  
├─sdb1    8:17   0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sdb2    8:18   0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sdb3    8:19   0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdc       8:32   0   5.5T  0 disk  
├─sdc1    8:33   0   5.5T  0 part  
└─sdc2    8:34   0     5G  0 part  
sdd       8:48   0   5.5T  0 disk  
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ceph-deploy disk list node1
[[1mceph_deploy.conf[0m][[1;34mDEBUG[0m ] found configuration file at: /home/megdc/.cephdeploy.conf
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] Invoked (1.5.33): /usr/bin/ceph-deploy disk list node1
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] ceph-deploy options:
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  username                      : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  verbose                       : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  overwrite_conf                : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  subcommand                    : list
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  quiet                         : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f23a9c1d5a8>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cluster                       : ceph
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  func                          : <function disk at 0x7f23aa0825f0>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  ceph_conf                     : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  default_release               : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  disk                          : [('node1', None, None)]
[[1mnode1[0m][[1;34mDEBUG[0m ] connection detected need for sudo
[[1mnode1[0m][[1;34mDEBUG[0m ] connected to host: node1 
[[1mnode1[0m][[1;34mDEBUG[0m ] detect platform information from remote host
[[1mnode1[0m][[1;34mDEBUG[0m ] detect machine type
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/initctl version
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mceph_deploy.osd[0m][[1;37mINFO[0m  ] Distro info: Ubuntu 14.04 trusty
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] Listing disks on node1...
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /usr/sbin/ceph-disk list
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop0 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop1 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop2 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop3 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop4 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop5 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop6 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop7 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/md0 other, ext3, mounted on /boot
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/md1 swap, swap
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/md2 other, ext4, mounted on /
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram0 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram1 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram10 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram11 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram12 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram13 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram14 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram15 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram2 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram3 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram4 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram5 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram6 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram7 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram8 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram9 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sda :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sda1 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sda2 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sda3 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sdb :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdb1 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdb2 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdb3 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sdc :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdc2 ceph journal, for /dev/sdc1
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdc1 ceph data, active, cluster ceph, osd.0, journal /dev/sdc2
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sdd other, unknown
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ceph-deploy disk list node1^C
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ 
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ 
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ceph-deploy osd prepare node1:sdd
[[1mceph_deploy.conf[0m][[1;34mDEBUG[0m ] found configuration file at: /home/megdc/.cephdeploy.conf
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] Invoked (1.5.33): /usr/bin/ceph-deploy osd prepare node1:sdd
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] ceph-deploy options:
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  username                      : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  disk                          : [('node1', '/dev/sdd', None)]
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  dmcrypt                       : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  verbose                       : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  bluestore                     : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  overwrite_conf                : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  subcommand                    : prepare
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  quiet                         : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f07c261bf80>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cluster                       : ceph
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  fs_type                       : xfs
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  func                          : <function osd at 0x7f07c2a7d578>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  ceph_conf                     : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  default_release               : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  zap_disk                      : False
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] Preparing cluster ceph disks node1:/dev/sdd:
[[1mnode1[0m][[1;34mDEBUG[0m ] connection detected need for sudo
[[1mnode1[0m][[1;34mDEBUG[0m ] connected to host: node1 
[[1mnode1[0m][[1;34mDEBUG[0m ] detect platform information from remote host
[[1mnode1[0m][[1;34mDEBUG[0m ] detect machine type
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/initctl version
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mceph_deploy.osd[0m][[1;37mINFO[0m  ] Distro info: Ubuntu 14.04 trusty
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] Deploying osd to node1
[[1mnode1[0m][[1;34mDEBUG[0m ] write cluster configuration to /etc/ceph/{cluster}.conf
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] Preparing host node1 disk /dev/sdd journal None activate False
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdd
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-osd --check-allows-journal -i 0 --cluster ceph
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-osd --check-wants-journal -i 0 --cluster ceph
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-osd --check-needs-journal -i 0 --cluster ceph
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdd uuid path is /sys/dev/block/8:48/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] set_type: Will colocate journal with data on /dev/sdd
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdd uuid path is /sys/dev/block/8:48/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdd uuid path is /sys/dev/block/8:48/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdd uuid path is /sys/dev/block/8:48/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdd uuid path is /sys/dev/block/8:48/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdd uuid path is /sys/dev/block/8:48/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] ptype_tobe_for_name: name = journal
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdd uuid path is /sys/dev/block/8:48/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] create_partition: Creating journal partition num 2 size 5120 on /dev/sdd
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/sgdisk --new=2:0:+5120M --change-name=2:ceph journal --partition-guid=2:b45707f8-de80-4e54-bcde-97c0f2a4fa9e --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdd
[[1mnode1[0m][[1;34mDEBUG[0m ] The operation has completed successfully.
[[1mnode1[0m][[1;33mWARNIN[0m] update_partition: Calling partprobe on created device /dev/sdd
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /sbin/partprobe /dev/sdd
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdd uuid path is /sys/dev/block/8:48/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdd uuid path is /sys/dev/block/8:48/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdd2 uuid path is /sys/dev/block/8:50/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/b45707f8-de80-4e54-bcde-97c0f2a4fa9e
[[1mnode1[0m][[1;33mWARNIN[0m] prepare_device: Journal is GPT partition /dev/disk/by-partuuid/b45707f8-de80-4e54-bcde-97c0f2a4fa9e
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdd uuid path is /sys/dev/block/8:48/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] set_data_partition: Creating osd partition on /dev/sdd
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdd uuid path is /sys/dev/block/8:48/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] ptype_tobe_for_name: name = data
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdd uuid path is /sys/dev/block/8:48/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] create_partition: Creating data partition num 1 size 0 on /dev/sdd
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:d89992e7-a7fd-4d6d-bdfa-c0bb5c79e907 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be --mbrtogpt -- /dev/sdd
[[1mnode1[0m][[1;34mDEBUG[0m ] The operation has completed successfully.
[[1mnode1[0m][[1;33mWARNIN[0m] update_partition: Calling partprobe on created device /dev/sdd
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /sbin/partprobe /dev/sdd
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdd uuid path is /sys/dev/block/8:48/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdd uuid path is /sys/dev/block/8:48/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdd1 uuid path is /sys/dev/block/8:49/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] populate_data_path_device: Creating xfs fs on /dev/sdd1
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdd1
[[1mnode1[0m][[1;34mDEBUG[0m ] meta-data=/dev/sdd1              isize=2048   agcount=32, agsize=45744365 blks
[[1mnode1[0m][[1;34mDEBUG[0m ]          =                       sectsz=4096  attr=2, projid32bit=0
[[1mnode1[0m][[1;34mDEBUG[0m ] data     =                       bsize=4096   blocks=1463819665, imaxpct=5
[[1mnode1[0m][[1;34mDEBUG[0m ]          =                       sunit=0      swidth=0 blks
[[1mnode1[0m][[1;34mDEBUG[0m ] naming   =version 2              bsize=4096   ascii-ci=0
[[1mnode1[0m][[1;34mDEBUG[0m ] log      =internal log           bsize=4096   blocks=521728, version=2
[[1mnode1[0m][[1;34mDEBUG[0m ]          =                       sectsz=4096  sunit=1 blks, lazy-count=1
[[1mnode1[0m][[1;34mDEBUG[0m ] realtime =none                   extsz=4096   blocks=0, rtextents=0
[[1mnode1[0m][[1;33mWARNIN[0m] mount: Mounting /dev/sdd1 on /var/lib/ceph/tmp/mnt.kB4O_A with options noatime,inode64
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdd1 /var/lib/ceph/tmp/mnt.kB4O_A
[[1mnode1[0m][[1;33mWARNIN[0m] populate_data_path: Preparing osd data dir /var/lib/ceph/tmp/mnt.kB4O_A
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.kB4O_A/ceph_fsid.21460.tmp
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.kB4O_A/fsid.21460.tmp
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.kB4O_A/magic.21460.tmp
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.kB4O_A/journal_uuid.21460.tmp
[[1mnode1[0m][[1;33mWARNIN[0m] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.kB4O_A/journal -> /dev/disk/by-partuuid/b45707f8-de80-4e54-bcde-97c0f2a4fa9e
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /bin/chown -R ceph:ceph /var/lib/ceph/tmp/mnt.kB4O_A
[[1mnode1[0m][[1;33mWARNIN[0m] unmount: Unmounting /var/lib/ceph/tmp/mnt.kB4O_A
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.kB4O_A
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdd uuid path is /sys/dev/block/8:48/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdd
[[1mnode1[0m][[1;34mDEBUG[0m ] The operation has completed successfully.
[[1mnode1[0m][[1;33mWARNIN[0m] update_partition: Calling partprobe on prepared device /dev/sdd
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /sbin/partprobe /dev/sdd
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/udevadm settle --timeout=600
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/udevadm trigger --action=add --sysname-match sdd1
[[1mnode1[0m][[1;37mINFO[0m  ] checking OSD status...
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] Host node1 is now ready for osd use.
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda       8:0    0 223.6G  0 disk  
├─sda1    8:1    0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sda2    8:2    0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sda3    8:3    0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdb       8:16   0 223.6G  0 disk  
├─sdb1    8:17   0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sdb2    8:18   0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sdb3    8:19   0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdc       8:32   0   5.5T  0 disk  
├─sdc1    8:33   0   5.5T  0 part  
└─sdc2    8:34   0     5G  0 part  
sdd       8:48   0   5.5T  0 disk  
├─sdd1    8:49   0   5.5T  0 part  
└─sdd2    8:50   0     5G  0 part  
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ceph-dk[Kisk list[K[K[K[Kdiks[K[Ksk list node1
usage: ceph-disk [-h] [-v] [--log-stdout] [--prepend-to-path PATH]
                 [--statedir PATH] [--sysconfdir PATH] [--setuser USER]
                 [--setgroup GROUP]
                 {prepare,activate,activate-lockbox,activate-block,activate-journal,activate-all,list,suppress-activate,unsuppress-activate,deactivate,destroy,zap,trigger}
                 ...
ceph-disk: error: invalid choice: 'disk' (choose from 'prepare', 'activate', 'activate-lockbox', 'activate-block', 'activate-journal', 'activate-all', 'list', 'suppress-activate', 'unsuppress-activate', 'deactivate', 'destroy', 'zap', 'trigger')
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ceph-disk disk list node1[C[1P[1P[1P[1P[1P[1@-[1@d[1@e[1@p[1@l[1@o[1@y
[[1mceph_deploy.conf[0m][[1;34mDEBUG[0m ] found configuration file at: /home/megdc/.cephdeploy.conf
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] Invoked (1.5.33): /usr/bin/ceph-deploy disk list node1
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] ceph-deploy options:
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  username                      : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  verbose                       : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  overwrite_conf                : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  subcommand                    : list
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  quiet                         : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7faee8f305a8>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cluster                       : ceph
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  func                          : <function disk at 0x7faee93955f0>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  ceph_conf                     : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  default_release               : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  disk                          : [('node1', None, None)]
[[1mnode1[0m][[1;34mDEBUG[0m ] connection detected need for sudo
[[1mnode1[0m][[1;34mDEBUG[0m ] connected to host: node1 
[[1mnode1[0m][[1;34mDEBUG[0m ] detect platform information from remote host
[[1mnode1[0m][[1;34mDEBUG[0m ] detect machine type
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/initctl version
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mceph_deploy.osd[0m][[1;37mINFO[0m  ] Distro info: Ubuntu 14.04 trusty
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] Listing disks on node1...
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /usr/sbin/ceph-disk list
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop0 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop1 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop2 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop3 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop4 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop5 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop6 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop7 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/md0 other, ext3, mounted on /boot
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/md1 swap, swap
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/md2 other, ext4, mounted on /
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram0 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram1 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram10 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram11 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram12 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram13 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram14 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram15 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram2 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram3 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram4 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram5 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram6 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram7 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram8 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram9 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sda :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sda1 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sda2 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sda3 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sdb :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdb1 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdb2 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdb3 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sdc :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdc2 ceph journal, for /dev/sdc1
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdc1 ceph data, active, cluster ceph, osd.0, journal /dev/sdc2
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sdd :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdd2 ceph journal, for /dev/sdd1
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdd1 ceph data, active, cluster ceph, osd.1, journal /dev/sdd2
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ceph-deploy osd activate node1:/dev/sdc1:/dev/sdc2
[[1mceph_deploy.conf[0m][[1;34mDEBUG[0m ] found configuration file at: /home/megdc/.cephdeploy.conf
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] Invoked (1.5.33): /usr/bin/ceph-deploy osd activate node1:/dev/sdc1:/dev/sdc2
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] ceph-deploy options:
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  username                      : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  verbose                       : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  overwrite_conf                : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  subcommand                    : activate
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  quiet                         : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f9f252e4f80>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cluster                       : ceph
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  func                          : <function osd at 0x7f9f25746578>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  ceph_conf                     : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  default_release               : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  disk                          : [('node1', '/dev/sdc1', '/dev/sdc2')]
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] Activating cluster ceph disks node1:/dev/sdc1:/dev/sdc2
[[1mnode1[0m][[1;34mDEBUG[0m ] connection detected need for sudo
[[1mnode1[0m][[1;34mDEBUG[0m ] connected to host: node1 
[[1mnode1[0m][[1;34mDEBUG[0m ] detect platform information from remote host
[[1mnode1[0m][[1;34mDEBUG[0m ] detect machine type
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/initctl version
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mceph_deploy.osd[0m][[1;37mINFO[0m  ] Distro info: Ubuntu 14.04 trusty
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] activating host node1 disk /dev/sdc1
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] will use init type: upstart
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /usr/sbin/ceph-disk -v activate --mark-init upstart --mount /dev/sdc1
[[1mnode1[0m][[1;33mWARNIN[0m] main_activate: path = /dev/sdc1
[[1mnode1[0m][[1;33mWARNIN[0m] get_dm_uuid: get_dm_uuid /dev/sdc1 uuid path is /sys/dev/block/8:33/dm/uuid
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /sbin/blkid -o udev -p /dev/sdc1
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /sbin/blkid -p -s TYPE -o value -- /dev/sdc1
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[[1mnode1[0m][[1;33mWARNIN[0m] mount: Mounting /dev/sdc1 on /var/lib/ceph/tmp/mnt.hqWR9I with options noatime,inode64
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /bin/mount -t xfs -o noatime,inode64 -- /dev/sdc1 /var/lib/ceph/tmp/mnt.hqWR9I
[[1mnode1[0m][[1;33mWARNIN[0m] activate: Cluster uuid is 768fbc30-f707-4f89-bdd8-59972cd96ff3
[[1mnode1[0m][[1;33mWARNIN[0m] command: Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[[1mnode1[0m][[1;33mWARNIN[0m] activate: Cluster name is ceph
[[1mnode1[0m][[1;33mWARNIN[0m] activate: OSD uuid is dab5457e-b095-4c0e-8ff4-9252844dab38
[[1mnode1[0m][[1;33mWARNIN[0m] activate: OSD id is 0
[[1mnode1[0m][[1;33mWARNIN[0m] activate: Marking with init system upstart
[[1mnode1[0m][[1;33mWARNIN[0m] activate: ceph osd.0 data dir is ready at /var/lib/ceph/tmp/mnt.hqWR9I
[[1mnode1[0m][[1;33mWARNIN[0m] mount_activate: ceph osd.0 already mounted in position; unmounting ours.
[[1mnode1[0m][[1;33mWARNIN[0m] unmount: Unmounting /var/lib/ceph/tmp/mnt.hqWR9I
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.hqWR9I
[[1mnode1[0m][[1;33mWARNIN[0m] start_daemon: Starting ceph osd.0...
[[1mnode1[0m][[1;33mWARNIN[0m] command_check_call: Running command: /sbin/initctl emit --no-wait -- ceph-osd cluster=ceph id=0
[[1mnode1[0m][[1;37mINFO[0m  ] checking OSD status...
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /usr/bin/ceph --cluster=ceph osd stat --format=json
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ce[K[Klsblk
NAME    MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda       8:0    0 223.6G  0 disk  
├─sda1    8:1    0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sda2    8:2    0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sda3    8:3    0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdb       8:16   0 223.6G  0 disk  
├─sdb1    8:17   0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sdb2    8:18   0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sdb3    8:19   0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdc       8:32   0   5.5T  0 disk  
├─sdc1    8:33   0   5.5T  0 part  
└─sdc2    8:34   0     5G  0 part  
sdd       8:48   0   5.5T  0 disk  
├─sdd1    8:49   0   5.5T  0 part  
└─sdd2    8:50   0     5G  0 part  
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ls[K[Kceph-deploy disk list node1
[[1mceph_deploy.conf[0m][[1;34mDEBUG[0m ] found configuration file at: /home/megdc/.cephdeploy.conf
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] Invoked (1.5.33): /usr/bin/ceph-deploy disk list node1
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ] ceph-deploy options:
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  username                      : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  verbose                       : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  overwrite_conf                : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  subcommand                    : list
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  quiet                         : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f01aefdc5a8>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  cluster                       : ceph
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  func                          : <function disk at 0x7f01af4415f0>
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  ceph_conf                     : None
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  default_release               : False
[[1mceph_deploy.cli[0m][[1;37mINFO[0m  ]  disk                          : [('node1', None, None)]
[[1mnode1[0m][[1;34mDEBUG[0m ] connection detected need for sudo
[[1mnode1[0m][[1;34mDEBUG[0m ] connected to host: node1 
[[1mnode1[0m][[1;34mDEBUG[0m ] detect platform information from remote host
[[1mnode1[0m][[1;34mDEBUG[0m ] detect machine type
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /sbin/initctl version
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mceph_deploy.osd[0m][[1;37mINFO[0m  ] Distro info: Ubuntu 14.04 trusty
[[1mceph_deploy.osd[0m][[1;34mDEBUG[0m ] Listing disks on node1...
[[1mnode1[0m][[1;34mDEBUG[0m ] find the location of an executable
[[1mnode1[0m][[1;37mINFO[0m  ] Running command: sudo /usr/sbin/ceph-disk list
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop0 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop1 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop2 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop3 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop4 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop5 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop6 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/loop7 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/md0 other, ext3, mounted on /boot
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/md1 swap, swap
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/md2 other, ext4, mounted on /
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram0 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram1 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram10 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram11 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram12 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram13 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram14 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram15 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram2 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram3 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram4 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram5 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram6 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram7 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram8 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/ram9 other, unknown
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sda :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sda1 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sda2 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sda3 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sdb :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdb1 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdb2 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdb3 other, linux_raid_member
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sdc :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdc2 ceph journal, for /dev/sdc1
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdc1 ceph data, active, cluster ceph, osd.0, journal /dev/sdc2
[[1mnode1[0m][[1;34mDEBUG[0m ] /dev/sdd :
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdd2 ceph journal, for /dev/sdd1
[[1mnode1[0m][[1;34mDEBUG[0m ]  /dev/sdd1 ceph data, active, cluster ceph, osd.1, journal /dev/sdd2
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ df -h
Filesystem      Size  Used Avail Use% Mounted on
udev             32G   12K   32G   1% /dev
tmpfs           6.3G  868K  6.3G   1% /run
/dev/md2        187G  5.5G  172G   4% /
none            4.0K     0  4.0K   0% /sys/fs/cgroup
none            5.0M     0  5.0M   0% /run/lock
none             32G     0   32G   0% /run/shm
none            100M     0  100M   0% /run/user
/dev/md0        488M   74M  388M  17% /boot
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda       8:0    0 223.6G  0 disk  
├─sda1    8:1    0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sda2    8:2    0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sda3    8:3    0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdb       8:16   0 223.6G  0 disk  
├─sdb1    8:17   0   512M  0 part  
│ └─md0   9:0    0 511.4M  0 raid1 /boot
├─sdb2    8:18   0    32G  0 part  
│ └─md1   9:1    0    32G  0 raid1 [SWAP]
└─sdb3    8:19   0   190G  0 part  
  └─md2   9:2    0 189.9G  0 raid1 /
sdc       8:32   0   5.5T  0 disk  
├─sdc1    8:33   0   5.5T  0 part  
└─sdc2    8:34   0     5G  0 part  
sdd       8:48   0   5.5T  0 disk  
├─sdd1    8:49   0   5.5T  0 part  
└─sdd2    8:50   0     5G  0 part  
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ cat /etc/fstab
proc /proc proc defaults 0 0
/dev/md/0 /boot ext3 defaults 0 0
/dev/md/1 none swap sw 0 0
/dev/md/2 / ext4 defaults 0 0
#/dev/sdc1 /storage1 ext4 defaults 0 0
#/dev/sdd1 /storage2 ext4 defaults 0 0
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ceph status
2016-05-23 13:49:26.510560 7fc588379700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin: (2) No such file or directory
2016-05-23 13:49:26.510576 7fc588379700 -1 monclient(hunting): ERROR: missing keyring, cannot use cephx for authentication
2016-05-23 13:49:26.510578 7fc588379700  0 librados: client.admin initialization error (2) No such file or directory
Error connecting to cluster: ObjectNotFound
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ls
ceph.bootstrap-mds.keyring  ceph.bootstrap-rgw.keyring  ceph.conf             ceph.mon.keyring
ceph.bootstrap-osd.keyring  ceph.client.admin.keyring   ceph-deploy-ceph.log
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ lsceph status[1@s[1@u[1@d[1@o[1@ 
    cluster 768fbc30-f707-4f89-bdd8-59972cd96ff3
     health HEALTH_WARN
            32 pgs degraded
            64 pgs stuck unclean
            32 pgs undersized
     monmap e1: 1 mons at {node1=[2a01:4f8:212:11e8::2]:6789/0}
            election epoch 3, quorum 0 node1
     osdmap e10: 2 osds: 2 up, 2 in; 32 remapped pgs
            flags sortbitwise
      pgmap v24: 64 pgs, 1 pools, 0 bytes data, 0 objects
            77548 kB used, 11164 GB / 11164 GB avail
                  32 active+undersized+degraded
                  32 active
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ chmod +x /c[Ketc/ceph/ceph.c.[Klient.admin.keyring 
chmod: changing permissions of ‘/etc/ceph/ceph.client.admin.keyring’: Operation not permitted
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ chmod +x /etc/ceph/ceph.client.admin.keyring [1@s[1@u[1@d[1@o[1@ 
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ceph status
2016-05-23 13:50:29.503949 7f66e2988700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin: (2) No such file or directory
2016-05-23 13:50:29.503963 7f66e2988700 -1 monclient(hunting): ERROR: missing keyring, cannot use cephx for authentication
2016-05-23 13:50:29.503965 7f66e2988700  0 librados: client.admin initialization error (2) No such file or directory
Error connecting to cluster: ObjectNotFound
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ cd [K[K[Kce [K[K[Kls -la /etc/ceph/
total 20
drwxr-xr-x  2 root root 4096 May 23 13:42 [0m[01;34m.[0m
drwxr-xr-x 94 root root 4096 May 23 13:46 [01;34m..[0m
-rwx--x--x  1 ceph ceph   63 May 23 13:33 [01;32mceph.client.admin.keyring[0m
-rw-r--r--  1 root root  251 May 23 13:42 ceph.conf
-rw-r--r--  1 root root   92 May 14 00:27 rbdmap
-rw-------  1 root root    0 May 23 13:33 tmptWqxir
]0;megdc@node1: ~/ceph-clustermegdc@node1:~/ceph-cluster$ ch[K[Kexit
exit

Script done on Mon 23 May 2016 01:51:08 PM EAT
